starting GA run for  Acrobot-v1
Generation: 1
Generation: 2
Generation: 3
Generation: 4
Generation: 5
Generation: 6
Generation: 7
Generation: 8
Generation: 9
Generation: 10
Generation: 11
Generation: 12
Generation: 13
Generation: 14
Generation: 15
Generation: 16
Generation: 17
Generation: 18
Generation: 19
Generation: 20
Generation: 21
Generation: 22
Generation: 23
Generation: 24
Generation: 25
Generation: 26
Generation: 27
Generation: 28
Generation: 29
Generation: 30
done
starting GA run for  LunarLander-v2
Generation: 1
Generation: 2
Generation: 3
Generation: 4
Generation: 5
Generation: 6
Generation: 7
Generation: 8
Generation: 9
Generation: 10
Generation: 11
Generation: 12
Generation: 13
Generation: 14
Generation: 15
Generation: 16
Generation: 17
Generation: 18
Generation: 19
Generation: 20
Generation: 21
Generation: 22
Generation: 23
Generation: 24
Generation: 25
Generation: 26
Generation: 27
Generation: 28
Generation: 29
Generation: 30
done
Fatal Python error: pygame_parachute: (pygame parachute) Segmentation Fault
Python runtime state: finalizing (tstate=0x0000000001340dd0)

Current thread 0x00007f0dede0c440 (most recent call first):
  <no Python frame>
starting GA run for  LunarLander-v2
Generation: 1
Generation: 2
Generation: 3
Generation: 4
Generation: 5
Generation: 6
Generation: 7
Generation: 8
Generation: 9
Generation: 10
Generation: 11
Generation: 12
Generation: 13
Generation: 14
Generation: 15
Generation: 16
Generation: 17
Generation: 18
Generation: 19
Generation: 20
Generation: 21
Generation: 22
Generation: 23
Generation: 24
Generation: 25
Generation: 26
Generation: 27
Generation: 28
Generation: 29
Generation: 30
done
Traceback (most recent call last):
  File "/home/ftn0813/projects/SparseArch/rl-baselines3-zoo/train.py", line 1, in <module>
    from rl_zoo3.train import train
  File "/home/ftn0813/projects/SparseArch/rl-baselines3-zoo/rl_zoo3/__init__.py", line 6, in <module>
    import rl_zoo3.gym_patches  # noqa: F401
  File "/home/ftn0813/projects/SparseArch/rl-baselines3-zoo/rl_zoo3/gym_patches.py", line 12, in <module>
    import gym  # noqa: E402
ModuleNotFoundError: No module named 'gym'
2023-07-27 07:36:42.934403: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
wandb: Currently logged in as: nhartzler (awesomepossum). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.1
wandb: Run data is saved locally in /home/ftn0813/projects/SparseArch/wandb/run-20230727_073643-xu42jib0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run LunarLander-v2__ppo_lstm__268318194__1690461402
wandb: ‚≠êÔ∏è View project at https://wandb.ai/awesomepossum/thesis
wandb: üöÄ View run at https://wandb.ai/awesomepossum/thesis/runs/xu42jib0
========== LunarLander-v2 ==========
Seed: 268318194
Loading hyperparameters from: ga_tuned_configs/ga_baseline_tuned.yml
Default hyperparameters for environment (ones being tuned will be overridden):
OrderedDict([('batch_size', 128),
             ('clip_range', 0.938),
             ('ent_coef', 0.953),
             ('gae_lambda', 0.833),
             ('gamma', 0.708),
             ('learning_rate', 0.819),
             ('n_envs', 32),
             ('n_epochs', 4),
             ('n_steps', 512),
             ('n_timesteps', 5000000.0),
             ('normalize', True),
             ('policy', 'MlpLstmPolicy'),
             ('policy_kwargs',
              'dict( ortho_init=False, activation_fn=nn.ReLU, '
              'lstm_hidden_size=64, enable_critic_lstm=True, '
              'net_arch=dict(pi=[64], vf=[64]) )'),
             ('vf_coef', 0.827)])
Using 32 environments
Creating test environment
Normalization activated: {'gamma': 0.708, 'norm_reward': False, 'training': False}
Normalization activated: {'gamma': 0.708}
Using cuda device
Log path: logs/ppo_lstm/LunarLander-v2_4
Logging to runs/LunarLander-v2__ppo_lstm__268318194__1690461402/LunarLander-v2/RecurrentPPO_1
2023-07-27 07:36:50.571472: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Eval num_timesteps=9984, episode_reward=-512.56 +/- 91.40
Episode length: 64.80 +/- 10.24
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64.8     |
|    mean_reward     | -513     |
| time/              |          |
|    total_timesteps | 9984     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 91.9     |
|    ep_rew_mean     | -191     |
| time/              |          |
|    fps             | 9078     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=19968, episode_reward=-135.15 +/- 24.68
Episode length: 72.90 +/- 13.08
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 72.9     |
|    mean_reward          | -135     |
| time/                   |          |
|    total_timesteps      | 19968    |
| train/                  |          |
|    approx_kl            | 4170.23  |
|    clip_fraction        | 0.998    |
|    clip_range           | 0.938    |
|    entropy_loss         | -0.00299 |
|    explained_variance   | 0.00296  |
|    learning_rate        | 0.819    |
|    loss                 | 0.76     |
|    n_updates            | 4        |
|    policy_gradient_loss | 0.155    |
|    value_loss           | 2.82e+03 |
--------------------------------------
New best mean reward!
Eval num_timesteps=29952, episode_reward=-138.67 +/- 54.69
Episode length: 68.50 +/- 8.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.5     |
|    mean_reward     | -139     |
| time/              |          |
|    total_timesteps | 29952    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.9     |
|    ep_rew_mean     | -133     |
| time/              |          |
|    fps             | 841      |
|    iterations      | 2        |
|    time_elapsed    | 38       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=39936, episode_reward=-139.15 +/- 18.21
Episode length: 72.30 +/- 11.38
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 72.3     |
|    mean_reward          | -139     |
| time/                   |          |
|    total_timesteps      | 39936    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | -17.7    |
|    learning_rate        | 0.819    |
|    loss                 | 1.12     |
|    n_updates            | 8        |
|    policy_gradient_loss | 8.32e-11 |
|    value_loss           | 15.2     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.4     |
|    ep_rew_mean     | -129     |
| time/              |          |
|    fps             | 685      |
|    iterations      | 3        |
|    time_elapsed    | 71       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=49920, episode_reward=-137.29 +/- 26.86
Episode length: 70.60 +/- 10.98
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 70.6     |
|    mean_reward          | -137     |
| time/                   |          |
|    total_timesteps      | 49920    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 1.02     |
|    n_updates            | 12       |
|    policy_gradient_loss | 3.64e-11 |
|    value_loss           | 1.31     |
--------------------------------------
Eval num_timesteps=59904, episode_reward=-145.21 +/- 15.71
Episode length: 71.40 +/- 11.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.4     |
|    mean_reward     | -145     |
| time/              |          |
|    total_timesteps | 59904    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.1     |
|    ep_rew_mean     | -133     |
| time/              |          |
|    fps             | 624      |
|    iterations      | 4        |
|    time_elapsed    | 105      |
|    total_timesteps | 65536    |
---------------------------------
Eval num_timesteps=69888, episode_reward=-153.66 +/- 19.06
Episode length: 68.70 +/- 10.09
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 68.7     |
|    mean_reward          | -154     |
| time/                   |          |
|    total_timesteps      | 69888    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 1.61     |
|    n_updates            | 16       |
|    policy_gradient_loss | 7.55e-10 |
|    value_loss           | 1.24     |
--------------------------------------
Eval num_timesteps=79872, episode_reward=-121.43 +/- 49.90
Episode length: 70.90 +/- 18.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.9     |
|    mean_reward     | -121     |
| time/              |          |
|    total_timesteps | 79872    |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.5     |
|    ep_rew_mean     | -133     |
| time/              |          |
|    fps             | 579      |
|    iterations      | 5        |
|    time_elapsed    | 141      |
|    total_timesteps | 81920    |
---------------------------------
Eval num_timesteps=89856, episode_reward=-141.59 +/- 20.82
Episode length: 70.30 +/- 12.15
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 70.3      |
|    mean_reward          | -142      |
| time/                   |           |
|    total_timesteps      | 89856     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.607     |
|    n_updates            | 20        |
|    policy_gradient_loss | -2.18e-11 |
|    value_loss           | 1.25      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.8     |
|    ep_rew_mean     | -133     |
| time/              |          |
|    fps             | 564      |
|    iterations      | 6        |
|    time_elapsed    | 174      |
|    total_timesteps | 98304    |
---------------------------------
Eval num_timesteps=99840, episode_reward=-124.32 +/- 39.71
Episode length: 73.70 +/- 9.03
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 73.7     |
|    mean_reward          | -124     |
| time/                   |          |
|    total_timesteps      | 99840    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 1.04     |
|    n_updates            | 24       |
|    policy_gradient_loss | 2.21e-11 |
|    value_loss           | 1.3      |
--------------------------------------
Eval num_timesteps=109824, episode_reward=-131.88 +/- 18.49
Episode length: 72.40 +/- 8.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.4     |
|    mean_reward     | -132     |
| time/              |          |
|    total_timesteps | 109824   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72.2     |
|    ep_rew_mean     | -137     |
| time/              |          |
|    fps             | 550      |
|    iterations      | 7        |
|    time_elapsed    | 208      |
|    total_timesteps | 114688   |
---------------------------------
Eval num_timesteps=119808, episode_reward=-148.67 +/- 24.97
Episode length: 72.60 +/- 11.12
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 72.6      |
|    mean_reward          | -149      |
| time/                   |           |
|    total_timesteps      | 119808    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.819     |
|    loss                 | 1.06      |
|    n_updates            | 28        |
|    policy_gradient_loss | -6.88e-10 |
|    value_loss           | 1.27      |
---------------------------------------
Eval num_timesteps=129792, episode_reward=-157.73 +/- 21.54
Episode length: 72.30 +/- 13.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.3     |
|    mean_reward     | -158     |
| time/              |          |
|    total_timesteps | 129792   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.3     |
|    ep_rew_mean     | -137     |
| time/              |          |
|    fps             | 549      |
|    iterations      | 8        |
|    time_elapsed    | 238      |
|    total_timesteps | 131072   |
---------------------------------
Eval num_timesteps=139776, episode_reward=-142.19 +/- 31.79
Episode length: 69.00 +/- 10.28
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 69       |
|    mean_reward          | -142     |
| time/                   |          |
|    total_timesteps      | 139776   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.944    |
|    n_updates            | 32       |
|    policy_gradient_loss | 3.32e-11 |
|    value_loss           | 1.21     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71       |
|    ep_rew_mean     | -131     |
| time/              |          |
|    fps             | 540      |
|    iterations      | 9        |
|    time_elapsed    | 272      |
|    total_timesteps | 147456   |
---------------------------------
Eval num_timesteps=149760, episode_reward=-123.71 +/- 21.55
Episode length: 65.60 +/- 10.05
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 65.6      |
|    mean_reward          | -124      |
| time/                   |           |
|    total_timesteps      | 149760    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.819     |
|    loss                 | 0.944     |
|    n_updates            | 36        |
|    policy_gradient_loss | 3.4e-10   |
|    value_loss           | 1.29      |
---------------------------------------
Eval num_timesteps=159744, episode_reward=-138.15 +/- 71.96
Episode length: 73.90 +/- 14.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.9     |
|    mean_reward     | -138     |
| time/              |          |
|    total_timesteps | 159744   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.8     |
|    ep_rew_mean     | -132     |
| time/              |          |
|    fps             | 534      |
|    iterations      | 10       |
|    time_elapsed    | 306      |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=169728, episode_reward=-130.51 +/- 15.86
Episode length: 71.30 +/- 10.98
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 71.3      |
|    mean_reward          | -131      |
| time/                   |           |
|    total_timesteps      | 169728    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 4.67e-05  |
|    learning_rate        | 0.819     |
|    loss                 | 0.567     |
|    n_updates            | 40        |
|    policy_gradient_loss | -2.18e-11 |
|    value_loss           | 5.08      |
---------------------------------------
Eval num_timesteps=179712, episode_reward=-137.43 +/- 26.44
Episode length: 68.30 +/- 11.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.3     |
|    mean_reward     | -137     |
| time/              |          |
|    total_timesteps | 179712   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.3     |
|    ep_rew_mean     | -133     |
| time/              |          |
|    fps             | 528      |
|    iterations      | 11       |
|    time_elapsed    | 341      |
|    total_timesteps | 180224   |
---------------------------------
Eval num_timesteps=189696, episode_reward=-126.73 +/- 19.53
Episode length: 67.70 +/- 11.91
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 67.7     |
|    mean_reward          | -127     |
| time/                   |          |
|    total_timesteps      | 189696   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | -1.47    |
|    learning_rate        | 0.819    |
|    loss                 | 0.632    |
|    n_updates            | 44       |
|    policy_gradient_loss | 5.99e-10 |
|    value_loss           | 25.6     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.8     |
|    ep_rew_mean     | -141     |
| time/              |          |
|    fps             | 524      |
|    iterations      | 12       |
|    time_elapsed    | 374      |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=199680, episode_reward=-158.70 +/- 31.45
Episode length: 76.40 +/- 14.68
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 76.4     |
|    mean_reward          | -159     |
| time/                   |          |
|    total_timesteps      | 199680   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 1.03     |
|    n_updates            | 48       |
|    policy_gradient_loss | 8.14e-11 |
|    value_loss           | 1.35     |
--------------------------------------
Eval num_timesteps=209664, episode_reward=-121.14 +/- 59.31
Episode length: 62.50 +/- 9.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.5     |
|    mean_reward     | -121     |
| time/              |          |
|    total_timesteps | 209664   |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.4     |
|    ep_rew_mean     | -125     |
| time/              |          |
|    fps             | 518      |
|    iterations      | 13       |
|    time_elapsed    | 410      |
|    total_timesteps | 212992   |
---------------------------------
Eval num_timesteps=219648, episode_reward=-126.03 +/- 48.39
Episode length: 75.30 +/- 13.24
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 75.3     |
|    mean_reward          | -126     |
| time/                   |          |
|    total_timesteps      | 219648   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 1.02     |
|    n_updates            | 52       |
|    policy_gradient_loss | 9.82e-11 |
|    value_loss           | 1.25     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.9     |
|    ep_rew_mean     | -142     |
| time/              |          |
|    fps             | 514      |
|    iterations      | 14       |
|    time_elapsed    | 445      |
|    total_timesteps | 229376   |
---------------------------------
Eval num_timesteps=229632, episode_reward=-85.63 +/- 64.97
Episode length: 70.50 +/- 14.73
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 70.5      |
|    mean_reward          | -85.6     |
| time/                   |           |
|    total_timesteps      | 229632    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 1.05      |
|    n_updates            | 56        |
|    policy_gradient_loss | -1.09e-11 |
|    value_loss           | 1.29      |
---------------------------------------
New best mean reward!
Eval num_timesteps=239616, episode_reward=-146.41 +/- 27.77
Episode length: 72.90 +/- 11.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.9     |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 239616   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.5     |
|    ep_rew_mean     | -130     |
| time/              |          |
|    fps             | 504      |
|    iterations      | 15       |
|    time_elapsed    | 487      |
|    total_timesteps | 245760   |
---------------------------------
Eval num_timesteps=249600, episode_reward=-124.81 +/- 19.70
Episode length: 73.40 +/- 13.36
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 73.4      |
|    mean_reward          | -125      |
| time/                   |           |
|    total_timesteps      | 249600    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 2.56      |
|    n_updates            | 60        |
|    policy_gradient_loss | -6.91e-11 |
|    value_loss           | 1.26      |
---------------------------------------
Eval num_timesteps=259584, episode_reward=-123.19 +/- 27.31
Episode length: 63.10 +/- 9.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 63.1     |
|    mean_reward     | -123     |
| time/              |          |
|    total_timesteps | 259584   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.4     |
|    ep_rew_mean     | -134     |
| time/              |          |
|    fps             | 499      |
|    iterations      | 16       |
|    time_elapsed    | 524      |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=269568, episode_reward=-142.35 +/- 30.05
Episode length: 69.40 +/- 13.54
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 69.4     |
|    mean_reward          | -142     |
| time/                   |          |
|    total_timesteps      | 269568   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.966    |
|    n_updates            | 64       |
|    policy_gradient_loss | 3.17e-10 |
|    value_loss           | 1.27     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.8     |
|    ep_rew_mean     | -130     |
| time/              |          |
|    fps             | 496      |
|    iterations      | 17       |
|    time_elapsed    | 561      |
|    total_timesteps | 278528   |
---------------------------------
Eval num_timesteps=279552, episode_reward=-134.06 +/- 30.03
Episode length: 65.30 +/- 8.28
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 65.3     |
|    mean_reward          | -134     |
| time/                   |          |
|    total_timesteps      | 279552   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 1.77     |
|    n_updates            | 68       |
|    policy_gradient_loss | 8.08e-10 |
|    value_loss           | 1.25     |
--------------------------------------
Eval num_timesteps=289536, episode_reward=-132.00 +/- 19.82
Episode length: 66.50 +/- 12.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 66.5     |
|    mean_reward     | -132     |
| time/              |          |
|    total_timesteps | 289536   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.5     |
|    ep_rew_mean     | -135     |
| time/              |          |
|    fps             | 489      |
|    iterations      | 18       |
|    time_elapsed    | 602      |
|    total_timesteps | 294912   |
---------------------------------
Eval num_timesteps=299520, episode_reward=-132.47 +/- 33.67
Episode length: 70.70 +/- 5.93
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 70.7     |
|    mean_reward          | -132     |
| time/                   |          |
|    total_timesteps      | 299520   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.985    |
|    n_updates            | 72       |
|    policy_gradient_loss | 2.02e-10 |
|    value_loss           | 1.35     |
--------------------------------------
Eval num_timesteps=309504, episode_reward=-120.04 +/- 39.66
Episode length: 70.60 +/- 12.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.6     |
|    mean_reward     | -120     |
| time/              |          |
|    total_timesteps | 309504   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71       |
|    ep_rew_mean     | -131     |
| time/              |          |
|    fps             | 487      |
|    iterations      | 19       |
|    time_elapsed    | 638      |
|    total_timesteps | 311296   |
---------------------------------
Eval num_timesteps=319488, episode_reward=-139.51 +/- 22.02
Episode length: 67.60 +/- 9.76
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 67.6     |
|    mean_reward          | -140     |
| time/                   |          |
|    total_timesteps      | 319488   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.893    |
|    n_updates            | 76       |
|    policy_gradient_loss | 3.11e-10 |
|    value_loss           | 1.27     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.6     |
|    ep_rew_mean     | -134     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 20       |
|    time_elapsed    | 675      |
|    total_timesteps | 327680   |
---------------------------------
Eval num_timesteps=329472, episode_reward=-145.13 +/- 30.35
Episode length: 70.20 +/- 12.51
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 70.2      |
|    mean_reward          | -145      |
| time/                   |           |
|    total_timesteps      | 329472    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.819     |
|    loss                 | 0.482     |
|    n_updates            | 80        |
|    policy_gradient_loss | -9.28e-11 |
|    value_loss           | 1.25      |
---------------------------------------
Eval num_timesteps=339456, episode_reward=-105.53 +/- 60.03
Episode length: 70.30 +/- 15.49
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.3     |
|    mean_reward     | -106     |
| time/              |          |
|    total_timesteps | 339456   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.4     |
|    ep_rew_mean     | -126     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 21       |
|    time_elapsed    | 713      |
|    total_timesteps | 344064   |
---------------------------------
Eval num_timesteps=349440, episode_reward=-121.22 +/- 40.44
Episode length: 66.80 +/- 9.96
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 66.8     |
|    mean_reward          | -121     |
| time/                   |          |
|    total_timesteps      | 349440   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.97     |
|    n_updates            | 84       |
|    policy_gradient_loss | -2.3e-10 |
|    value_loss           | 1.28     |
--------------------------------------
Eval num_timesteps=359424, episode_reward=-125.06 +/- 51.59
Episode length: 75.00 +/- 12.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75       |
|    mean_reward     | -125     |
| time/              |          |
|    total_timesteps | 359424   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70       |
|    ep_rew_mean     | -135     |
| time/              |          |
|    fps             | 479      |
|    iterations      | 22       |
|    time_elapsed    | 751      |
|    total_timesteps | 360448   |
---------------------------------
Eval num_timesteps=369408, episode_reward=-145.32 +/- 26.25
Episode length: 68.20 +/- 10.01
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 68.2      |
|    mean_reward          | -145      |
| time/                   |           |
|    total_timesteps      | 369408    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.819     |
|    loss                 | 0.893     |
|    n_updates            | 88        |
|    policy_gradient_loss | 4.74e-10  |
|    value_loss           | 1.24      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.2     |
|    ep_rew_mean     | -132     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 23       |
|    time_elapsed    | 788      |
|    total_timesteps | 376832   |
---------------------------------
Eval num_timesteps=379392, episode_reward=-135.22 +/- 15.40
Episode length: 63.90 +/- 10.76
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 63.9      |
|    mean_reward          | -135      |
| time/                   |           |
|    total_timesteps      | 379392    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 1.79e-07  |
|    learning_rate        | 0.819     |
|    loss                 | 1.09      |
|    n_updates            | 92        |
|    policy_gradient_loss | -8.25e-10 |
|    value_loss           | 1.26      |
---------------------------------------
Eval num_timesteps=389376, episode_reward=-135.16 +/- 25.40
Episode length: 69.50 +/- 11.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.5     |
|    mean_reward     | -135     |
| time/              |          |
|    total_timesteps | 389376   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.8     |
|    ep_rew_mean     | -136     |
| time/              |          |
|    fps             | 475      |
|    iterations      | 24       |
|    time_elapsed    | 827      |
|    total_timesteps | 393216   |
---------------------------------
Eval num_timesteps=399360, episode_reward=-132.40 +/- 44.29
Episode length: 70.10 +/- 13.18
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 70.1     |
|    mean_reward          | -132     |
| time/                   |          |
|    total_timesteps      | 399360   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.546    |
|    n_updates            | 96       |
|    policy_gradient_loss | 3.02e-10 |
|    value_loss           | 1.26     |
--------------------------------------
Eval num_timesteps=409344, episode_reward=-126.56 +/- 16.71
Episode length: 67.80 +/- 10.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67.8     |
|    mean_reward     | -127     |
| time/              |          |
|    total_timesteps | 409344   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.3     |
|    ep_rew_mean     | -133     |
| time/              |          |
|    fps             | 476      |
|    iterations      | 25       |
|    time_elapsed    | 858      |
|    total_timesteps | 409600   |
---------------------------------
Eval num_timesteps=419328, episode_reward=-134.79 +/- 25.66
Episode length: 69.30 +/- 12.69
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 69.3     |
|    mean_reward          | -135     |
| time/                   |          |
|    total_timesteps      | 419328   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | -0.612   |
|    learning_rate        | 0.819    |
|    loss                 | 1.02     |
|    n_updates            | 100      |
|    policy_gradient_loss | 3.02e-10 |
|    value_loss           | 1.48     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68       |
|    ep_rew_mean     | -132     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 26       |
|    time_elapsed    | 891      |
|    total_timesteps | 425984   |
---------------------------------
Eval num_timesteps=429312, episode_reward=-129.78 +/- 31.68
Episode length: 73.80 +/- 13.27
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 73.8      |
|    mean_reward          | -130      |
| time/                   |           |
|    total_timesteps      | 429312    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.819     |
|    loss                 | 1.06      |
|    n_updates            | 104       |
|    policy_gradient_loss | -1.84e-10 |
|    value_loss           | 1.22      |
---------------------------------------
Eval num_timesteps=439296, episode_reward=-141.06 +/- 31.41
Episode length: 69.10 +/- 8.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.1     |
|    mean_reward     | -141     |
| time/              |          |
|    total_timesteps | 439296   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72.4     |
|    ep_rew_mean     | -137     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 27       |
|    time_elapsed    | 926      |
|    total_timesteps | 442368   |
---------------------------------
Eval num_timesteps=449280, episode_reward=-128.74 +/- 27.35
Episode length: 70.40 +/- 14.16
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 70.4     |
|    mean_reward          | -129     |
| time/                   |          |
|    total_timesteps      | 449280   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.536    |
|    n_updates            | 108      |
|    policy_gradient_loss | 5.23e-12 |
|    value_loss           | 1.21     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.4     |
|    ep_rew_mean     | -130     |
| time/              |          |
|    fps             | 473      |
|    iterations      | 28       |
|    time_elapsed    | 968      |
|    total_timesteps | 458752   |
---------------------------------
Eval num_timesteps=459264, episode_reward=-117.75 +/- 46.34
Episode length: 69.60 +/- 12.10
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 69.6     |
|    mean_reward          | -118     |
| time/                   |          |
|    total_timesteps      | 459264   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.91     |
|    n_updates            | 112      |
|    policy_gradient_loss | 1.11e-10 |
|    value_loss           | 1.27     |
--------------------------------------
Eval num_timesteps=469248, episode_reward=-145.72 +/- 22.01
Episode length: 71.60 +/- 11.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.6     |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 469248   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 67.8     |
|    ep_rew_mean     | -126     |
| time/              |          |
|    fps             | 471      |
|    iterations      | 29       |
|    time_elapsed    | 1007     |
|    total_timesteps | 475136   |
---------------------------------
Eval num_timesteps=479232, episode_reward=-149.56 +/- 18.26
Episode length: 72.20 +/- 11.66
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 72.2      |
|    mean_reward          | -150      |
| time/                   |           |
|    total_timesteps      | 479232    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.969     |
|    n_updates            | 116       |
|    policy_gradient_loss | -1.38e-10 |
|    value_loss           | 1.27      |
---------------------------------------
Eval num_timesteps=489216, episode_reward=-108.11 +/- 69.63
Episode length: 79.70 +/- 13.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 79.7     |
|    mean_reward     | -108     |
| time/              |          |
|    total_timesteps | 489216   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72.4     |
|    ep_rew_mean     | -135     |
| time/              |          |
|    fps             | 471      |
|    iterations      | 30       |
|    time_elapsed    | 1042     |
|    total_timesteps | 491520   |
---------------------------------
Eval num_timesteps=499200, episode_reward=-139.23 +/- 16.44
Episode length: 76.30 +/- 10.56
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 76.3     |
|    mean_reward          | -139     |
| time/                   |          |
|    total_timesteps      | 499200   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 1.05     |
|    n_updates            | 120      |
|    policy_gradient_loss | -4.2e-10 |
|    value_loss           | 1.17     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.2     |
|    ep_rew_mean     | -128     |
| time/              |          |
|    fps             | 472      |
|    iterations      | 31       |
|    time_elapsed    | 1075     |
|    total_timesteps | 507904   |
---------------------------------
Eval num_timesteps=509184, episode_reward=-120.40 +/- 44.93
Episode length: 70.40 +/- 14.19
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 70.4      |
|    mean_reward          | -120      |
| time/                   |           |
|    total_timesteps      | 509184    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 1.02      |
|    n_updates            | 124       |
|    policy_gradient_loss | -6.27e-10 |
|    value_loss           | 1.26      |
---------------------------------------
Eval num_timesteps=519168, episode_reward=-119.06 +/- 29.72
Episode length: 65.50 +/- 9.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 65.5     |
|    mean_reward     | -119     |
| time/              |          |
|    total_timesteps | 519168   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.2     |
|    ep_rew_mean     | -135     |
| time/              |          |
|    fps             | 473      |
|    iterations      | 32       |
|    time_elapsed    | 1106     |
|    total_timesteps | 524288   |
---------------------------------
Eval num_timesteps=529152, episode_reward=-137.33 +/- 32.37
Episode length: 74.70 +/- 11.88
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 74.7      |
|    mean_reward          | -137      |
| time/                   |           |
|    total_timesteps      | 529152    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | -2.38e-07 |
|    learning_rate        | 0.819     |
|    loss                 | 1.43      |
|    n_updates            | 128       |
|    policy_gradient_loss | -2.9e-10  |
|    value_loss           | 1.3       |
---------------------------------------
Eval num_timesteps=539136, episode_reward=-127.95 +/- 43.17
Episode length: 75.50 +/- 6.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.5     |
|    mean_reward     | -128     |
| time/              |          |
|    total_timesteps | 539136   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.8     |
|    ep_rew_mean     | -127     |
| time/              |          |
|    fps             | 475      |
|    iterations      | 33       |
|    time_elapsed    | 1137     |
|    total_timesteps | 540672   |
---------------------------------
Eval num_timesteps=549120, episode_reward=-125.45 +/- 46.31
Episode length: 73.40 +/- 14.33
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 73.4     |
|    mean_reward          | -125     |
| time/                   |          |
|    total_timesteps      | 549120   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.971    |
|    n_updates            | 132      |
|    policy_gradient_loss | 2.56e-10 |
|    value_loss           | 1.31     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.9     |
|    ep_rew_mean     | -125     |
| time/              |          |
|    fps             | 476      |
|    iterations      | 34       |
|    time_elapsed    | 1168     |
|    total_timesteps | 557056   |
---------------------------------
Eval num_timesteps=559104, episode_reward=-130.94 +/- 28.94
Episode length: 71.50 +/- 9.17
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 71.5      |
|    mean_reward          | -131      |
| time/                   |           |
|    total_timesteps      | 559104    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 1.66      |
|    n_updates            | 136       |
|    policy_gradient_loss | -1.94e-10 |
|    value_loss           | 1.31      |
---------------------------------------
Eval num_timesteps=569088, episode_reward=-138.87 +/- 32.39
Episode length: 68.80 +/- 10.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.8     |
|    mean_reward     | -139     |
| time/              |          |
|    total_timesteps | 569088   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.1     |
|    ep_rew_mean     | -129     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 35       |
|    time_elapsed    | 1199     |
|    total_timesteps | 573440   |
---------------------------------
Eval num_timesteps=579072, episode_reward=-132.14 +/- 19.49
Episode length: 67.40 +/- 10.22
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 67.4      |
|    mean_reward          | -132      |
| time/                   |           |
|    total_timesteps      | 579072    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 1.87      |
|    n_updates            | 140       |
|    policy_gradient_loss | -1.34e-09 |
|    value_loss           | 1.36      |
---------------------------------------
Eval num_timesteps=589056, episode_reward=-142.17 +/- 20.12
Episode length: 70.50 +/- 12.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.5     |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 589056   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70       |
|    ep_rew_mean     | -131     |
| time/              |          |
|    fps             | 479      |
|    iterations      | 36       |
|    time_elapsed    | 1230     |
|    total_timesteps | 589824   |
---------------------------------
Eval num_timesteps=599040, episode_reward=-134.93 +/- 58.98
Episode length: 70.40 +/- 12.43
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 70.4      |
|    mean_reward          | -135      |
| time/                   |           |
|    total_timesteps      | 599040    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | -0.646    |
|    learning_rate        | 0.819     |
|    loss                 | 1.01      |
|    n_updates            | 144       |
|    policy_gradient_loss | -4.85e-10 |
|    value_loss           | 459       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.2     |
|    ep_rew_mean     | -129     |
| time/              |          |
|    fps             | 480      |
|    iterations      | 37       |
|    time_elapsed    | 1262     |
|    total_timesteps | 606208   |
---------------------------------
Eval num_timesteps=609024, episode_reward=-142.11 +/- 34.68
Episode length: 67.10 +/- 12.97
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 67.1      |
|    mean_reward          | -142      |
| time/                   |           |
|    total_timesteps      | 609024    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | -6.13     |
|    learning_rate        | 0.819     |
|    loss                 | 0.988     |
|    n_updates            | 148       |
|    policy_gradient_loss | -5.42e-10 |
|    value_loss           | 7.5       |
---------------------------------------
Eval num_timesteps=619008, episode_reward=-144.81 +/- 29.12
Episode length: 65.90 +/- 8.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 65.9     |
|    mean_reward     | -145     |
| time/              |          |
|    total_timesteps | 619008   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.8     |
|    ep_rew_mean     | -136     |
| time/              |          |
|    fps             | 480      |
|    iterations      | 38       |
|    time_elapsed    | 1295     |
|    total_timesteps | 622592   |
---------------------------------
Eval num_timesteps=628992, episode_reward=-151.91 +/- 68.64
Episode length: 68.80 +/- 17.81
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 68.8      |
|    mean_reward          | -152      |
| time/                   |           |
|    total_timesteps      | 628992    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 1.14      |
|    n_updates            | 152       |
|    policy_gradient_loss | -5.26e-10 |
|    value_loss           | 1.25      |
---------------------------------------
Eval num_timesteps=638976, episode_reward=-152.81 +/- 25.15
Episode length: 76.90 +/- 10.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.9     |
|    mean_reward     | -153     |
| time/              |          |
|    total_timesteps | 638976   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.8     |
|    ep_rew_mean     | -123     |
| time/              |          |
|    fps             | 481      |
|    iterations      | 39       |
|    time_elapsed    | 1327     |
|    total_timesteps | 638976   |
---------------------------------
Eval num_timesteps=648960, episode_reward=-133.79 +/- 41.27
Episode length: 68.30 +/- 16.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 68.3      |
|    mean_reward          | -134      |
| time/                   |           |
|    total_timesteps      | 648960    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.819     |
|    loss                 | 1.47      |
|    n_updates            | 156       |
|    policy_gradient_loss | 8.54e-10  |
|    value_loss           | 1.31      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.1     |
|    ep_rew_mean     | -131     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 40       |
|    time_elapsed    | 1358     |
|    total_timesteps | 655360   |
---------------------------------
Eval num_timesteps=658944, episode_reward=-129.10 +/- 19.74
Episode length: 68.00 +/- 12.58
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 68       |
|    mean_reward          | -129     |
| time/                   |          |
|    total_timesteps      | 658944   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | -9.02    |
|    learning_rate        | 0.819    |
|    loss                 | 1.04     |
|    n_updates            | 160      |
|    policy_gradient_loss | 7.13e-10 |
|    value_loss           | 9.43     |
--------------------------------------
Eval num_timesteps=668928, episode_reward=-117.18 +/- 46.63
Episode length: 68.20 +/- 14.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.2     |
|    mean_reward     | -117     |
| time/              |          |
|    total_timesteps | 668928   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.4     |
|    ep_rew_mean     | -131     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 41       |
|    time_elapsed    | 1388     |
|    total_timesteps | 671744   |
---------------------------------
Eval num_timesteps=678912, episode_reward=-134.59 +/- 48.54
Episode length: 77.10 +/- 16.84
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 77.1      |
|    mean_reward          | -135      |
| time/                   |           |
|    total_timesteps      | 678912    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.819     |
|    loss                 | 1.48      |
|    n_updates            | 164       |
|    policy_gradient_loss | -1.22e-09 |
|    value_loss           | 1.29      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.8     |
|    ep_rew_mean     | -133     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 42       |
|    time_elapsed    | 1422     |
|    total_timesteps | 688128   |
---------------------------------
Eval num_timesteps=688896, episode_reward=-144.48 +/- 61.64
Episode length: 69.60 +/- 10.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 69.6      |
|    mean_reward          | -144      |
| time/                   |           |
|    total_timesteps      | 688896    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.922     |
|    n_updates            | 168       |
|    policy_gradient_loss | -8.97e-10 |
|    value_loss           | 1.2       |
---------------------------------------
Eval num_timesteps=698880, episode_reward=-140.96 +/- 29.93
Episode length: 76.20 +/- 12.17
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.2     |
|    mean_reward     | -141     |
| time/              |          |
|    total_timesteps | 698880   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.3     |
|    ep_rew_mean     | -130     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 43       |
|    time_elapsed    | 1457     |
|    total_timesteps | 704512   |
---------------------------------
Eval num_timesteps=708864, episode_reward=-145.69 +/- 27.29
Episode length: 74.40 +/- 11.07
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 74.4     |
|    mean_reward          | -146     |
| time/                   |          |
|    total_timesteps      | 708864   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.98     |
|    n_updates            | 172      |
|    policy_gradient_loss | 3.14e-10 |
|    value_loss           | 1.27     |
--------------------------------------
Eval num_timesteps=718848, episode_reward=-127.72 +/- 47.89
Episode length: 72.30 +/- 14.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.3     |
|    mean_reward     | -128     |
| time/              |          |
|    total_timesteps | 718848   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.3     |
|    ep_rew_mean     | -133     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 44       |
|    time_elapsed    | 1494     |
|    total_timesteps | 720896   |
---------------------------------
Eval num_timesteps=728832, episode_reward=-117.95 +/- 30.85
Episode length: 65.40 +/- 12.20
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 65.4     |
|    mean_reward          | -118     |
| time/                   |          |
|    total_timesteps      | 728832   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | -14.4    |
|    learning_rate        | 0.819    |
|    loss                 | 1.04     |
|    n_updates            | 176      |
|    policy_gradient_loss | -7.1e-10 |
|    value_loss           | 8.39     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | -132     |
| time/              |          |
|    fps             | 480      |
|    iterations      | 45       |
|    time_elapsed    | 1533     |
|    total_timesteps | 737280   |
---------------------------------
Eval num_timesteps=738816, episode_reward=-127.64 +/- 55.53
Episode length: 73.00 +/- 13.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 73        |
|    mean_reward          | -128      |
| time/                   |           |
|    total_timesteps      | 738816    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 1.67      |
|    n_updates            | 180       |
|    policy_gradient_loss | -7.87e-11 |
|    value_loss           | 1.26      |
---------------------------------------
Eval num_timesteps=748800, episode_reward=-143.45 +/- 34.05
Episode length: 70.30 +/- 14.06
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.3     |
|    mean_reward     | -143     |
| time/              |          |
|    total_timesteps | 748800   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.3     |
|    ep_rew_mean     | -125     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 46       |
|    time_elapsed    | 1573     |
|    total_timesteps | 753664   |
---------------------------------
Eval num_timesteps=758784, episode_reward=-101.67 +/- 68.12
Episode length: 73.40 +/- 14.43
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 73.4     |
|    mean_reward          | -102     |
| time/                   |          |
|    total_timesteps      | 758784   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.499    |
|    n_updates            | 184      |
|    policy_gradient_loss | 2.4e-10  |
|    value_loss           | 1.3      |
--------------------------------------
Eval num_timesteps=768768, episode_reward=-147.80 +/- 68.01
Episode length: 71.80 +/- 12.11
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.8     |
|    mean_reward     | -148     |
| time/              |          |
|    total_timesteps | 768768   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70       |
|    ep_rew_mean     | -134     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 47       |
|    time_elapsed    | 1613     |
|    total_timesteps | 770048   |
---------------------------------
Eval num_timesteps=778752, episode_reward=-123.34 +/- 50.20
Episode length: 67.80 +/- 14.45
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 67.8     |
|    mean_reward          | -123     |
| time/                   |          |
|    total_timesteps      | 778752   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 1.52     |
|    n_updates            | 188      |
|    policy_gradient_loss | 3.06e-10 |
|    value_loss           | 1.27     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.6     |
|    ep_rew_mean     | -132     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 48       |
|    time_elapsed    | 1647     |
|    total_timesteps | 786432   |
---------------------------------
Eval num_timesteps=788736, episode_reward=-123.03 +/- 52.13
Episode length: 70.50 +/- 10.35
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 70.5     |
|    mean_reward          | -123     |
| time/                   |          |
|    total_timesteps      | 788736   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | -0.00109 |
|    learning_rate        | 0.819    |
|    loss                 | 2.35     |
|    n_updates            | 192      |
|    policy_gradient_loss | 4.4e-10  |
|    value_loss           | 1.32     |
--------------------------------------
Eval num_timesteps=798720, episode_reward=-116.15 +/- 35.34
Episode length: 74.30 +/- 15.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.3     |
|    mean_reward     | -116     |
| time/              |          |
|    total_timesteps | 798720   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.7     |
|    ep_rew_mean     | -135     |
| time/              |          |
|    fps             | 476      |
|    iterations      | 49       |
|    time_elapsed    | 1683     |
|    total_timesteps | 802816   |
---------------------------------
Eval num_timesteps=808704, episode_reward=-127.88 +/- 16.74
Episode length: 75.60 +/- 10.78
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 75.6     |
|    mean_reward          | -128     |
| time/                   |          |
|    total_timesteps      | 808704   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.485    |
|    n_updates            | 196      |
|    policy_gradient_loss | 6.51e-10 |
|    value_loss           | 1.16     |
--------------------------------------
Eval num_timesteps=818688, episode_reward=-113.37 +/- 60.10
Episode length: 62.80 +/- 12.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.8     |
|    mean_reward     | -113     |
| time/              |          |
|    total_timesteps | 818688   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.4     |
|    ep_rew_mean     | -121     |
| time/              |          |
|    fps             | 475      |
|    iterations      | 50       |
|    time_elapsed    | 1723     |
|    total_timesteps | 819200   |
---------------------------------
Eval num_timesteps=828672, episode_reward=-116.78 +/- 32.28
Episode length: 65.30 +/- 11.77
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 65.3     |
|    mean_reward          | -117     |
| time/                   |          |
|    total_timesteps      | 828672   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | -1.42    |
|    learning_rate        | 0.819    |
|    loss                 | 1.03     |
|    n_updates            | 200      |
|    policy_gradient_loss | 9.74e-10 |
|    value_loss           | 1.78     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.6     |
|    ep_rew_mean     | -128     |
| time/              |          |
|    fps             | 476      |
|    iterations      | 51       |
|    time_elapsed    | 1755     |
|    total_timesteps | 835584   |
---------------------------------
Eval num_timesteps=838656, episode_reward=-125.57 +/- 36.19
Episode length: 64.30 +/- 9.79
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 64.3      |
|    mean_reward          | -126      |
| time/                   |           |
|    total_timesteps      | 838656    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.819     |
|    loss                 | 0.896     |
|    n_updates            | 204       |
|    policy_gradient_loss | -7e-10    |
|    value_loss           | 1.21      |
---------------------------------------
Eval num_timesteps=848640, episode_reward=-133.48 +/- 25.83
Episode length: 68.70 +/- 11.75
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.7     |
|    mean_reward     | -133     |
| time/              |          |
|    total_timesteps | 848640   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.2     |
|    ep_rew_mean     | -137     |
| time/              |          |
|    fps             | 476      |
|    iterations      | 52       |
|    time_elapsed    | 1787     |
|    total_timesteps | 851968   |
---------------------------------
Eval num_timesteps=858624, episode_reward=-132.75 +/- 17.50
Episode length: 64.30 +/- 7.07
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 64.3      |
|    mean_reward          | -133      |
| time/                   |           |
|    total_timesteps      | 858624    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.819     |
|    loss                 | 1.13      |
|    n_updates            | 208       |
|    policy_gradient_loss | 3.37e-10  |
|    value_loss           | 1.32      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.8     |
|    ep_rew_mean     | -131     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 53       |
|    time_elapsed    | 1819     |
|    total_timesteps | 868352   |
---------------------------------
Eval num_timesteps=868608, episode_reward=-131.91 +/- 24.55
Episode length: 72.10 +/- 10.29
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 72.1     |
|    mean_reward          | -132     |
| time/                   |          |
|    total_timesteps      | 868608   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 1.79e-07 |
|    learning_rate        | 0.819    |
|    loss                 | 0.994    |
|    n_updates            | 212      |
|    policy_gradient_loss | 1.14e-10 |
|    value_loss           | 1.29     |
--------------------------------------
Eval num_timesteps=878592, episode_reward=-118.07 +/- 46.04
Episode length: 73.30 +/- 14.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.3     |
|    mean_reward     | -118     |
| time/              |          |
|    total_timesteps | 878592   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.3     |
|    ep_rew_mean     | -132     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 54       |
|    time_elapsed    | 1852     |
|    total_timesteps | 884736   |
---------------------------------
Eval num_timesteps=888576, episode_reward=-120.51 +/- 32.09
Episode length: 73.60 +/- 11.71
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 73.6     |
|    mean_reward          | -121     |
| time/                   |          |
|    total_timesteps      | 888576   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.987    |
|    n_updates            | 216      |
|    policy_gradient_loss | 5.06e-10 |
|    value_loss           | 1.25     |
--------------------------------------
Eval num_timesteps=898560, episode_reward=-135.19 +/- 25.00
Episode length: 68.10 +/- 10.43
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.1     |
|    mean_reward     | -135     |
| time/              |          |
|    total_timesteps | 898560   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 67.7     |
|    ep_rew_mean     | -130     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 55       |
|    time_elapsed    | 1884     |
|    total_timesteps | 901120   |
---------------------------------
Eval num_timesteps=908544, episode_reward=-136.05 +/- 18.29
Episode length: 66.80 +/- 10.35
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 66.8     |
|    mean_reward          | -136     |
| time/                   |          |
|    total_timesteps      | 908544   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.983    |
|    n_updates            | 220      |
|    policy_gradient_loss | -2.3e-10 |
|    value_loss           | 1.28     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.5     |
|    ep_rew_mean     | -133     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 56       |
|    time_elapsed    | 1917     |
|    total_timesteps | 917504   |
---------------------------------
Eval num_timesteps=918528, episode_reward=-137.84 +/- 22.42
Episode length: 69.30 +/- 10.95
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 69.3     |
|    mean_reward          | -138     |
| time/                   |          |
|    total_timesteps      | 918528   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 2.09     |
|    n_updates            | 224      |
|    policy_gradient_loss | 6.89e-10 |
|    value_loss           | 1.26     |
--------------------------------------
Eval num_timesteps=928512, episode_reward=-151.25 +/- 34.35
Episode length: 66.90 +/- 11.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 66.9     |
|    mean_reward     | -151     |
| time/              |          |
|    total_timesteps | 928512   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.6     |
|    ep_rew_mean     | -138     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 57       |
|    time_elapsed    | 1954     |
|    total_timesteps | 933888   |
---------------------------------
Eval num_timesteps=938496, episode_reward=-129.43 +/- 17.45
Episode length: 66.70 +/- 9.77
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 66.7     |
|    mean_reward          | -129     |
| time/                   |          |
|    total_timesteps      | 938496   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.886    |
|    n_updates            | 228      |
|    policy_gradient_loss | 1.22e-09 |
|    value_loss           | 1.21     |
--------------------------------------
Eval num_timesteps=948480, episode_reward=-140.44 +/- 29.12
Episode length: 64.40 +/- 10.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64.4     |
|    mean_reward     | -140     |
| time/              |          |
|    total_timesteps | 948480   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 66.4     |
|    ep_rew_mean     | -133     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 58       |
|    time_elapsed    | 1984     |
|    total_timesteps | 950272   |
---------------------------------
Eval num_timesteps=958464, episode_reward=-141.84 +/- 25.83
Episode length: 67.10 +/- 12.81
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 67.1     |
|    mean_reward          | -142     |
| time/                   |          |
|    total_timesteps      | 958464   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 1.05     |
|    n_updates            | 232      |
|    policy_gradient_loss | 1.53e-10 |
|    value_loss           | 1.25     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.8     |
|    ep_rew_mean     | -129     |
| time/              |          |
|    fps             | 479      |
|    iterations      | 59       |
|    time_elapsed    | 2016     |
|    total_timesteps | 966656   |
---------------------------------
Eval num_timesteps=968448, episode_reward=-138.79 +/- 36.64
Episode length: 75.00 +/- 10.56
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 75        |
|    mean_reward          | -139      |
| time/                   |           |
|    total_timesteps      | 968448    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.955     |
|    n_updates            | 236       |
|    policy_gradient_loss | -1.39e-09 |
|    value_loss           | 1.31      |
---------------------------------------
Eval num_timesteps=978432, episode_reward=-120.37 +/- 46.70
Episode length: 68.80 +/- 11.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.8     |
|    mean_reward     | -120     |
| time/              |          |
|    total_timesteps | 978432   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.9     |
|    ep_rew_mean     | -133     |
| time/              |          |
|    fps             | 479      |
|    iterations      | 60       |
|    time_elapsed    | 2049     |
|    total_timesteps | 983040   |
---------------------------------
Eval num_timesteps=988416, episode_reward=-114.59 +/- 50.71
Episode length: 65.20 +/- 9.59
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 65.2     |
|    mean_reward          | -115     |
| time/                   |          |
|    total_timesteps      | 988416   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 1.43     |
|    n_updates            | 240      |
|    policy_gradient_loss | 3.35e-10 |
|    value_loss           | 1.31     |
--------------------------------------
Eval num_timesteps=998400, episode_reward=-134.89 +/- 61.12
Episode length: 71.30 +/- 10.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.3     |
|    mean_reward     | -135     |
| time/              |          |
|    total_timesteps | 998400   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.1     |
|    ep_rew_mean     | -137     |
| time/              |          |
|    fps             | 480      |
|    iterations      | 61       |
|    time_elapsed    | 2080     |
|    total_timesteps | 999424   |
---------------------------------
Eval num_timesteps=1008384, episode_reward=-141.12 +/- 31.32
Episode length: 76.00 +/- 11.27
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 76       |
|    mean_reward          | -141     |
| time/                   |          |
|    total_timesteps      | 1008384  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 1.11     |
|    n_updates            | 244      |
|    policy_gradient_loss | 1.54e-10 |
|    value_loss           | 1.23     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.2     |
|    ep_rew_mean     | -130     |
| time/              |          |
|    fps             | 480      |
|    iterations      | 62       |
|    time_elapsed    | 2113     |
|    total_timesteps | 1015808  |
---------------------------------
Eval num_timesteps=1018368, episode_reward=-126.97 +/- 20.08
Episode length: 68.10 +/- 13.21
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 68.1     |
|    mean_reward          | -127     |
| time/                   |          |
|    total_timesteps      | 1018368  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.966    |
|    n_updates            | 248      |
|    policy_gradient_loss | 6.98e-10 |
|    value_loss           | 1.22     |
--------------------------------------
Eval num_timesteps=1028352, episode_reward=-125.21 +/- 25.53
Episode length: 68.30 +/- 10.71
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.3     |
|    mean_reward     | -125     |
| time/              |          |
|    total_timesteps | 1028352  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.2     |
|    ep_rew_mean     | -131     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 63       |
|    time_elapsed    | 2158     |
|    total_timesteps | 1032192  |
---------------------------------
Eval num_timesteps=1038336, episode_reward=-111.51 +/- 49.13
Episode length: 69.20 +/- 11.06
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 69.2     |
|    mean_reward          | -112     |
| time/                   |          |
|    total_timesteps      | 1038336  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 1.19e-07 |
|    learning_rate        | 0.819    |
|    loss                 | 0.927    |
|    n_updates            | 252      |
|    policy_gradient_loss | 2.41e-10 |
|    value_loss           | 1.23     |
--------------------------------------
Eval num_timesteps=1048320, episode_reward=-119.40 +/- 54.30
Episode length: 68.60 +/- 8.98
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.6     |
|    mean_reward     | -119     |
| time/              |          |
|    total_timesteps | 1048320  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.9     |
|    ep_rew_mean     | -133     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 64       |
|    time_elapsed    | 2193     |
|    total_timesteps | 1048576  |
---------------------------------
Eval num_timesteps=1058304, episode_reward=-138.13 +/- 21.92
Episode length: 68.50 +/- 11.06
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 68.5     |
|    mean_reward          | -138     |
| time/                   |          |
|    total_timesteps      | 1058304  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | -0.0238  |
|    learning_rate        | 0.819    |
|    loss                 | 1.06     |
|    n_updates            | 256      |
|    policy_gradient_loss | 2.51e-10 |
|    value_loss           | 1.33     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.2     |
|    ep_rew_mean     | -134     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 65       |
|    time_elapsed    | 2226     |
|    total_timesteps | 1064960  |
---------------------------------
Eval num_timesteps=1068288, episode_reward=-150.22 +/- 27.58
Episode length: 69.30 +/- 7.44
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 69.3     |
|    mean_reward          | -150     |
| time/                   |          |
|    total_timesteps      | 1068288  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | -2.59    |
|    learning_rate        | 0.819    |
|    loss                 | 0.856    |
|    n_updates            | 260      |
|    policy_gradient_loss | 1.26e-10 |
|    value_loss           | 2.22     |
--------------------------------------
Eval num_timesteps=1078272, episode_reward=-109.07 +/- 44.76
Episode length: 66.70 +/- 13.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 66.7     |
|    mean_reward     | -109     |
| time/              |          |
|    total_timesteps | 1078272  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.3     |
|    ep_rew_mean     | -128     |
| time/              |          |
|    fps             | 479      |
|    iterations      | 66       |
|    time_elapsed    | 2256     |
|    total_timesteps | 1081344  |
---------------------------------
Eval num_timesteps=1088256, episode_reward=-130.58 +/- 52.74
Episode length: 68.30 +/- 5.92
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 68.3     |
|    mean_reward          | -131     |
| time/                   |          |
|    total_timesteps      | 1088256  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 1.04     |
|    n_updates            | 264      |
|    policy_gradient_loss | 3.78e-10 |
|    value_loss           | 1.3      |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 67.7     |
|    ep_rew_mean     | -133     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 67       |
|    time_elapsed    | 2295     |
|    total_timesteps | 1097728  |
---------------------------------
Eval num_timesteps=1098240, episode_reward=-129.18 +/- 23.49
Episode length: 64.10 +/- 10.19
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 64.1      |
|    mean_reward          | -129      |
| time/                   |           |
|    total_timesteps      | 1098240   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 1.04      |
|    n_updates            | 268       |
|    policy_gradient_loss | -3.02e-10 |
|    value_loss           | 1.26      |
---------------------------------------
Eval num_timesteps=1108224, episode_reward=-125.16 +/- 47.73
Episode length: 71.80 +/- 13.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.8     |
|    mean_reward     | -125     |
| time/              |          |
|    total_timesteps | 1108224  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.9     |
|    ep_rew_mean     | -130     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 68       |
|    time_elapsed    | 2334     |
|    total_timesteps | 1114112  |
---------------------------------
Eval num_timesteps=1118208, episode_reward=-121.69 +/- 52.36
Episode length: 67.20 +/- 9.09
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 67.2     |
|    mean_reward          | -122     |
| time/                   |          |
|    total_timesteps      | 1118208  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | -3.29    |
|    learning_rate        | 0.819    |
|    loss                 | 1.11     |
|    n_updates            | 272      |
|    policy_gradient_loss | 3.88e-10 |
|    value_loss           | 2.4      |
--------------------------------------
Eval num_timesteps=1128192, episode_reward=-132.01 +/- 56.10
Episode length: 72.30 +/- 12.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.3     |
|    mean_reward     | -132     |
| time/              |          |
|    total_timesteps | 1128192  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.1     |
|    ep_rew_mean     | -133     |
| time/              |          |
|    fps             | 476      |
|    iterations      | 69       |
|    time_elapsed    | 2374     |
|    total_timesteps | 1130496  |
---------------------------------
Eval num_timesteps=1138176, episode_reward=-127.17 +/- 62.39
Episode length: 72.10 +/- 11.66
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 72.1      |
|    mean_reward          | -127      |
| time/                   |           |
|    total_timesteps      | 1138176   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 1.05      |
|    n_updates            | 276       |
|    policy_gradient_loss | -1.46e-09 |
|    value_loss           | 1.26      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.4     |
|    ep_rew_mean     | -132     |
| time/              |          |
|    fps             | 476      |
|    iterations      | 70       |
|    time_elapsed    | 2408     |
|    total_timesteps | 1146880  |
---------------------------------
Eval num_timesteps=1148160, episode_reward=-124.83 +/- 38.61
Episode length: 68.20 +/- 12.69
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 68.2     |
|    mean_reward          | -125     |
| time/                   |          |
|    total_timesteps      | 1148160  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 1.03     |
|    n_updates            | 280      |
|    policy_gradient_loss | -1e-10   |
|    value_loss           | 1.35     |
--------------------------------------
Eval num_timesteps=1158144, episode_reward=-127.21 +/- 27.63
Episode length: 71.80 +/- 11.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.8     |
|    mean_reward     | -127     |
| time/              |          |
|    total_timesteps | 1158144  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.7     |
|    ep_rew_mean     | -134     |
| time/              |          |
|    fps             | 475      |
|    iterations      | 71       |
|    time_elapsed    | 2445     |
|    total_timesteps | 1163264  |
---------------------------------
Eval num_timesteps=1168128, episode_reward=-116.10 +/- 36.08
Episode length: 76.00 +/- 12.07
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 76       |
|    mean_reward          | -116     |
| time/                   |          |
|    total_timesteps      | 1168128  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.997    |
|    n_updates            | 284      |
|    policy_gradient_loss | 5.01e-10 |
|    value_loss           | 1.31     |
--------------------------------------
Eval num_timesteps=1178112, episode_reward=-99.24 +/- 67.03
Episode length: 68.30 +/- 11.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.3     |
|    mean_reward     | -99.2    |
| time/              |          |
|    total_timesteps | 1178112  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.7     |
|    ep_rew_mean     | -135     |
| time/              |          |
|    fps             | 475      |
|    iterations      | 72       |
|    time_elapsed    | 2480     |
|    total_timesteps | 1179648  |
---------------------------------
Eval num_timesteps=1188096, episode_reward=-111.93 +/- 45.63
Episode length: 69.20 +/- 9.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 69.2      |
|    mean_reward          | -112      |
| time/                   |           |
|    total_timesteps      | 1188096   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.819     |
|    loss                 | 1.01      |
|    n_updates            | 288       |
|    policy_gradient_loss | -1.91e-10 |
|    value_loss           | 1.2       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.6     |
|    ep_rew_mean     | -128     |
| time/              |          |
|    fps             | 475      |
|    iterations      | 73       |
|    time_elapsed    | 2515     |
|    total_timesteps | 1196032  |
---------------------------------
Eval num_timesteps=1198080, episode_reward=-137.59 +/- 17.73
Episode length: 66.70 +/- 9.03
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 66.7     |
|    mean_reward          | -138     |
| time/                   |          |
|    total_timesteps      | 1198080  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 1.32     |
|    n_updates            | 292      |
|    policy_gradient_loss | 7.54e-10 |
|    value_loss           | 1.3      |
--------------------------------------
Eval num_timesteps=1208064, episode_reward=-120.26 +/- 18.09
Episode length: 68.40 +/- 11.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.4     |
|    mean_reward     | -120     |
| time/              |          |
|    total_timesteps | 1208064  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.2     |
|    ep_rew_mean     | -133     |
| time/              |          |
|    fps             | 474      |
|    iterations      | 74       |
|    time_elapsed    | 2553     |
|    total_timesteps | 1212416  |
---------------------------------
Eval num_timesteps=1218048, episode_reward=-145.40 +/- 26.81
Episode length: 71.20 +/- 12.46
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 71.2      |
|    mean_reward          | -145      |
| time/                   |           |
|    total_timesteps      | 1218048   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.978     |
|    n_updates            | 296       |
|    policy_gradient_loss | -7.73e-11 |
|    value_loss           | 1.31      |
---------------------------------------
Eval num_timesteps=1228032, episode_reward=-120.68 +/- 59.40
Episode length: 78.90 +/- 12.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.9     |
|    mean_reward     | -121     |
| time/              |          |
|    total_timesteps | 1228032  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.5     |
|    ep_rew_mean     | -135     |
| time/              |          |
|    fps             | 475      |
|    iterations      | 75       |
|    time_elapsed    | 2585     |
|    total_timesteps | 1228800  |
---------------------------------
Eval num_timesteps=1238016, episode_reward=-138.69 +/- 29.54
Episode length: 76.60 +/- 9.33
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 76.6     |
|    mean_reward          | -139     |
| time/                   |          |
|    total_timesteps      | 1238016  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 1.05     |
|    n_updates            | 300      |
|    policy_gradient_loss | 2.91e-10 |
|    value_loss           | 1.3      |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.2     |
|    ep_rew_mean     | -136     |
| time/              |          |
|    fps             | 476      |
|    iterations      | 76       |
|    time_elapsed    | 2615     |
|    total_timesteps | 1245184  |
---------------------------------
Eval num_timesteps=1248000, episode_reward=-122.28 +/- 30.58
Episode length: 66.50 +/- 12.89
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 66.5      |
|    mean_reward          | -122      |
| time/                   |           |
|    total_timesteps      | 1248000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 1.14      |
|    n_updates            | 304       |
|    policy_gradient_loss | -1.07e-10 |
|    value_loss           | 1.29      |
---------------------------------------
Eval num_timesteps=1257984, episode_reward=-133.40 +/- 28.54
Episode length: 69.30 +/- 8.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.3     |
|    mean_reward     | -133     |
| time/              |          |
|    total_timesteps | 1257984  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 67.5     |
|    ep_rew_mean     | -132     |
| time/              |          |
|    fps             | 475      |
|    iterations      | 77       |
|    time_elapsed    | 2652     |
|    total_timesteps | 1261568  |
---------------------------------
Eval num_timesteps=1267968, episode_reward=-144.01 +/- 26.99
Episode length: 74.50 +/- 10.91
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 74.5      |
|    mean_reward          | -144      |
| time/                   |           |
|    total_timesteps      | 1267968   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.95      |
|    n_updates            | 308       |
|    policy_gradient_loss | -8.02e-10 |
|    value_loss           | 1.16      |
---------------------------------------
Eval num_timesteps=1277952, episode_reward=-138.41 +/- 6.89
Episode length: 63.30 +/- 9.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 63.3     |
|    mean_reward     | -138     |
| time/              |          |
|    total_timesteps | 1277952  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.7     |
|    ep_rew_mean     | -132     |
| time/              |          |
|    fps             | 475      |
|    iterations      | 78       |
|    time_elapsed    | 2688     |
|    total_timesteps | 1277952  |
---------------------------------
Eval num_timesteps=1287936, episode_reward=-119.12 +/- 52.66
Episode length: 65.60 +/- 10.01
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 65.6      |
|    mean_reward          | -119      |
| time/                   |           |
|    total_timesteps      | 1287936   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.925     |
|    n_updates            | 312       |
|    policy_gradient_loss | -8.17e-10 |
|    value_loss           | 1.16      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72.2     |
|    ep_rew_mean     | -140     |
| time/              |          |
|    fps             | 476      |
|    iterations      | 79       |
|    time_elapsed    | 2718     |
|    total_timesteps | 1294336  |
---------------------------------
Eval num_timesteps=1297920, episode_reward=-107.93 +/- 42.90
Episode length: 73.20 +/- 14.67
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 73.2      |
|    mean_reward          | -108      |
| time/                   |           |
|    total_timesteps      | 1297920   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 1.79e-07  |
|    learning_rate        | 0.819     |
|    loss                 | 1.27      |
|    n_updates            | 316       |
|    policy_gradient_loss | -8.24e-10 |
|    value_loss           | 1.19      |
---------------------------------------
Eval num_timesteps=1307904, episode_reward=-168.37 +/- 52.62
Episode length: 77.80 +/- 14.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.8     |
|    mean_reward     | -168     |
| time/              |          |
|    total_timesteps | 1307904  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.2     |
|    ep_rew_mean     | -130     |
| time/              |          |
|    fps             | 476      |
|    iterations      | 80       |
|    time_elapsed    | 2750     |
|    total_timesteps | 1310720  |
---------------------------------
Eval num_timesteps=1317888, episode_reward=-148.22 +/- 23.56
Episode length: 73.00 +/- 10.84
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 73        |
|    mean_reward          | -148      |
| time/                   |           |
|    total_timesteps      | 1317888   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.819     |
|    loss                 | 0.969     |
|    n_updates            | 320       |
|    policy_gradient_loss | 3.71e-10  |
|    value_loss           | 1.28      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.8     |
|    ep_rew_mean     | -132     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 81       |
|    time_elapsed    | 2780     |
|    total_timesteps | 1327104  |
---------------------------------
Eval num_timesteps=1327872, episode_reward=-130.94 +/- 55.34
Episode length: 69.40 +/- 9.21
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 69.4      |
|    mean_reward          | -131      |
| time/                   |           |
|    total_timesteps      | 1327872   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | -2.73     |
|    learning_rate        | 0.819     |
|    loss                 | 0.923     |
|    n_updates            | 324       |
|    policy_gradient_loss | -3.06e-10 |
|    value_loss           | 2.22      |
---------------------------------------
Eval num_timesteps=1337856, episode_reward=-136.23 +/- 38.06
Episode length: 71.70 +/- 12.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.7     |
|    mean_reward     | -136     |
| time/              |          |
|    total_timesteps | 1337856  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.1     |
|    ep_rew_mean     | -130     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 82       |
|    time_elapsed    | 2810     |
|    total_timesteps | 1343488  |
---------------------------------
Eval num_timesteps=1347840, episode_reward=-119.19 +/- 54.89
Episode length: 66.70 +/- 7.60
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 66.7     |
|    mean_reward          | -119     |
| time/                   |          |
|    total_timesteps      | 1347840  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.977    |
|    n_updates            | 328      |
|    policy_gradient_loss | -1.9e-10 |
|    value_loss           | 1.21     |
--------------------------------------
Eval num_timesteps=1357824, episode_reward=-123.73 +/- 44.42
Episode length: 69.80 +/- 12.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.8     |
|    mean_reward     | -124     |
| time/              |          |
|    total_timesteps | 1357824  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.1     |
|    ep_rew_mean     | -131     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 83       |
|    time_elapsed    | 2843     |
|    total_timesteps | 1359872  |
---------------------------------
Eval num_timesteps=1367808, episode_reward=-116.34 +/- 33.12
Episode length: 69.90 +/- 13.87
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 69.9     |
|    mean_reward          | -116     |
| time/                   |          |
|    total_timesteps      | 1367808  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.95     |
|    n_updates            | 332      |
|    policy_gradient_loss | 6.24e-10 |
|    value_loss           | 1.24     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.7     |
|    ep_rew_mean     | -129     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 84       |
|    time_elapsed    | 2874     |
|    total_timesteps | 1376256  |
---------------------------------
Eval num_timesteps=1377792, episode_reward=-161.66 +/- 52.36
Episode length: 74.00 +/- 12.39
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 74       |
|    mean_reward          | -162     |
| time/                   |          |
|    total_timesteps      | 1377792  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 1.04     |
|    n_updates            | 336      |
|    policy_gradient_loss | 1.32e-09 |
|    value_loss           | 1.25     |
--------------------------------------
Eval num_timesteps=1387776, episode_reward=-136.06 +/- 28.78
Episode length: 66.80 +/- 9.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 66.8     |
|    mean_reward     | -136     |
| time/              |          |
|    total_timesteps | 1387776  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69       |
|    ep_rew_mean     | -137     |
| time/              |          |
|    fps             | 479      |
|    iterations      | 85       |
|    time_elapsed    | 2906     |
|    total_timesteps | 1392640  |
---------------------------------
Eval num_timesteps=1397760, episode_reward=-133.57 +/- 17.23
Episode length: 70.50 +/- 10.96
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 70.5     |
|    mean_reward          | -134     |
| time/                   |          |
|    total_timesteps      | 1397760  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.651    |
|    n_updates            | 340      |
|    policy_gradient_loss | 1.22e-10 |
|    value_loss           | 1.29     |
--------------------------------------
Eval num_timesteps=1407744, episode_reward=-138.29 +/- 26.85
Episode length: 66.80 +/- 10.32
---------------------------------
| eval/              |          |
|    mean_ep_length  | 66.8     |
|    mean_reward     | -138     |
| time/              |          |
|    total_timesteps | 1407744  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71       |
|    ep_rew_mean     | -136     |
| time/              |          |
|    fps             | 479      |
|    iterations      | 86       |
|    time_elapsed    | 2939     |
|    total_timesteps | 1409024  |
---------------------------------
Eval num_timesteps=1417728, episode_reward=-128.46 +/- 25.77
Episode length: 64.90 +/- 11.75
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 64.9     |
|    mean_reward          | -128     |
| time/                   |          |
|    total_timesteps      | 1417728  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 1.19e-07 |
|    learning_rate        | 0.819    |
|    loss                 | 2.14     |
|    n_updates            | 344      |
|    policy_gradient_loss | 7.46e-11 |
|    value_loss           | 1.27     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72.5     |
|    ep_rew_mean     | -134     |
| time/              |          |
|    fps             | 479      |
|    iterations      | 87       |
|    time_elapsed    | 2975     |
|    total_timesteps | 1425408  |
---------------------------------
Eval num_timesteps=1427712, episode_reward=-149.49 +/- 31.96
Episode length: 70.80 +/- 8.39
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 70.8     |
|    mean_reward          | -149     |
| time/                   |          |
|    total_timesteps      | 1427712  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 1.79e-07 |
|    learning_rate        | 0.819    |
|    loss                 | 0.994    |
|    n_updates            | 348      |
|    policy_gradient_loss | 6.73e-10 |
|    value_loss           | 1.3      |
--------------------------------------
Eval num_timesteps=1437696, episode_reward=-141.75 +/- 22.99
Episode length: 72.70 +/- 9.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.7     |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 1437696  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.6     |
|    ep_rew_mean     | -133     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 88       |
|    time_elapsed    | 3011     |
|    total_timesteps | 1441792  |
---------------------------------
Eval num_timesteps=1447680, episode_reward=-129.83 +/- 19.16
Episode length: 72.80 +/- 8.41
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 72.8      |
|    mean_reward          | -130      |
| time/                   |           |
|    total_timesteps      | 1447680   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.955     |
|    n_updates            | 352       |
|    policy_gradient_loss | -6.73e-10 |
|    value_loss           | 1.25      |
---------------------------------------
Eval num_timesteps=1457664, episode_reward=-131.30 +/- 49.66
Episode length: 72.30 +/- 9.94
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.3     |
|    mean_reward     | -131     |
| time/              |          |
|    total_timesteps | 1457664  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.3     |
|    ep_rew_mean     | -133     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 89       |
|    time_elapsed    | 3050     |
|    total_timesteps | 1458176  |
---------------------------------
Eval num_timesteps=1467648, episode_reward=-141.59 +/- 26.50
Episode length: 68.20 +/- 13.17
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 68.2      |
|    mean_reward          | -142      |
| time/                   |           |
|    total_timesteps      | 1467648   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.561     |
|    n_updates            | 356       |
|    policy_gradient_loss | -1.36e-10 |
|    value_loss           | 1.14      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.7     |
|    ep_rew_mean     | -136     |
| time/              |          |
|    fps             | 476      |
|    iterations      | 90       |
|    time_elapsed    | 3092     |
|    total_timesteps | 1474560  |
---------------------------------
Eval num_timesteps=1477632, episode_reward=-136.88 +/- 19.86
Episode length: 64.50 +/- 9.74
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 64.5      |
|    mean_reward          | -137      |
| time/                   |           |
|    total_timesteps      | 1477632   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.986     |
|    n_updates            | 360       |
|    policy_gradient_loss | -1.93e-09 |
|    value_loss           | 1.25      |
---------------------------------------
Eval num_timesteps=1487616, episode_reward=-130.63 +/- 36.20
Episode length: 73.90 +/- 11.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.9     |
|    mean_reward     | -131     |
| time/              |          |
|    total_timesteps | 1487616  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.8     |
|    ep_rew_mean     | -132     |
| time/              |          |
|    fps             | 476      |
|    iterations      | 91       |
|    time_elapsed    | 3126     |
|    total_timesteps | 1490944  |
---------------------------------
Eval num_timesteps=1497600, episode_reward=-141.89 +/- 48.16
Episode length: 76.40 +/- 12.68
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 76.4      |
|    mean_reward          | -142      |
| time/                   |           |
|    total_timesteps      | 1497600   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.819     |
|    loss                 | 1.7       |
|    n_updates            | 364       |
|    policy_gradient_loss | 4.29e-10  |
|    value_loss           | 1.22      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.4     |
|    ep_rew_mean     | -129     |
| time/              |          |
|    fps             | 476      |
|    iterations      | 92       |
|    time_elapsed    | 3160     |
|    total_timesteps | 1507328  |
---------------------------------
Eval num_timesteps=1507584, episode_reward=-136.94 +/- 33.60
Episode length: 72.80 +/- 11.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 72.8      |
|    mean_reward          | -137      |
| time/                   |           |
|    total_timesteps      | 1507584   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.819     |
|    loss                 | 1.03      |
|    n_updates            | 368       |
|    policy_gradient_loss | 7.37e-10  |
|    value_loss           | 1.29      |
---------------------------------------
Eval num_timesteps=1517568, episode_reward=-128.75 +/- 20.32
Episode length: 70.70 +/- 9.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.7     |
|    mean_reward     | -129     |
| time/              |          |
|    total_timesteps | 1517568  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.4     |
|    ep_rew_mean     | -132     |
| time/              |          |
|    fps             | 476      |
|    iterations      | 93       |
|    time_elapsed    | 3199     |
|    total_timesteps | 1523712  |
---------------------------------
Eval num_timesteps=1527552, episode_reward=-135.14 +/- 31.64
Episode length: 72.40 +/- 10.30
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 72.4     |
|    mean_reward          | -135     |
| time/                   |          |
|    total_timesteps      | 1527552  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 1.04     |
|    n_updates            | 372      |
|    policy_gradient_loss | 1.94e-10 |
|    value_loss           | 1.34     |
--------------------------------------
Eval num_timesteps=1537536, episode_reward=-146.30 +/- 23.65
Episode length: 70.90 +/- 10.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.9     |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 1537536  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.3     |
|    ep_rew_mean     | -134     |
| time/              |          |
|    fps             | 475      |
|    iterations      | 94       |
|    time_elapsed    | 3238     |
|    total_timesteps | 1540096  |
---------------------------------
Eval num_timesteps=1547520, episode_reward=-132.23 +/- 30.26
Episode length: 67.20 +/- 10.28
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 67.2     |
|    mean_reward          | -132     |
| time/                   |          |
|    total_timesteps      | 1547520  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 2.94     |
|    n_updates            | 376      |
|    policy_gradient_loss | 3.62e-10 |
|    value_loss           | 1.25     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.8     |
|    ep_rew_mean     | -138     |
| time/              |          |
|    fps             | 476      |
|    iterations      | 95       |
|    time_elapsed    | 3268     |
|    total_timesteps | 1556480  |
---------------------------------
Eval num_timesteps=1557504, episode_reward=-143.69 +/- 34.08
Episode length: 71.50 +/- 13.01
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 71.5     |
|    mean_reward          | -144     |
| time/                   |          |
|    total_timesteps      | 1557504  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 2.73     |
|    n_updates            | 380      |
|    policy_gradient_loss | 5.48e-10 |
|    value_loss           | 1.26     |
--------------------------------------
Eval num_timesteps=1567488, episode_reward=-151.15 +/- 16.31
Episode length: 67.40 +/- 10.77
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67.4     |
|    mean_reward     | -151     |
| time/              |          |
|    total_timesteps | 1567488  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.2     |
|    ep_rew_mean     | -130     |
| time/              |          |
|    fps             | 476      |
|    iterations      | 96       |
|    time_elapsed    | 3301     |
|    total_timesteps | 1572864  |
---------------------------------
Eval num_timesteps=1577472, episode_reward=-142.75 +/- 17.83
Episode length: 72.70 +/- 8.12
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 72.7     |
|    mean_reward          | -143     |
| time/                   |          |
|    total_timesteps      | 1577472  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.591    |
|    n_updates            | 384      |
|    policy_gradient_loss | -6.1e-10 |
|    value_loss           | 1.23     |
--------------------------------------
Eval num_timesteps=1587456, episode_reward=-124.24 +/- 48.50
Episode length: 68.80 +/- 11.78
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.8     |
|    mean_reward     | -124     |
| time/              |          |
|    total_timesteps | 1587456  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.9     |
|    ep_rew_mean     | -130     |
| time/              |          |
|    fps             | 476      |
|    iterations      | 97       |
|    time_elapsed    | 3337     |
|    total_timesteps | 1589248  |
---------------------------------
Eval num_timesteps=1597440, episode_reward=-108.97 +/- 57.35
Episode length: 73.90 +/- 19.43
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 73.9      |
|    mean_reward          | -109      |
| time/                   |           |
|    total_timesteps      | 1597440   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.565     |
|    n_updates            | 388       |
|    policy_gradient_loss | -1.76e-10 |
|    value_loss           | 1.17      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71       |
|    ep_rew_mean     | -134     |
| time/              |          |
|    fps             | 476      |
|    iterations      | 98       |
|    time_elapsed    | 3372     |
|    total_timesteps | 1605632  |
---------------------------------
Eval num_timesteps=1607424, episode_reward=-131.71 +/- 15.38
Episode length: 64.80 +/- 9.69
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 64.8     |
|    mean_reward          | -132     |
| time/                   |          |
|    total_timesteps      | 1607424  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 2.06     |
|    n_updates            | 392      |
|    policy_gradient_loss | 2.82e-10 |
|    value_loss           | 1.22     |
--------------------------------------
Eval num_timesteps=1617408, episode_reward=-121.12 +/- 31.44
Episode length: 72.40 +/- 15.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.4     |
|    mean_reward     | -121     |
| time/              |          |
|    total_timesteps | 1617408  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.7     |
|    ep_rew_mean     | -127     |
| time/              |          |
|    fps             | 476      |
|    iterations      | 99       |
|    time_elapsed    | 3402     |
|    total_timesteps | 1622016  |
---------------------------------
Eval num_timesteps=1627392, episode_reward=-147.25 +/- 29.70
Episode length: 71.80 +/- 14.45
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 71.8      |
|    mean_reward          | -147      |
| time/                   |           |
|    total_timesteps      | 1627392   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.96      |
|    n_updates            | 396       |
|    policy_gradient_loss | -7.81e-10 |
|    value_loss           | 1.37      |
---------------------------------------
Eval num_timesteps=1637376, episode_reward=-101.75 +/- 57.14
Episode length: 70.40 +/- 18.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.4     |
|    mean_reward     | -102     |
| time/              |          |
|    total_timesteps | 1637376  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.4     |
|    ep_rew_mean     | -133     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 100      |
|    time_elapsed    | 3433     |
|    total_timesteps | 1638400  |
---------------------------------
Eval num_timesteps=1647360, episode_reward=-113.95 +/- 56.55
Episode length: 66.00 +/- 11.98
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 66       |
|    mean_reward          | -114     |
| time/                   |          |
|    total_timesteps      | 1647360  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 1.08     |
|    n_updates            | 400      |
|    policy_gradient_loss | 1.08e-09 |
|    value_loss           | 1.26     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.6     |
|    ep_rew_mean     | -132     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 101      |
|    time_elapsed    | 3466     |
|    total_timesteps | 1654784  |
---------------------------------
Eval num_timesteps=1657344, episode_reward=-121.84 +/- 68.54
Episode length: 70.60 +/- 14.87
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 70.6      |
|    mean_reward          | -122      |
| time/                   |           |
|    total_timesteps      | 1657344   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.819     |
|    loss                 | 0.512     |
|    n_updates            | 404       |
|    policy_gradient_loss | 1.84e-10  |
|    value_loss           | 1.36      |
---------------------------------------
Eval num_timesteps=1667328, episode_reward=-133.18 +/- 44.49
Episode length: 68.50 +/- 11.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.5     |
|    mean_reward     | -133     |
| time/              |          |
|    total_timesteps | 1667328  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.1     |
|    ep_rew_mean     | -136     |
| time/              |          |
|    fps             | 477      |
|    iterations      | 102      |
|    time_elapsed    | 3497     |
|    total_timesteps | 1671168  |
---------------------------------
Eval num_timesteps=1677312, episode_reward=-119.75 +/- 58.61
Episode length: 68.00 +/- 9.49
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 68       |
|    mean_reward          | -120     |
| time/                   |          |
|    total_timesteps      | 1677312  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 1.19e-07 |
|    learning_rate        | 0.819    |
|    loss                 | 0.904    |
|    n_updates            | 408      |
|    policy_gradient_loss | 8.95e-10 |
|    value_loss           | 1.33     |
--------------------------------------
Eval num_timesteps=1687296, episode_reward=-158.74 +/- 47.46
Episode length: 78.10 +/- 11.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.1     |
|    mean_reward     | -159     |
| time/              |          |
|    total_timesteps | 1687296  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.4     |
|    ep_rew_mean     | -134     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 103      |
|    time_elapsed    | 3526     |
|    total_timesteps | 1687552  |
---------------------------------
Eval num_timesteps=1697280, episode_reward=-123.38 +/- 56.67
Episode length: 75.10 +/- 11.14
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 75.1      |
|    mean_reward          | -123      |
| time/                   |           |
|    total_timesteps      | 1697280   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.819     |
|    loss                 | 1.14      |
|    n_updates            | 412       |
|    policy_gradient_loss | -5.61e-10 |
|    value_loss           | 1.29      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.5     |
|    ep_rew_mean     | -124     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 104      |
|    time_elapsed    | 3558     |
|    total_timesteps | 1703936  |
---------------------------------
Eval num_timesteps=1707264, episode_reward=-159.82 +/- 66.69
Episode length: 72.60 +/- 16.91
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 72.6     |
|    mean_reward          | -160     |
| time/                   |          |
|    total_timesteps      | 1707264  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | -1.86    |
|    learning_rate        | 0.819    |
|    loss                 | 0.969    |
|    n_updates            | 416      |
|    policy_gradient_loss | 1.64e-10 |
|    value_loss           | 1.91     |
--------------------------------------
Eval num_timesteps=1717248, episode_reward=-95.36 +/- 60.72
Episode length: 69.90 +/- 12.48
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.9     |
|    mean_reward     | -95.4    |
| time/              |          |
|    total_timesteps | 1717248  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.6     |
|    ep_rew_mean     | -137     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 105      |
|    time_elapsed    | 3591     |
|    total_timesteps | 1720320  |
---------------------------------
Eval num_timesteps=1727232, episode_reward=-144.69 +/- 14.27
Episode length: 66.70 +/- 10.48
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 66.7     |
|    mean_reward          | -145     |
| time/                   |          |
|    total_timesteps      | 1727232  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.509    |
|    n_updates            | 420      |
|    policy_gradient_loss | 2.46e-10 |
|    value_loss           | 1.3      |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.5     |
|    ep_rew_mean     | -130     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 106      |
|    time_elapsed    | 3627     |
|    total_timesteps | 1736704  |
---------------------------------
Eval num_timesteps=1737216, episode_reward=-134.26 +/- 48.68
Episode length: 75.50 +/- 14.53
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 75.5      |
|    mean_reward          | -134      |
| time/                   |           |
|    total_timesteps      | 1737216   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | -14.9     |
|    learning_rate        | 0.819     |
|    loss                 | 0.534     |
|    n_updates            | 424       |
|    policy_gradient_loss | -9.02e-10 |
|    value_loss           | 15.7      |
---------------------------------------
Eval num_timesteps=1747200, episode_reward=-138.10 +/- 25.54
Episode length: 67.30 +/- 9.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67.3     |
|    mean_reward     | -138     |
| time/              |          |
|    total_timesteps | 1747200  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.1     |
|    ep_rew_mean     | -132     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 107      |
|    time_elapsed    | 3661     |
|    total_timesteps | 1753088  |
---------------------------------
Eval num_timesteps=1757184, episode_reward=-124.01 +/- 21.52
Episode length: 61.10 +/- 8.28
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 61.1      |
|    mean_reward          | -124      |
| time/                   |           |
|    total_timesteps      | 1757184   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | -0.152    |
|    learning_rate        | 0.819     |
|    loss                 | 1.2       |
|    n_updates            | 428       |
|    policy_gradient_loss | -7.59e-10 |
|    value_loss           | 1.32      |
---------------------------------------
Eval num_timesteps=1767168, episode_reward=-139.05 +/- 35.60
Episode length: 72.90 +/- 15.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.9     |
|    mean_reward     | -139     |
| time/              |          |
|    total_timesteps | 1767168  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.5     |
|    ep_rew_mean     | -131     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 108      |
|    time_elapsed    | 3697     |
|    total_timesteps | 1769472  |
---------------------------------
Eval num_timesteps=1777152, episode_reward=-136.10 +/- 24.37
Episode length: 64.00 +/- 9.70
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 64       |
|    mean_reward          | -136     |
| time/                   |          |
|    total_timesteps      | 1777152  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.477    |
|    n_updates            | 432      |
|    policy_gradient_loss | 3.68e-11 |
|    value_loss           | 1.23     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72.3     |
|    ep_rew_mean     | -130     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 109      |
|    time_elapsed    | 3734     |
|    total_timesteps | 1785856  |
---------------------------------
Eval num_timesteps=1787136, episode_reward=-137.33 +/- 14.61
Episode length: 68.70 +/- 10.75
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 68.7      |
|    mean_reward          | -137      |
| time/                   |           |
|    total_timesteps      | 1787136   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 1.39      |
|    n_updates            | 436       |
|    policy_gradient_loss | -5.11e-10 |
|    value_loss           | 1.21      |
---------------------------------------
Eval num_timesteps=1797120, episode_reward=-131.68 +/- 22.64
Episode length: 65.90 +/- 6.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 65.9     |
|    mean_reward     | -132     |
| time/              |          |
|    total_timesteps | 1797120  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.9     |
|    ep_rew_mean     | -135     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 110      |
|    time_elapsed    | 3766     |
|    total_timesteps | 1802240  |
---------------------------------
Eval num_timesteps=1807104, episode_reward=-107.29 +/- 61.10
Episode length: 74.20 +/- 10.98
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 74.2      |
|    mean_reward          | -107      |
| time/                   |           |
|    total_timesteps      | 1807104   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 1.04      |
|    n_updates            | 440       |
|    policy_gradient_loss | -7.77e-10 |
|    value_loss           | 1.27      |
---------------------------------------
Eval num_timesteps=1817088, episode_reward=-148.99 +/- 29.76
Episode length: 69.90 +/- 12.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.9     |
|    mean_reward     | -149     |
| time/              |          |
|    total_timesteps | 1817088  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71       |
|    ep_rew_mean     | -133     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 111      |
|    time_elapsed    | 3803     |
|    total_timesteps | 1818624  |
---------------------------------
Eval num_timesteps=1827072, episode_reward=-133.98 +/- 27.13
Episode length: 77.70 +/- 16.24
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 77.7      |
|    mean_reward          | -134      |
| time/                   |           |
|    total_timesteps      | 1827072   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 1.16      |
|    n_updates            | 444       |
|    policy_gradient_loss | -2.44e-10 |
|    value_loss           | 1.2       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69       |
|    ep_rew_mean     | -130     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 112      |
|    time_elapsed    | 3838     |
|    total_timesteps | 1835008  |
---------------------------------
Eval num_timesteps=1837056, episode_reward=-137.60 +/- 29.15
Episode length: 67.10 +/- 10.62
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 67.1      |
|    mean_reward          | -138      |
| time/                   |           |
|    total_timesteps      | 1837056   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.623     |
|    n_updates            | 448       |
|    policy_gradient_loss | -2.15e-10 |
|    value_loss           | 1.27      |
---------------------------------------
Eval num_timesteps=1847040, episode_reward=-128.73 +/- 16.48
Episode length: 67.70 +/- 11.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67.7     |
|    mean_reward     | -129     |
| time/              |          |
|    total_timesteps | 1847040  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.6     |
|    ep_rew_mean     | -132     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 113      |
|    time_elapsed    | 3871     |
|    total_timesteps | 1851392  |
---------------------------------
Eval num_timesteps=1857024, episode_reward=-132.04 +/- 21.84
Episode length: 66.10 +/- 10.04
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 66.1     |
|    mean_reward          | -132     |
| time/                   |          |
|    total_timesteps      | 1857024  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.999    |
|    n_updates            | 452      |
|    policy_gradient_loss | 5.25e-10 |
|    value_loss           | 1.22     |
--------------------------------------
Eval num_timesteps=1867008, episode_reward=-133.20 +/- 25.63
Episode length: 64.00 +/- 11.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64       |
|    mean_reward     | -133     |
| time/              |          |
|    total_timesteps | 1867008  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.8     |
|    ep_rew_mean     | -129     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 114      |
|    time_elapsed    | 3906     |
|    total_timesteps | 1867776  |
---------------------------------
Eval num_timesteps=1876992, episode_reward=-118.09 +/- 24.65
Episode length: 67.50 +/- 12.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 67.5      |
|    mean_reward          | -118      |
| time/                   |           |
|    total_timesteps      | 1876992   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 2.05      |
|    n_updates            | 456       |
|    policy_gradient_loss | -4.16e-10 |
|    value_loss           | 1.23      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.5     |
|    ep_rew_mean     | -123     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 115      |
|    time_elapsed    | 3938     |
|    total_timesteps | 1884160  |
---------------------------------
Eval num_timesteps=1886976, episode_reward=-130.80 +/- 88.21
Episode length: 72.70 +/- 13.09
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 72.7     |
|    mean_reward          | -131     |
| time/                   |          |
|    total_timesteps      | 1886976  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.935    |
|    n_updates            | 460      |
|    policy_gradient_loss | -2.4e-10 |
|    value_loss           | 1.34     |
--------------------------------------
Eval num_timesteps=1896960, episode_reward=-160.63 +/- 61.90
Episode length: 71.30 +/- 11.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.3     |
|    mean_reward     | -161     |
| time/              |          |
|    total_timesteps | 1896960  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.1     |
|    ep_rew_mean     | -124     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 116      |
|    time_elapsed    | 3969     |
|    total_timesteps | 1900544  |
---------------------------------
Eval num_timesteps=1906944, episode_reward=-129.56 +/- 19.50
Episode length: 66.10 +/- 8.77
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 66.1     |
|    mean_reward          | -130     |
| time/                   |          |
|    total_timesteps      | 1906944  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.543    |
|    n_updates            | 464      |
|    policy_gradient_loss | 3.92e-10 |
|    value_loss           | 1.29     |
--------------------------------------
Eval num_timesteps=1916928, episode_reward=-111.94 +/- 48.61
Episode length: 71.00 +/- 13.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71       |
|    mean_reward     | -112     |
| time/              |          |
|    total_timesteps | 1916928  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.7     |
|    ep_rew_mean     | -134     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 117      |
|    time_elapsed    | 4005     |
|    total_timesteps | 1916928  |
---------------------------------
Eval num_timesteps=1926912, episode_reward=-115.18 +/- 31.10
Episode length: 69.10 +/- 12.11
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 69.1     |
|    mean_reward          | -115     |
| time/                   |          |
|    total_timesteps      | 1926912  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 1.04     |
|    n_updates            | 468      |
|    policy_gradient_loss | 5.08e-10 |
|    value_loss           | 1.19     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.7     |
|    ep_rew_mean     | -138     |
| time/              |          |
|    fps             | 478      |
|    iterations      | 118      |
|    time_elapsed    | 4037     |
|    total_timesteps | 1933312  |
---------------------------------
Eval num_timesteps=1936896, episode_reward=-134.88 +/- 17.28
Episode length: 61.60 +/- 8.19
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 61.6      |
|    mean_reward          | -135      |
| time/                   |           |
|    total_timesteps      | 1936896   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | -2.38e-07 |
|    learning_rate        | 0.819     |
|    loss                 | 1.44      |
|    n_updates            | 472       |
|    policy_gradient_loss | 1.91e-11  |
|    value_loss           | 1.23      |
---------------------------------------
Eval num_timesteps=1946880, episode_reward=-134.36 +/- 50.45
Episode length: 69.90 +/- 15.67
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.9     |
|    mean_reward     | -134     |
| time/              |          |
|    total_timesteps | 1946880  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.3     |
|    ep_rew_mean     | -132     |
| time/              |          |
|    fps             | 479      |
|    iterations      | 119      |
|    time_elapsed    | 4067     |
|    total_timesteps | 1949696  |
---------------------------------
Eval num_timesteps=1956864, episode_reward=-125.16 +/- 49.20
Episode length: 78.00 +/- 12.81
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 78       |
|    mean_reward          | -125     |
| time/                   |          |
|    total_timesteps      | 1956864  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 2.41     |
|    n_updates            | 476      |
|    policy_gradient_loss | 7.97e-10 |
|    value_loss           | 1.28     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.6     |
|    ep_rew_mean     | -121     |
| time/              |          |
|    fps             | 479      |
|    iterations      | 120      |
|    time_elapsed    | 4097     |
|    total_timesteps | 1966080  |
---------------------------------
Eval num_timesteps=1966848, episode_reward=-140.67 +/- 19.34
Episode length: 74.40 +/- 8.81
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 74.4     |
|    mean_reward          | -141     |
| time/                   |          |
|    total_timesteps      | 1966848  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 1.36     |
|    n_updates            | 480      |
|    policy_gradient_loss | -1.7e-10 |
|    value_loss           | 1.27     |
--------------------------------------
Eval num_timesteps=1976832, episode_reward=-138.34 +/- 22.12
Episode length: 63.60 +/- 7.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 63.6     |
|    mean_reward     | -138     |
| time/              |          |
|    total_timesteps | 1976832  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.5     |
|    ep_rew_mean     | -137     |
| time/              |          |
|    fps             | 480      |
|    iterations      | 121      |
|    time_elapsed    | 4129     |
|    total_timesteps | 1982464  |
---------------------------------
Eval num_timesteps=1986816, episode_reward=-126.51 +/- 21.81
Episode length: 65.40 +/- 10.21
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 65.4      |
|    mean_reward          | -127      |
| time/                   |           |
|    total_timesteps      | 1986816   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.991     |
|    n_updates            | 484       |
|    policy_gradient_loss | -2.88e-10 |
|    value_loss           | 1.22      |
---------------------------------------
Eval num_timesteps=1996800, episode_reward=-134.68 +/- 29.91
Episode length: 70.20 +/- 14.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.2     |
|    mean_reward     | -135     |
| time/              |          |
|    total_timesteps | 1996800  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.2     |
|    ep_rew_mean     | -128     |
| time/              |          |
|    fps             | 479      |
|    iterations      | 122      |
|    time_elapsed    | 4165     |
|    total_timesteps | 1998848  |
---------------------------------
Eval num_timesteps=2006784, episode_reward=-112.50 +/- 53.23
Episode length: 68.10 +/- 9.75
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 68.1      |
|    mean_reward          | -112      |
| time/                   |           |
|    total_timesteps      | 2006784   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.528     |
|    n_updates            | 488       |
|    policy_gradient_loss | -1.58e-10 |
|    value_loss           | 1.24      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71       |
|    ep_rew_mean     | -130     |
| time/              |          |
|    fps             | 480      |
|    iterations      | 123      |
|    time_elapsed    | 4197     |
|    total_timesteps | 2015232  |
---------------------------------
Eval num_timesteps=2016768, episode_reward=-122.82 +/- 49.38
Episode length: 68.60 +/- 14.04
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 68.6      |
|    mean_reward          | -123      |
| time/                   |           |
|    total_timesteps      | 2016768   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.933     |
|    n_updates            | 492       |
|    policy_gradient_loss | -7.46e-11 |
|    value_loss           | 1.25      |
---------------------------------------
Eval num_timesteps=2026752, episode_reward=-130.79 +/- 21.84
Episode length: 67.50 +/- 11.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67.5     |
|    mean_reward     | -131     |
| time/              |          |
|    total_timesteps | 2026752  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.7     |
|    ep_rew_mean     | -130     |
| time/              |          |
|    fps             | 480      |
|    iterations      | 124      |
|    time_elapsed    | 4230     |
|    total_timesteps | 2031616  |
---------------------------------
Eval num_timesteps=2036736, episode_reward=-123.86 +/- 41.34
Episode length: 71.40 +/- 10.41
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 71.4      |
|    mean_reward          | -124      |
| time/                   |           |
|    total_timesteps      | 2036736   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.533     |
|    n_updates            | 496       |
|    policy_gradient_loss | -3.47e-10 |
|    value_loss           | 1.19      |
---------------------------------------
Eval num_timesteps=2046720, episode_reward=-141.54 +/- 22.77
Episode length: 68.50 +/- 10.47
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.5     |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 2046720  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.4     |
|    ep_rew_mean     | -127     |
| time/              |          |
|    fps             | 480      |
|    iterations      | 125      |
|    time_elapsed    | 4262     |
|    total_timesteps | 2048000  |
---------------------------------
Eval num_timesteps=2056704, episode_reward=-129.98 +/- 17.83
Episode length: 70.00 +/- 9.58
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 70       |
|    mean_reward          | -130     |
| time/                   |          |
|    total_timesteps      | 2056704  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 1.01     |
|    n_updates            | 500      |
|    policy_gradient_loss | 4.34e-10 |
|    value_loss           | 1.29     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.7     |
|    ep_rew_mean     | -135     |
| time/              |          |
|    fps             | 480      |
|    iterations      | 126      |
|    time_elapsed    | 4293     |
|    total_timesteps | 2064384  |
---------------------------------
Eval num_timesteps=2066688, episode_reward=-125.63 +/- 48.01
Episode length: 73.50 +/- 11.65
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 73.5     |
|    mean_reward          | -126     |
| time/                   |          |
|    total_timesteps      | 2066688  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 1.08     |
|    n_updates            | 504      |
|    policy_gradient_loss | 2.55e-10 |
|    value_loss           | 1.32     |
--------------------------------------
Eval num_timesteps=2076672, episode_reward=-137.44 +/- 19.70
Episode length: 70.40 +/- 9.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.4     |
|    mean_reward     | -137     |
| time/              |          |
|    total_timesteps | 2076672  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.1     |
|    ep_rew_mean     | -129     |
| time/              |          |
|    fps             | 480      |
|    iterations      | 127      |
|    time_elapsed    | 4326     |
|    total_timesteps | 2080768  |
---------------------------------
Eval num_timesteps=2086656, episode_reward=-136.54 +/- 65.59
Episode length: 77.60 +/- 11.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 77.6      |
|    mean_reward          | -137      |
| time/                   |           |
|    total_timesteps      | 2086656   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 1.01      |
|    n_updates            | 508       |
|    policy_gradient_loss | -7.73e-10 |
|    value_loss           | 1.29      |
---------------------------------------
Eval num_timesteps=2096640, episode_reward=-120.28 +/- 35.80
Episode length: 65.40 +/- 10.86
---------------------------------
| eval/              |          |
|    mean_ep_length  | 65.4     |
|    mean_reward     | -120     |
| time/              |          |
|    total_timesteps | 2096640  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.4     |
|    ep_rew_mean     | -139     |
| time/              |          |
|    fps             | 481      |
|    iterations      | 128      |
|    time_elapsed    | 4359     |
|    total_timesteps | 2097152  |
---------------------------------
Eval num_timesteps=2106624, episode_reward=-133.09 +/- 22.01
Episode length: 72.30 +/- 12.33
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 72.3     |
|    mean_reward          | -133     |
| time/                   |          |
|    total_timesteps      | 2106624  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 1.01     |
|    n_updates            | 512      |
|    policy_gradient_loss | 3.58e-10 |
|    value_loss           | 1.28     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70       |
|    ep_rew_mean     | -128     |
| time/              |          |
|    fps             | 480      |
|    iterations      | 129      |
|    time_elapsed    | 4394     |
|    total_timesteps | 2113536  |
---------------------------------
Eval num_timesteps=2116608, episode_reward=-145.06 +/- 42.80
Episode length: 70.10 +/- 13.33
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 70.1     |
|    mean_reward          | -145     |
| time/                   |          |
|    total_timesteps      | 2116608  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 1.03     |
|    n_updates            | 516      |
|    policy_gradient_loss | 6.45e-10 |
|    value_loss           | 1.28     |
--------------------------------------
Eval num_timesteps=2126592, episode_reward=-152.70 +/- 25.28
Episode length: 73.30 +/- 7.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.3     |
|    mean_reward     | -153     |
| time/              |          |
|    total_timesteps | 2126592  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.7     |
|    ep_rew_mean     | -132     |
| time/              |          |
|    fps             | 481      |
|    iterations      | 130      |
|    time_elapsed    | 4426     |
|    total_timesteps | 2129920  |
---------------------------------
Eval num_timesteps=2136576, episode_reward=-149.87 +/- 52.13
Episode length: 72.90 +/- 14.71
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 72.9      |
|    mean_reward          | -150      |
| time/                   |           |
|    total_timesteps      | 2136576   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.819     |
|    loss                 | 0.918     |
|    n_updates            | 520       |
|    policy_gradient_loss | 5.41e-10  |
|    value_loss           | 1.22      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.5     |
|    ep_rew_mean     | -132     |
| time/              |          |
|    fps             | 481      |
|    iterations      | 131      |
|    time_elapsed    | 4457     |
|    total_timesteps | 2146304  |
---------------------------------
Eval num_timesteps=2146560, episode_reward=-144.15 +/- 28.00
Episode length: 70.80 +/- 12.30
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 70.8     |
|    mean_reward          | -144     |
| time/                   |          |
|    total_timesteps      | 2146560  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.996    |
|    n_updates            | 524      |
|    policy_gradient_loss | 9.45e-10 |
|    value_loss           | 1.25     |
--------------------------------------
Eval num_timesteps=2156544, episode_reward=-121.20 +/- 45.62
Episode length: 69.40 +/- 11.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.4     |
|    mean_reward     | -121     |
| time/              |          |
|    total_timesteps | 2156544  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.1     |
|    ep_rew_mean     | -134     |
| time/              |          |
|    fps             | 481      |
|    iterations      | 132      |
|    time_elapsed    | 4490     |
|    total_timesteps | 2162688  |
---------------------------------
Eval num_timesteps=2166528, episode_reward=-117.59 +/- 50.73
Episode length: 72.80 +/- 7.55
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 72.8     |
|    mean_reward          | -118     |
| time/                   |          |
|    total_timesteps      | 2166528  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 1        |
|    n_updates            | 528      |
|    policy_gradient_loss | 6.09e-11 |
|    value_loss           | 1.24     |
--------------------------------------
Eval num_timesteps=2176512, episode_reward=-148.98 +/- 25.70
Episode length: 72.80 +/- 16.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.8     |
|    mean_reward     | -149     |
| time/              |          |
|    total_timesteps | 2176512  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70       |
|    ep_rew_mean     | -133     |
| time/              |          |
|    fps             | 481      |
|    iterations      | 133      |
|    time_elapsed    | 4524     |
|    total_timesteps | 2179072  |
---------------------------------
Eval num_timesteps=2186496, episode_reward=-135.56 +/- 43.98
Episode length: 68.10 +/- 9.30
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 68.1      |
|    mean_reward          | -136      |
| time/                   |           |
|    total_timesteps      | 2186496   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.819     |
|    loss                 | 1.43      |
|    n_updates            | 532       |
|    policy_gradient_loss | -1.02e-10 |
|    value_loss           | 1.27      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.2     |
|    ep_rew_mean     | -123     |
| time/              |          |
|    fps             | 481      |
|    iterations      | 134      |
|    time_elapsed    | 4559     |
|    total_timesteps | 2195456  |
---------------------------------
Eval num_timesteps=2196480, episode_reward=-144.76 +/- 70.68
Episode length: 72.30 +/- 16.46
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 72.3     |
|    mean_reward          | -145     |
| time/                   |          |
|    total_timesteps      | 2196480  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 1.79e-07 |
|    learning_rate        | 0.819    |
|    loss                 | 1.23     |
|    n_updates            | 536      |
|    policy_gradient_loss | 6.81e-10 |
|    value_loss           | 1.21     |
--------------------------------------
Eval num_timesteps=2206464, episode_reward=-103.36 +/- 71.75
Episode length: 72.30 +/- 12.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.3     |
|    mean_reward     | -103     |
| time/              |          |
|    total_timesteps | 2206464  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.7     |
|    ep_rew_mean     | -129     |
| time/              |          |
|    fps             | 481      |
|    iterations      | 135      |
|    time_elapsed    | 4593     |
|    total_timesteps | 2211840  |
---------------------------------
Eval num_timesteps=2216448, episode_reward=-131.73 +/- 30.60
Episode length: 71.70 +/- 10.54
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 71.7     |
|    mean_reward          | -132     |
| time/                   |          |
|    total_timesteps      | 2216448  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | -0.275   |
|    learning_rate        | 0.819    |
|    loss                 | 0.972    |
|    n_updates            | 540      |
|    policy_gradient_loss | 8.64e-11 |
|    value_loss           | 1.38     |
--------------------------------------
Eval num_timesteps=2226432, episode_reward=-122.85 +/- 27.93
Episode length: 69.60 +/- 10.40
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.6     |
|    mean_reward     | -123     |
| time/              |          |
|    total_timesteps | 2226432  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70       |
|    ep_rew_mean     | -125     |
| time/              |          |
|    fps             | 481      |
|    iterations      | 136      |
|    time_elapsed    | 4627     |
|    total_timesteps | 2228224  |
---------------------------------
Eval num_timesteps=2236416, episode_reward=-128.67 +/- 52.98
Episode length: 67.20 +/- 10.64
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 67.2      |
|    mean_reward          | -129      |
| time/                   |           |
|    total_timesteps      | 2236416   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.909     |
|    n_updates            | 544       |
|    policy_gradient_loss | -3.83e-10 |
|    value_loss           | 1.29      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.9     |
|    ep_rew_mean     | -129     |
| time/              |          |
|    fps             | 481      |
|    iterations      | 137      |
|    time_elapsed    | 4660     |
|    total_timesteps | 2244608  |
---------------------------------
Eval num_timesteps=2246400, episode_reward=-119.73 +/- 18.73
Episode length: 62.20 +/- 8.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 62.2      |
|    mean_reward          | -120      |
| time/                   |           |
|    total_timesteps      | 2246400   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 1.05      |
|    n_updates            | 548       |
|    policy_gradient_loss | -1.23e-09 |
|    value_loss           | 1.3       |
---------------------------------------
Eval num_timesteps=2256384, episode_reward=-151.46 +/- 74.01
Episode length: 71.80 +/- 13.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.8     |
|    mean_reward     | -151     |
| time/              |          |
|    total_timesteps | 2256384  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.1     |
|    ep_rew_mean     | -128     |
| time/              |          |
|    fps             | 481      |
|    iterations      | 138      |
|    time_elapsed    | 4691     |
|    total_timesteps | 2260992  |
---------------------------------
Eval num_timesteps=2266368, episode_reward=-159.24 +/- 77.17
Episode length: 74.80 +/- 13.01
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 74.8     |
|    mean_reward          | -159     |
| time/                   |          |
|    total_timesteps      | 2266368  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 1.19e-07 |
|    learning_rate        | 0.819    |
|    loss                 | 0.996    |
|    n_updates            | 552      |
|    policy_gradient_loss | 1.4e-09  |
|    value_loss           | 1.22     |
--------------------------------------
Eval num_timesteps=2276352, episode_reward=-145.62 +/- 28.77
Episode length: 62.50 +/- 7.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.5     |
|    mean_reward     | -146     |
| time/              |          |
|    total_timesteps | 2276352  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.7     |
|    ep_rew_mean     | -122     |
| time/              |          |
|    fps             | 481      |
|    iterations      | 139      |
|    time_elapsed    | 4728     |
|    total_timesteps | 2277376  |
---------------------------------
Eval num_timesteps=2286336, episode_reward=-163.92 +/- 63.74
Episode length: 74.80 +/- 12.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 74.8      |
|    mean_reward          | -164      |
| time/                   |           |
|    total_timesteps      | 2286336   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.542     |
|    n_updates            | 556       |
|    policy_gradient_loss | -4.36e-10 |
|    value_loss           | 1.29      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.5     |
|    ep_rew_mean     | -137     |
| time/              |          |
|    fps             | 481      |
|    iterations      | 140      |
|    time_elapsed    | 4761     |
|    total_timesteps | 2293760  |
---------------------------------
Eval num_timesteps=2296320, episode_reward=-152.58 +/- 44.94
Episode length: 74.60 +/- 14.06
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 74.6      |
|    mean_reward          | -153      |
| time/                   |           |
|    total_timesteps      | 2296320   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.902     |
|    n_updates            | 560       |
|    policy_gradient_loss | -4.26e-10 |
|    value_loss           | 1.33      |
---------------------------------------
Eval num_timesteps=2306304, episode_reward=-133.80 +/- 21.26
Episode length: 73.50 +/- 11.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.5     |
|    mean_reward     | -134     |
| time/              |          |
|    total_timesteps | 2306304  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.7     |
|    ep_rew_mean     | -129     |
| time/              |          |
|    fps             | 481      |
|    iterations      | 141      |
|    time_elapsed    | 4794     |
|    total_timesteps | 2310144  |
---------------------------------
Eval num_timesteps=2316288, episode_reward=-134.33 +/- 21.79
Episode length: 72.20 +/- 11.43
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 72.2     |
|    mean_reward          | -134     |
| time/                   |          |
|    total_timesteps      | 2316288  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 1.05     |
|    n_updates            | 564      |
|    policy_gradient_loss | 1.06e-10 |
|    value_loss           | 1.21     |
--------------------------------------
Eval num_timesteps=2326272, episode_reward=-125.33 +/- 49.72
Episode length: 76.20 +/- 11.60
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.2     |
|    mean_reward     | -125     |
| time/              |          |
|    total_timesteps | 2326272  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.6     |
|    ep_rew_mean     | -131     |
| time/              |          |
|    fps             | 481      |
|    iterations      | 142      |
|    time_elapsed    | 4831     |
|    total_timesteps | 2326528  |
---------------------------------
Eval num_timesteps=2336256, episode_reward=-124.45 +/- 50.87
Episode length: 72.30 +/- 12.45
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 72.3     |
|    mean_reward          | -124     |
| time/                   |          |
|    total_timesteps      | 2336256  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | -0.00383 |
|    learning_rate        | 0.819    |
|    loss                 | 2.14     |
|    n_updates            | 568      |
|    policy_gradient_loss | 5.34e-10 |
|    value_loss           | 1.17     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 67.2     |
|    ep_rew_mean     | -133     |
| time/              |          |
|    fps             | 480      |
|    iterations      | 143      |
|    time_elapsed    | 4871     |
|    total_timesteps | 2342912  |
---------------------------------
Eval num_timesteps=2346240, episode_reward=-100.14 +/- 47.65
Episode length: 66.60 +/- 13.44
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 66.6     |
|    mean_reward          | -100     |
| time/                   |          |
|    total_timesteps      | 2346240  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 1.03     |
|    n_updates            | 572      |
|    policy_gradient_loss | 6.73e-11 |
|    value_loss           | 1.29     |
--------------------------------------
Eval num_timesteps=2356224, episode_reward=-144.59 +/- 14.46
Episode length: 71.00 +/- 4.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71       |
|    mean_reward     | -145     |
| time/              |          |
|    total_timesteps | 2356224  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.7     |
|    ep_rew_mean     | -131     |
| time/              |          |
|    fps             | 481      |
|    iterations      | 144      |
|    time_elapsed    | 4902     |
|    total_timesteps | 2359296  |
---------------------------------
Eval num_timesteps=2366208, episode_reward=-143.97 +/- 25.12
Episode length: 73.30 +/- 9.81
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 73.3     |
|    mean_reward          | -144     |
| time/                   |          |
|    total_timesteps      | 2366208  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0.000256 |
|    learning_rate        | 0.819    |
|    loss                 | 1.04     |
|    n_updates            | 576      |
|    policy_gradient_loss | 6.07e-10 |
|    value_loss           | 315      |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.1     |
|    ep_rew_mean     | -140     |
| time/              |          |
|    fps             | 481      |
|    iterations      | 145      |
|    time_elapsed    | 4935     |
|    total_timesteps | 2375680  |
---------------------------------
Eval num_timesteps=2376192, episode_reward=-157.36 +/- 50.91
Episode length: 70.20 +/- 12.48
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 70.2      |
|    mean_reward          | -157      |
| time/                   |           |
|    total_timesteps      | 2376192   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.819     |
|    loss                 | 0.833     |
|    n_updates            | 580       |
|    policy_gradient_loss | -3.23e-10 |
|    value_loss           | 1.3       |
---------------------------------------
Eval num_timesteps=2386176, episode_reward=-136.23 +/- 18.61
Episode length: 69.10 +/- 14.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.1     |
|    mean_reward     | -136     |
| time/              |          |
|    total_timesteps | 2386176  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.7     |
|    ep_rew_mean     | -137     |
| time/              |          |
|    fps             | 481      |
|    iterations      | 146      |
|    time_elapsed    | 4967     |
|    total_timesteps | 2392064  |
---------------------------------
Eval num_timesteps=2396160, episode_reward=-145.17 +/- 44.90
Episode length: 67.90 +/- 8.75
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 67.9     |
|    mean_reward          | -145     |
| time/                   |          |
|    total_timesteps      | 2396160  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.949    |
|    n_updates            | 584      |
|    policy_gradient_loss | 9.7e-10  |
|    value_loss           | 1.27     |
--------------------------------------
Eval num_timesteps=2406144, episode_reward=-122.23 +/- 39.31
Episode length: 65.70 +/- 9.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 65.7     |
|    mean_reward     | -122     |
| time/              |          |
|    total_timesteps | 2406144  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.8     |
|    ep_rew_mean     | -135     |
| time/              |          |
|    fps             | 481      |
|    iterations      | 147      |
|    time_elapsed    | 4999     |
|    total_timesteps | 2408448  |
---------------------------------
Eval num_timesteps=2416128, episode_reward=-139.39 +/- 27.11
Episode length: 69.70 +/- 7.50
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 69.7      |
|    mean_reward          | -139      |
| time/                   |           |
|    total_timesteps      | 2416128   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.819     |
|    loss                 | 0.976     |
|    n_updates            | 588       |
|    policy_gradient_loss | -7.04e-10 |
|    value_loss           | 1.19      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.8     |
|    ep_rew_mean     | -136     |
| time/              |          |
|    fps             | 481      |
|    iterations      | 148      |
|    time_elapsed    | 5032     |
|    total_timesteps | 2424832  |
---------------------------------
Eval num_timesteps=2426112, episode_reward=-112.27 +/- 38.27
Episode length: 64.40 +/- 10.26
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 64.4      |
|    mean_reward          | -112      |
| time/                   |           |
|    total_timesteps      | 2426112   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 1.48      |
|    n_updates            | 592       |
|    policy_gradient_loss | -1.05e-10 |
|    value_loss           | 1.28      |
---------------------------------------
Eval num_timesteps=2436096, episode_reward=-137.37 +/- 29.37
Episode length: 70.70 +/- 12.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.7     |
|    mean_reward     | -137     |
| time/              |          |
|    total_timesteps | 2436096  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.6     |
|    ep_rew_mean     | -135     |
| time/              |          |
|    fps             | 481      |
|    iterations      | 149      |
|    time_elapsed    | 5065     |
|    total_timesteps | 2441216  |
---------------------------------
Eval num_timesteps=2446080, episode_reward=-115.02 +/- 36.13
Episode length: 65.70 +/- 11.98
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 65.7      |
|    mean_reward          | -115      |
| time/                   |           |
|    total_timesteps      | 2446080   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.961     |
|    n_updates            | 596       |
|    policy_gradient_loss | -2.46e-10 |
|    value_loss           | 1.3       |
---------------------------------------
Eval num_timesteps=2456064, episode_reward=-128.48 +/- 14.17
Episode length: 64.70 +/- 11.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64.7     |
|    mean_reward     | -128     |
| time/              |          |
|    total_timesteps | 2456064  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71       |
|    ep_rew_mean     | -131     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 150      |
|    time_elapsed    | 5097     |
|    total_timesteps | 2457600  |
---------------------------------
Eval num_timesteps=2466048, episode_reward=-110.61 +/- 51.70
Episode length: 74.20 +/- 15.15
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 74.2     |
|    mean_reward          | -111     |
| time/                   |          |
|    total_timesteps      | 2466048  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.959    |
|    n_updates            | 600      |
|    policy_gradient_loss | 3.44e-10 |
|    value_loss           | 1.22     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71       |
|    ep_rew_mean     | -134     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 151      |
|    time_elapsed    | 5129     |
|    total_timesteps | 2473984  |
---------------------------------
Eval num_timesteps=2476032, episode_reward=-122.47 +/- 25.78
Episode length: 69.30 +/- 11.24
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 69.3      |
|    mean_reward          | -122      |
| time/                   |           |
|    total_timesteps      | 2476032   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.9       |
|    n_updates            | 604       |
|    policy_gradient_loss | -7.26e-10 |
|    value_loss           | 1.25      |
---------------------------------------
Eval num_timesteps=2486016, episode_reward=-129.14 +/- 46.47
Episode length: 64.60 +/- 8.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64.6     |
|    mean_reward     | -129     |
| time/              |          |
|    total_timesteps | 2486016  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.6     |
|    ep_rew_mean     | -127     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 152      |
|    time_elapsed    | 5161     |
|    total_timesteps | 2490368  |
---------------------------------
Eval num_timesteps=2496000, episode_reward=-135.22 +/- 20.49
Episode length: 68.50 +/- 11.68
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 68.5      |
|    mean_reward          | -135      |
| time/                   |           |
|    total_timesteps      | 2496000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | -2.38e-07 |
|    learning_rate        | 0.819     |
|    loss                 | 1.01      |
|    n_updates            | 608       |
|    policy_gradient_loss | -9.53e-10 |
|    value_loss           | 1.27      |
---------------------------------------
Eval num_timesteps=2505984, episode_reward=-87.27 +/- 72.36
Episode length: 73.00 +/- 11.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73       |
|    mean_reward     | -87.3    |
| time/              |          |
|    total_timesteps | 2505984  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72.4     |
|    ep_rew_mean     | -131     |
| time/              |          |
|    fps             | 482      |
|    iterations      | 153      |
|    time_elapsed    | 5192     |
|    total_timesteps | 2506752  |
---------------------------------
Eval num_timesteps=2515968, episode_reward=-153.84 +/- 21.36
Episode length: 74.60 +/- 10.51
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 74.6      |
|    mean_reward          | -154      |
| time/                   |           |
|    total_timesteps      | 2515968   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 1.79e-07  |
|    learning_rate        | 0.819     |
|    loss                 | 1.7       |
|    n_updates            | 612       |
|    policy_gradient_loss | -3.31e-10 |
|    value_loss           | 1.23      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70       |
|    ep_rew_mean     | -130     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 154      |
|    time_elapsed    | 5223     |
|    total_timesteps | 2523136  |
---------------------------------
Eval num_timesteps=2525952, episode_reward=-138.42 +/- 26.97
Episode length: 68.20 +/- 15.17
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 68.2     |
|    mean_reward          | -138     |
| time/                   |          |
|    total_timesteps      | 2525952  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 1.5      |
|    n_updates            | 616      |
|    policy_gradient_loss | 3.53e-10 |
|    value_loss           | 1.26     |
--------------------------------------
Eval num_timesteps=2535936, episode_reward=-114.11 +/- 41.71
Episode length: 69.10 +/- 11.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.1     |
|    mean_reward     | -114     |
| time/              |          |
|    total_timesteps | 2535936  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.3     |
|    ep_rew_mean     | -125     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 155      |
|    time_elapsed    | 5254     |
|    total_timesteps | 2539520  |
---------------------------------
Eval num_timesteps=2545920, episode_reward=-145.84 +/- 30.66
Episode length: 70.20 +/- 10.71
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 70.2      |
|    mean_reward          | -146      |
| time/                   |           |
|    total_timesteps      | 2545920   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.819     |
|    loss                 | 1.03      |
|    n_updates            | 620       |
|    policy_gradient_loss | 8.73e-11  |
|    value_loss           | 1.23      |
---------------------------------------
Eval num_timesteps=2555904, episode_reward=-160.36 +/- 53.74
Episode length: 73.00 +/- 11.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73       |
|    mean_reward     | -160     |
| time/              |          |
|    total_timesteps | 2555904  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.2     |
|    ep_rew_mean     | -129     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 156      |
|    time_elapsed    | 5286     |
|    total_timesteps | 2555904  |
---------------------------------
Eval num_timesteps=2565888, episode_reward=-130.69 +/- 23.98
Episode length: 64.00 +/- 8.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 64        |
|    mean_reward          | -131      |
| time/                   |           |
|    total_timesteps      | 2565888   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 1.44      |
|    n_updates            | 624       |
|    policy_gradient_loss | -2.56e-10 |
|    value_loss           | 1.29      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70       |
|    ep_rew_mean     | -132     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 157      |
|    time_elapsed    | 5317     |
|    total_timesteps | 2572288  |
---------------------------------
Eval num_timesteps=2575872, episode_reward=-140.66 +/- 25.44
Episode length: 69.10 +/- 11.11
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 69.1     |
|    mean_reward          | -141     |
| time/                   |          |
|    total_timesteps      | 2575872  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 1        |
|    n_updates            | 628      |
|    policy_gradient_loss | 8.26e-10 |
|    value_loss           | 1.31     |
--------------------------------------
Eval num_timesteps=2585856, episode_reward=-130.41 +/- 23.98
Episode length: 62.90 +/- 6.96
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.9     |
|    mean_reward     | -130     |
| time/              |          |
|    total_timesteps | 2585856  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.2     |
|    ep_rew_mean     | -135     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 158      |
|    time_elapsed    | 5352     |
|    total_timesteps | 2588672  |
---------------------------------
Eval num_timesteps=2595840, episode_reward=-147.37 +/- 33.31
Episode length: 74.70 +/- 14.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 74.7      |
|    mean_reward          | -147      |
| time/                   |           |
|    total_timesteps      | 2595840   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 1.47      |
|    n_updates            | 632       |
|    policy_gradient_loss | -4.31e-10 |
|    value_loss           | 1.22      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.7     |
|    ep_rew_mean     | -134     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 159      |
|    time_elapsed    | 5386     |
|    total_timesteps | 2605056  |
---------------------------------
Eval num_timesteps=2605824, episode_reward=-150.07 +/- 17.35
Episode length: 67.10 +/- 12.68
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 67.1     |
|    mean_reward          | -150     |
| time/                   |          |
|    total_timesteps      | 2605824  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.446    |
|    n_updates            | 636      |
|    policy_gradient_loss | 1.29e-09 |
|    value_loss           | 1.15     |
--------------------------------------
Eval num_timesteps=2615808, episode_reward=-143.64 +/- 32.58
Episode length: 78.70 +/- 7.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.7     |
|    mean_reward     | -144     |
| time/              |          |
|    total_timesteps | 2615808  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.7     |
|    ep_rew_mean     | -135     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 160      |
|    time_elapsed    | 5418     |
|    total_timesteps | 2621440  |
---------------------------------
Eval num_timesteps=2625792, episode_reward=-125.82 +/- 55.52
Episode length: 77.20 +/- 11.48
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 77.2      |
|    mean_reward          | -126      |
| time/                   |           |
|    total_timesteps      | 2625792   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.942     |
|    n_updates            | 640       |
|    policy_gradient_loss | -6.59e-10 |
|    value_loss           | 1.28      |
---------------------------------------
Eval num_timesteps=2635776, episode_reward=-106.88 +/- 48.89
Episode length: 68.50 +/- 12.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.5     |
|    mean_reward     | -107     |
| time/              |          |
|    total_timesteps | 2635776  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.2     |
|    ep_rew_mean     | -131     |
| time/              |          |
|    fps             | 483      |
|    iterations      | 161      |
|    time_elapsed    | 5450     |
|    total_timesteps | 2637824  |
---------------------------------
Eval num_timesteps=2645760, episode_reward=-136.76 +/- 40.24
Episode length: 67.20 +/- 10.30
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 67.2      |
|    mean_reward          | -137      |
| time/                   |           |
|    total_timesteps      | 2645760   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.819     |
|    loss                 | 1.41      |
|    n_updates            | 644       |
|    policy_gradient_loss | 7.68e-10  |
|    value_loss           | 1.32      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72.2     |
|    ep_rew_mean     | -134     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 162      |
|    time_elapsed    | 5483     |
|    total_timesteps | 2654208  |
---------------------------------
Eval num_timesteps=2655744, episode_reward=-126.06 +/- 50.51
Episode length: 62.80 +/- 10.68
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 62.8     |
|    mean_reward          | -126     |
| time/                   |          |
|    total_timesteps      | 2655744  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 1.03     |
|    n_updates            | 648      |
|    policy_gradient_loss | 3.02e-10 |
|    value_loss           | 1.2      |
--------------------------------------
Eval num_timesteps=2665728, episode_reward=-115.59 +/- 47.79
Episode length: 65.30 +/- 9.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 65.3     |
|    mean_reward     | -116     |
| time/              |          |
|    total_timesteps | 2665728  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.8     |
|    ep_rew_mean     | -131     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 163      |
|    time_elapsed    | 5517     |
|    total_timesteps | 2670592  |
---------------------------------
Eval num_timesteps=2675712, episode_reward=-120.45 +/- 42.39
Episode length: 66.20 +/- 12.68
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 66.2      |
|    mean_reward          | -120      |
| time/                   |           |
|    total_timesteps      | 2675712   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 1.34      |
|    n_updates            | 652       |
|    policy_gradient_loss | -1.15e-09 |
|    value_loss           | 1.18      |
---------------------------------------
Eval num_timesteps=2685696, episode_reward=-113.98 +/- 54.91
Episode length: 68.30 +/- 11.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.3     |
|    mean_reward     | -114     |
| time/              |          |
|    total_timesteps | 2685696  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.7     |
|    ep_rew_mean     | -132     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 164      |
|    time_elapsed    | 5550     |
|    total_timesteps | 2686976  |
---------------------------------
Eval num_timesteps=2695680, episode_reward=-137.08 +/- 48.34
Episode length: 71.70 +/- 12.93
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 71.7      |
|    mean_reward          | -137      |
| time/                   |           |
|    total_timesteps      | 2695680   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.819     |
|    loss                 | 0.918     |
|    n_updates            | 656       |
|    policy_gradient_loss | 4.22e-10  |
|    value_loss           | 1.37      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.2     |
|    ep_rew_mean     | -132     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 165      |
|    time_elapsed    | 5583     |
|    total_timesteps | 2703360  |
---------------------------------
Eval num_timesteps=2705664, episode_reward=-124.97 +/- 45.01
Episode length: 68.00 +/- 12.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 68        |
|    mean_reward          | -125      |
| time/                   |           |
|    total_timesteps      | 2705664   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.819     |
|    loss                 | 0.575     |
|    n_updates            | 660       |
|    policy_gradient_loss | -7.56e-10 |
|    value_loss           | 1.3       |
---------------------------------------
Eval num_timesteps=2715648, episode_reward=-126.50 +/- 22.60
Episode length: 60.20 +/- 11.12
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.2     |
|    mean_reward     | -126     |
| time/              |          |
|    total_timesteps | 2715648  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.3     |
|    ep_rew_mean     | -130     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 166      |
|    time_elapsed    | 5616     |
|    total_timesteps | 2719744  |
---------------------------------
Eval num_timesteps=2725632, episode_reward=-125.75 +/- 23.31
Episode length: 71.70 +/- 12.83
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 71.7      |
|    mean_reward          | -126      |
| time/                   |           |
|    total_timesteps      | 2725632   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 1.18      |
|    n_updates            | 664       |
|    policy_gradient_loss | -3.64e-11 |
|    value_loss           | 1.23      |
---------------------------------------
Eval num_timesteps=2735616, episode_reward=-129.39 +/- 55.84
Episode length: 73.60 +/- 12.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.6     |
|    mean_reward     | -129     |
| time/              |          |
|    total_timesteps | 2735616  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.9     |
|    ep_rew_mean     | -130     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 167      |
|    time_elapsed    | 5651     |
|    total_timesteps | 2736128  |
---------------------------------
Eval num_timesteps=2745600, episode_reward=-124.12 +/- 44.25
Episode length: 68.10 +/- 7.29
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 68.1      |
|    mean_reward          | -124      |
| time/                   |           |
|    total_timesteps      | 2745600   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 1         |
|    n_updates            | 668       |
|    policy_gradient_loss | -1.58e-10 |
|    value_loss           | 1.3       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.6     |
|    ep_rew_mean     | -136     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 168      |
|    time_elapsed    | 5681     |
|    total_timesteps | 2752512  |
---------------------------------
Eval num_timesteps=2755584, episode_reward=-144.99 +/- 30.96
Episode length: 68.80 +/- 12.96
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 68.8      |
|    mean_reward          | -145      |
| time/                   |           |
|    total_timesteps      | 2755584   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 1.01      |
|    n_updates            | 672       |
|    policy_gradient_loss | -4.46e-10 |
|    value_loss           | 1.3       |
---------------------------------------
Eval num_timesteps=2765568, episode_reward=-159.03 +/- 62.09
Episode length: 78.60 +/- 14.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 78.6     |
|    mean_reward     | -159     |
| time/              |          |
|    total_timesteps | 2765568  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.2     |
|    ep_rew_mean     | -133     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 169      |
|    time_elapsed    | 5713     |
|    total_timesteps | 2768896  |
---------------------------------
Eval num_timesteps=2775552, episode_reward=-121.74 +/- 22.23
Episode length: 66.90 +/- 10.90
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 66.9      |
|    mean_reward          | -122      |
| time/                   |           |
|    total_timesteps      | 2775552   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.819     |
|    loss                 | 0.914     |
|    n_updates            | 676       |
|    policy_gradient_loss | -5.25e-10 |
|    value_loss           | 1.3       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.1     |
|    ep_rew_mean     | -130     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 170      |
|    time_elapsed    | 5745     |
|    total_timesteps | 2785280  |
---------------------------------
Eval num_timesteps=2785536, episode_reward=-131.30 +/- 68.86
Episode length: 73.50 +/- 15.05
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 73.5      |
|    mean_reward          | -131      |
| time/                   |           |
|    total_timesteps      | 2785536   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.819     |
|    loss                 | 0.962     |
|    n_updates            | 680       |
|    policy_gradient_loss | -1.07e-10 |
|    value_loss           | 1.29      |
---------------------------------------
Eval num_timesteps=2795520, episode_reward=-127.17 +/- 47.40
Episode length: 68.80 +/- 11.21
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.8     |
|    mean_reward     | -127     |
| time/              |          |
|    total_timesteps | 2795520  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.9     |
|    ep_rew_mean     | -128     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 171      |
|    time_elapsed    | 5778     |
|    total_timesteps | 2801664  |
---------------------------------
Eval num_timesteps=2805504, episode_reward=-131.26 +/- 33.01
Episode length: 67.90 +/- 14.43
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 67.9      |
|    mean_reward          | -131      |
| time/                   |           |
|    total_timesteps      | 2805504   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.819     |
|    loss                 | 0.961     |
|    n_updates            | 684       |
|    policy_gradient_loss | 9.16e-10  |
|    value_loss           | 1.22      |
---------------------------------------
Eval num_timesteps=2815488, episode_reward=-147.66 +/- 29.16
Episode length: 68.30 +/- 9.89
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.3     |
|    mean_reward     | -148     |
| time/              |          |
|    total_timesteps | 2815488  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.7     |
|    ep_rew_mean     | -128     |
| time/              |          |
|    fps             | 484      |
|    iterations      | 172      |
|    time_elapsed    | 5810     |
|    total_timesteps | 2818048  |
---------------------------------
Eval num_timesteps=2825472, episode_reward=-140.70 +/- 86.64
Episode length: 77.60 +/- 13.26
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 77.6      |
|    mean_reward          | -141      |
| time/                   |           |
|    total_timesteps      | 2825472   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.819     |
|    loss                 | 0.533     |
|    n_updates            | 688       |
|    policy_gradient_loss | 4.75e-10  |
|    value_loss           | 1.25      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.7     |
|    ep_rew_mean     | -128     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 173      |
|    time_elapsed    | 5841     |
|    total_timesteps | 2834432  |
---------------------------------
Eval num_timesteps=2835456, episode_reward=-133.57 +/- 23.78
Episode length: 63.60 +/- 8.42
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 63.6      |
|    mean_reward          | -134      |
| time/                   |           |
|    total_timesteps      | 2835456   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.819     |
|    loss                 | 0.967     |
|    n_updates            | 692       |
|    policy_gradient_loss | -2.18e-10 |
|    value_loss           | 1.29      |
---------------------------------------
Eval num_timesteps=2845440, episode_reward=-126.45 +/- 33.50
Episode length: 64.50 +/- 10.30
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64.5     |
|    mean_reward     | -126     |
| time/              |          |
|    total_timesteps | 2845440  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.2     |
|    ep_rew_mean     | -127     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 174      |
|    time_elapsed    | 5875     |
|    total_timesteps | 2850816  |
---------------------------------
Eval num_timesteps=2855424, episode_reward=-155.79 +/- 76.78
Episode length: 71.10 +/- 16.69
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 71.1     |
|    mean_reward          | -156     |
| time/                   |          |
|    total_timesteps      | 2855424  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 1.02     |
|    n_updates            | 696      |
|    policy_gradient_loss | 4.37e-10 |
|    value_loss           | 1.2      |
--------------------------------------
Eval num_timesteps=2865408, episode_reward=-158.70 +/- 40.81
Episode length: 70.90 +/- 13.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.9     |
|    mean_reward     | -159     |
| time/              |          |
|    total_timesteps | 2865408  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.4     |
|    ep_rew_mean     | -133     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 175      |
|    time_elapsed    | 5907     |
|    total_timesteps | 2867200  |
---------------------------------
Eval num_timesteps=2875392, episode_reward=-123.37 +/- 25.59
Episode length: 68.40 +/- 10.11
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 68.4      |
|    mean_reward          | -123      |
| time/                   |           |
|    total_timesteps      | 2875392   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 1.33      |
|    n_updates            | 700       |
|    policy_gradient_loss | -4.94e-10 |
|    value_loss           | 1.26      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.7     |
|    ep_rew_mean     | -135     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 176      |
|    time_elapsed    | 5941     |
|    total_timesteps | 2883584  |
---------------------------------
Eval num_timesteps=2885376, episode_reward=-147.39 +/- 22.73
Episode length: 69.10 +/- 11.03
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 69.1      |
|    mean_reward          | -147      |
| time/                   |           |
|    total_timesteps      | 2885376   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 1.33      |
|    n_updates            | 704       |
|    policy_gradient_loss | -2.73e-10 |
|    value_loss           | 1.19      |
---------------------------------------
Eval num_timesteps=2895360, episode_reward=-130.73 +/- 13.45
Episode length: 63.60 +/- 10.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 63.6     |
|    mean_reward     | -131     |
| time/              |          |
|    total_timesteps | 2895360  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.9     |
|    ep_rew_mean     | -137     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 177      |
|    time_elapsed    | 5975     |
|    total_timesteps | 2899968  |
---------------------------------
Eval num_timesteps=2905344, episode_reward=-128.83 +/- 26.06
Episode length: 71.50 +/- 7.63
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 71.5     |
|    mean_reward          | -129     |
| time/                   |          |
|    total_timesteps      | 2905344  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 1.02     |
|    n_updates            | 708      |
|    policy_gradient_loss | 6.28e-11 |
|    value_loss           | 1.33     |
--------------------------------------
Eval num_timesteps=2915328, episode_reward=-156.63 +/- 64.17
Episode length: 75.70 +/- 14.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.7     |
|    mean_reward     | -157     |
| time/              |          |
|    total_timesteps | 2915328  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71       |
|    ep_rew_mean     | -135     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 178      |
|    time_elapsed    | 6007     |
|    total_timesteps | 2916352  |
---------------------------------
Eval num_timesteps=2925312, episode_reward=-155.33 +/- 23.43
Episode length: 74.80 +/- 9.84
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 74.8      |
|    mean_reward          | -155      |
| time/                   |           |
|    total_timesteps      | 2925312   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.514     |
|    n_updates            | 712       |
|    policy_gradient_loss | -3.82e-10 |
|    value_loss           | 1.18      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.5     |
|    ep_rew_mean     | -128     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 179      |
|    time_elapsed    | 6040     |
|    total_timesteps | 2932736  |
---------------------------------
Eval num_timesteps=2935296, episode_reward=-142.45 +/- 25.05
Episode length: 69.00 +/- 12.60
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 69        |
|    mean_reward          | -142      |
| time/                   |           |
|    total_timesteps      | 2935296   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.819     |
|    loss                 | 0.529     |
|    n_updates            | 716       |
|    policy_gradient_loss | 1.34e-10  |
|    value_loss           | 1.3       |
---------------------------------------
Eval num_timesteps=2945280, episode_reward=-139.52 +/- 19.71
Episode length: 67.50 +/- 8.82
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67.5     |
|    mean_reward     | -140     |
| time/              |          |
|    total_timesteps | 2945280  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.7     |
|    ep_rew_mean     | -132     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 180      |
|    time_elapsed    | 6071     |
|    total_timesteps | 2949120  |
---------------------------------
Eval num_timesteps=2955264, episode_reward=-125.00 +/- 54.50
Episode length: 72.70 +/- 11.31
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 72.7     |
|    mean_reward          | -125     |
| time/                   |          |
|    total_timesteps      | 2955264  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 1.41     |
|    n_updates            | 720      |
|    policy_gradient_loss | 5.12e-10 |
|    value_loss           | 1.37     |
--------------------------------------
Eval num_timesteps=2965248, episode_reward=-147.72 +/- 33.42
Episode length: 71.00 +/- 11.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71       |
|    mean_reward     | -148     |
| time/              |          |
|    total_timesteps | 2965248  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.2     |
|    ep_rew_mean     | -130     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 181      |
|    time_elapsed    | 6105     |
|    total_timesteps | 2965504  |
---------------------------------
Eval num_timesteps=2975232, episode_reward=-126.87 +/- 29.05
Episode length: 75.10 +/- 14.02
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 75.1     |
|    mean_reward          | -127     |
| time/                   |          |
|    total_timesteps      | 2975232  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 1.07     |
|    n_updates            | 724      |
|    policy_gradient_loss | 9.39e-10 |
|    value_loss           | 1.28     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 73.3     |
|    ep_rew_mean     | -131     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 182      |
|    time_elapsed    | 6136     |
|    total_timesteps | 2981888  |
---------------------------------
Eval num_timesteps=2985216, episode_reward=-148.10 +/- 32.32
Episode length: 73.10 +/- 10.45
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 73.1     |
|    mean_reward          | -148     |
| time/                   |          |
|    total_timesteps      | 2985216  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.568    |
|    n_updates            | 728      |
|    policy_gradient_loss | 1.4e-10  |
|    value_loss           | 1.23     |
--------------------------------------
Eval num_timesteps=2995200, episode_reward=-130.57 +/- 34.94
Episode length: 73.10 +/- 11.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.1     |
|    mean_reward     | -131     |
| time/              |          |
|    total_timesteps | 2995200  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.5     |
|    ep_rew_mean     | -123     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 183      |
|    time_elapsed    | 6169     |
|    total_timesteps | 2998272  |
---------------------------------
Eval num_timesteps=3005184, episode_reward=-165.02 +/- 74.62
Episode length: 74.80 +/- 15.99
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 74.8     |
|    mean_reward          | -165     |
| time/                   |          |
|    total_timesteps      | 3005184  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.977    |
|    n_updates            | 732      |
|    policy_gradient_loss | 3.96e-10 |
|    value_loss           | 1.21     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.2     |
|    ep_rew_mean     | -130     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 184      |
|    time_elapsed    | 6202     |
|    total_timesteps | 3014656  |
---------------------------------
Eval num_timesteps=3015168, episode_reward=-139.13 +/- 28.42
Episode length: 69.50 +/- 13.60
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 69.5      |
|    mean_reward          | -139      |
| time/                   |           |
|    total_timesteps      | 3015168   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.957     |
|    n_updates            | 736       |
|    policy_gradient_loss | -8.38e-10 |
|    value_loss           | 1.21      |
---------------------------------------
Eval num_timesteps=3025152, episode_reward=-122.31 +/- 16.72
Episode length: 73.10 +/- 12.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.1     |
|    mean_reward     | -122     |
| time/              |          |
|    total_timesteps | 3025152  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.5     |
|    ep_rew_mean     | -142     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 185      |
|    time_elapsed    | 6234     |
|    total_timesteps | 3031040  |
---------------------------------
Eval num_timesteps=3035136, episode_reward=-128.75 +/- 25.44
Episode length: 62.10 +/- 9.58
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 62.1     |
|    mean_reward          | -129     |
| time/                   |          |
|    total_timesteps      | 3035136  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 1.21     |
|    n_updates            | 740      |
|    policy_gradient_loss | 9.09e-10 |
|    value_loss           | 1.29     |
--------------------------------------
Eval num_timesteps=3045120, episode_reward=-122.21 +/- 18.01
Episode length: 61.70 +/- 6.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 61.7     |
|    mean_reward     | -122     |
| time/              |          |
|    total_timesteps | 3045120  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.4     |
|    ep_rew_mean     | -134     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 186      |
|    time_elapsed    | 6267     |
|    total_timesteps | 3047424  |
---------------------------------
Eval num_timesteps=3055104, episode_reward=-122.44 +/- 27.98
Episode length: 68.30 +/- 12.17
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 68.3     |
|    mean_reward          | -122     |
| time/                   |          |
|    total_timesteps      | 3055104  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.954    |
|    n_updates            | 744      |
|    policy_gradient_loss | -3.2e-10 |
|    value_loss           | 1.22     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.6     |
|    ep_rew_mean     | -125     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 187      |
|    time_elapsed    | 6304     |
|    total_timesteps | 3063808  |
---------------------------------
Eval num_timesteps=3065088, episode_reward=-174.38 +/- 63.08
Episode length: 71.90 +/- 12.32
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 71.9     |
|    mean_reward          | -174     |
| time/                   |          |
|    total_timesteps      | 3065088  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.909    |
|    n_updates            | 748      |
|    policy_gradient_loss | 2.18e-11 |
|    value_loss           | 1.21     |
--------------------------------------
Eval num_timesteps=3075072, episode_reward=-116.73 +/- 51.67
Episode length: 65.00 +/- 12.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 65       |
|    mean_reward     | -117     |
| time/              |          |
|    total_timesteps | 3075072  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.8     |
|    ep_rew_mean     | -134     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 188      |
|    time_elapsed    | 6338     |
|    total_timesteps | 3080192  |
---------------------------------
Eval num_timesteps=3085056, episode_reward=-111.41 +/- 56.51
Episode length: 67.60 +/- 11.39
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 67.6      |
|    mean_reward          | -111      |
| time/                   |           |
|    total_timesteps      | 3085056   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | -2.38e-07 |
|    learning_rate        | 0.819     |
|    loss                 | 1         |
|    n_updates            | 752       |
|    policy_gradient_loss | 1.23e-09  |
|    value_loss           | 1.27      |
---------------------------------------
Eval num_timesteps=3095040, episode_reward=-133.61 +/- 20.09
Episode length: 69.50 +/- 14.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.5     |
|    mean_reward     | -134     |
| time/              |          |
|    total_timesteps | 3095040  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.1     |
|    ep_rew_mean     | -136     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 189      |
|    time_elapsed    | 6375     |
|    total_timesteps | 3096576  |
---------------------------------
Eval num_timesteps=3105024, episode_reward=-132.45 +/- 26.20
Episode length: 74.40 +/- 10.62
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 74.4      |
|    mean_reward          | -132      |
| time/                   |           |
|    total_timesteps      | 3105024   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 1.03      |
|    n_updates            | 756       |
|    policy_gradient_loss | -3.34e-10 |
|    value_loss           | 1.23      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.5     |
|    ep_rew_mean     | -131     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 190      |
|    time_elapsed    | 6410     |
|    total_timesteps | 3112960  |
---------------------------------
Eval num_timesteps=3115008, episode_reward=-139.23 +/- 21.29
Episode length: 67.40 +/- 10.84
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 67.4      |
|    mean_reward          | -139      |
| time/                   |           |
|    total_timesteps      | 3115008   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.969     |
|    n_updates            | 760       |
|    policy_gradient_loss | -2.06e-10 |
|    value_loss           | 1.26      |
---------------------------------------
Eval num_timesteps=3124992, episode_reward=-128.99 +/- 36.41
Episode length: 67.40 +/- 10.39
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67.4     |
|    mean_reward     | -129     |
| time/              |          |
|    total_timesteps | 3124992  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.3     |
|    ep_rew_mean     | -130     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 191      |
|    time_elapsed    | 6443     |
|    total_timesteps | 3129344  |
---------------------------------
Eval num_timesteps=3134976, episode_reward=-155.33 +/- 73.34
Episode length: 70.60 +/- 14.85
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 70.6      |
|    mean_reward          | -155      |
| time/                   |           |
|    total_timesteps      | 3134976   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.819     |
|    loss                 | 0.605     |
|    n_updates            | 764       |
|    policy_gradient_loss | -9.52e-10 |
|    value_loss           | 1.2       |
---------------------------------------
Eval num_timesteps=3144960, episode_reward=-119.90 +/- 45.50
Episode length: 70.60 +/- 12.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.6     |
|    mean_reward     | -120     |
| time/              |          |
|    total_timesteps | 3144960  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.9     |
|    ep_rew_mean     | -125     |
| time/              |          |
|    fps             | 485      |
|    iterations      | 192      |
|    time_elapsed    | 6474     |
|    total_timesteps | 3145728  |
---------------------------------
Eval num_timesteps=3154944, episode_reward=-120.93 +/- 47.12
Episode length: 66.30 +/- 12.93
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 66.3      |
|    mean_reward          | -121      |
| time/                   |           |
|    total_timesteps      | 3154944   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.819     |
|    loss                 | 0.543     |
|    n_updates            | 768       |
|    policy_gradient_loss | -7.19e-10 |
|    value_loss           | 1.25      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.1     |
|    ep_rew_mean     | -135     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 193      |
|    time_elapsed    | 6506     |
|    total_timesteps | 3162112  |
---------------------------------
Eval num_timesteps=3164928, episode_reward=-140.27 +/- 21.84
Episode length: 79.10 +/- 8.07
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 79.1      |
|    mean_reward          | -140      |
| time/                   |           |
|    total_timesteps      | 3164928   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.971     |
|    n_updates            | 772       |
|    policy_gradient_loss | -5.25e-10 |
|    value_loss           | 1.23      |
---------------------------------------
Eval num_timesteps=3174912, episode_reward=-110.29 +/- 35.94
Episode length: 68.60 +/- 11.52
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.6     |
|    mean_reward     | -110     |
| time/              |          |
|    total_timesteps | 3174912  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.7     |
|    ep_rew_mean     | -138     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 194      |
|    time_elapsed    | 6537     |
|    total_timesteps | 3178496  |
---------------------------------
Eval num_timesteps=3184896, episode_reward=-124.66 +/- 47.00
Episode length: 70.20 +/- 8.58
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 70.2      |
|    mean_reward          | -125      |
| time/                   |           |
|    total_timesteps      | 3184896   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.819     |
|    loss                 | 1.32      |
|    n_updates            | 776       |
|    policy_gradient_loss | -1.81e-10 |
|    value_loss           | 1.2       |
---------------------------------------
Eval num_timesteps=3194880, episode_reward=-135.03 +/- 27.17
Episode length: 69.80 +/- 11.73
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.8     |
|    mean_reward     | -135     |
| time/              |          |
|    total_timesteps | 3194880  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 67.7     |
|    ep_rew_mean     | -127     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 195      |
|    time_elapsed    | 6570     |
|    total_timesteps | 3194880  |
---------------------------------
Eval num_timesteps=3204864, episode_reward=-105.17 +/- 45.19
Episode length: 65.80 +/- 9.01
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 65.8      |
|    mean_reward          | -105      |
| time/                   |           |
|    total_timesteps      | 3204864   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.924     |
|    n_updates            | 780       |
|    policy_gradient_loss | -5.09e-10 |
|    value_loss           | 1.28      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.8     |
|    ep_rew_mean     | -132     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 196      |
|    time_elapsed    | 6603     |
|    total_timesteps | 3211264  |
---------------------------------
Eval num_timesteps=3214848, episode_reward=-162.60 +/- 76.43
Episode length: 77.20 +/- 10.42
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 77.2      |
|    mean_reward          | -163      |
| time/                   |           |
|    total_timesteps      | 3214848   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.819     |
|    loss                 | 0.554     |
|    n_updates            | 784       |
|    policy_gradient_loss | 1.48e-09  |
|    value_loss           | 1.22      |
---------------------------------------
Eval num_timesteps=3224832, episode_reward=-130.60 +/- 27.11
Episode length: 68.00 +/- 10.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68       |
|    mean_reward     | -131     |
| time/              |          |
|    total_timesteps | 3224832  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.1     |
|    ep_rew_mean     | -133     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 197      |
|    time_elapsed    | 6636     |
|    total_timesteps | 3227648  |
---------------------------------
Eval num_timesteps=3234816, episode_reward=-125.38 +/- 31.81
Episode length: 66.30 +/- 13.73
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 66.3     |
|    mean_reward          | -125     |
| time/                   |          |
|    total_timesteps      | 3234816  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.496    |
|    n_updates            | 788      |
|    policy_gradient_loss | -1.4e-10 |
|    value_loss           | 1.32     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68       |
|    ep_rew_mean     | -129     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 198      |
|    time_elapsed    | 6668     |
|    total_timesteps | 3244032  |
---------------------------------
Eval num_timesteps=3244800, episode_reward=-129.39 +/- 47.00
Episode length: 72.10 +/- 9.91
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 72.1      |
|    mean_reward          | -129      |
| time/                   |           |
|    total_timesteps      | 3244800   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.819     |
|    loss                 | 0.973     |
|    n_updates            | 792       |
|    policy_gradient_loss | 1.16e-10  |
|    value_loss           | 1.22      |
---------------------------------------
Eval num_timesteps=3254784, episode_reward=-128.84 +/- 54.04
Episode length: 76.80 +/- 14.50
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.8     |
|    mean_reward     | -129     |
| time/              |          |
|    total_timesteps | 3254784  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.4     |
|    ep_rew_mean     | -124     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 199      |
|    time_elapsed    | 6701     |
|    total_timesteps | 3260416  |
---------------------------------
Eval num_timesteps=3264768, episode_reward=-133.46 +/- 73.10
Episode length: 73.30 +/- 13.21
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 73.3     |
|    mean_reward          | -133     |
| time/                   |          |
|    total_timesteps      | 3264768  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.943    |
|    n_updates            | 796      |
|    policy_gradient_loss | -5.6e-10 |
|    value_loss           | 1.34     |
--------------------------------------
Eval num_timesteps=3274752, episode_reward=-113.56 +/- 48.75
Episode length: 72.30 +/- 13.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.3     |
|    mean_reward     | -114     |
| time/              |          |
|    total_timesteps | 3274752  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.4     |
|    ep_rew_mean     | -130     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 200      |
|    time_elapsed    | 6733     |
|    total_timesteps | 3276800  |
---------------------------------
Eval num_timesteps=3284736, episode_reward=-107.43 +/- 54.48
Episode length: 70.00 +/- 12.26
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 70        |
|    mean_reward          | -107      |
| time/                   |           |
|    total_timesteps      | 3284736   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.819     |
|    loss                 | 0.519     |
|    n_updates            | 800       |
|    policy_gradient_loss | -4.84e-10 |
|    value_loss           | 1.27      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.6     |
|    ep_rew_mean     | -137     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 201      |
|    time_elapsed    | 6764     |
|    total_timesteps | 3293184  |
---------------------------------
Eval num_timesteps=3294720, episode_reward=-146.65 +/- 79.80
Episode length: 77.20 +/- 14.36
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 77.2      |
|    mean_reward          | -147      |
| time/                   |           |
|    total_timesteps      | 3294720   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.902     |
|    n_updates            | 804       |
|    policy_gradient_loss | -3.94e-10 |
|    value_loss           | 1.24      |
---------------------------------------
Eval num_timesteps=3304704, episode_reward=-118.40 +/- 48.08
Episode length: 70.50 +/- 9.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.5     |
|    mean_reward     | -118     |
| time/              |          |
|    total_timesteps | 3304704  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.9     |
|    ep_rew_mean     | -136     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 202      |
|    time_elapsed    | 6797     |
|    total_timesteps | 3309568  |
---------------------------------
Eval num_timesteps=3314688, episode_reward=-153.68 +/- 25.90
Episode length: 73.70 +/- 9.63
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 73.7      |
|    mean_reward          | -154      |
| time/                   |           |
|    total_timesteps      | 3314688   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.819     |
|    loss                 | 0.958     |
|    n_updates            | 808       |
|    policy_gradient_loss | -1.43e-09 |
|    value_loss           | 1.21      |
---------------------------------------
Eval num_timesteps=3324672, episode_reward=-130.85 +/- 21.33
Episode length: 73.60 +/- 12.08
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.6     |
|    mean_reward     | -131     |
| time/              |          |
|    total_timesteps | 3324672  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.5     |
|    ep_rew_mean     | -129     |
| time/              |          |
|    fps             | 486      |
|    iterations      | 203      |
|    time_elapsed    | 6829     |
|    total_timesteps | 3325952  |
---------------------------------
Eval num_timesteps=3334656, episode_reward=-140.50 +/- 36.07
Episode length: 70.90 +/- 11.69
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 70.9     |
|    mean_reward          | -141     |
| time/                   |          |
|    total_timesteps      | 3334656  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.58     |
|    n_updates            | 812      |
|    policy_gradient_loss | 3.1e-10  |
|    value_loss           | 1.23     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 66.8     |
|    ep_rew_mean     | -130     |
| time/              |          |
|    fps             | 487      |
|    iterations      | 204      |
|    time_elapsed    | 6860     |
|    total_timesteps | 3342336  |
---------------------------------
Eval num_timesteps=3344640, episode_reward=-127.32 +/- 28.07
Episode length: 65.90 +/- 13.84
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 65.9      |
|    mean_reward          | -127      |
| time/                   |           |
|    total_timesteps      | 3344640   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.988     |
|    n_updates            | 816       |
|    policy_gradient_loss | -2.89e-10 |
|    value_loss           | 1.22      |
---------------------------------------
Eval num_timesteps=3354624, episode_reward=-138.85 +/- 25.75
Episode length: 70.20 +/- 9.68
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.2     |
|    mean_reward     | -139     |
| time/              |          |
|    total_timesteps | 3354624  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.6     |
|    ep_rew_mean     | -137     |
| time/              |          |
|    fps             | 487      |
|    iterations      | 205      |
|    time_elapsed    | 6895     |
|    total_timesteps | 3358720  |
---------------------------------
Eval num_timesteps=3364608, episode_reward=-132.85 +/- 23.48
Episode length: 65.60 +/- 9.75
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 65.6     |
|    mean_reward          | -133     |
| time/                   |          |
|    total_timesteps      | 3364608  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.924    |
|    n_updates            | 820      |
|    policy_gradient_loss | 4.93e-10 |
|    value_loss           | 1.27     |
--------------------------------------
Eval num_timesteps=3374592, episode_reward=-123.19 +/- 49.27
Episode length: 67.20 +/- 10.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67.2     |
|    mean_reward     | -123     |
| time/              |          |
|    total_timesteps | 3374592  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.3     |
|    ep_rew_mean     | -137     |
| time/              |          |
|    fps             | 487      |
|    iterations      | 206      |
|    time_elapsed    | 6930     |
|    total_timesteps | 3375104  |
---------------------------------
Eval num_timesteps=3384576, episode_reward=-150.46 +/- 52.90
Episode length: 73.30 +/- 15.23
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 73.3     |
|    mean_reward          | -150     |
| time/                   |          |
|    total_timesteps      | 3384576  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.567    |
|    n_updates            | 824      |
|    policy_gradient_loss | -4.4e-10 |
|    value_loss           | 1.2      |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72.1     |
|    ep_rew_mean     | -126     |
| time/              |          |
|    fps             | 487      |
|    iterations      | 207      |
|    time_elapsed    | 6961     |
|    total_timesteps | 3391488  |
---------------------------------
Eval num_timesteps=3394560, episode_reward=-112.59 +/- 42.73
Episode length: 73.90 +/- 14.05
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 73.9     |
|    mean_reward          | -113     |
| time/                   |          |
|    total_timesteps      | 3394560  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.582    |
|    n_updates            | 828      |
|    policy_gradient_loss | 4.56e-10 |
|    value_loss           | 1.35     |
--------------------------------------
Eval num_timesteps=3404544, episode_reward=-121.45 +/- 30.44
Episode length: 66.50 +/- 12.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 66.5     |
|    mean_reward     | -121     |
| time/              |          |
|    total_timesteps | 3404544  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.2     |
|    ep_rew_mean     | -125     |
| time/              |          |
|    fps             | 487      |
|    iterations      | 208      |
|    time_elapsed    | 6993     |
|    total_timesteps | 3407872  |
---------------------------------
Eval num_timesteps=3414528, episode_reward=-131.87 +/- 21.50
Episode length: 70.80 +/- 8.06
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 70.8      |
|    mean_reward          | -132      |
| time/                   |           |
|    total_timesteps      | 3414528   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | -0.919    |
|    learning_rate        | 0.819     |
|    loss                 | 1.02      |
|    n_updates            | 832       |
|    policy_gradient_loss | -1.56e-09 |
|    value_loss           | 1.57      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.7     |
|    ep_rew_mean     | -136     |
| time/              |          |
|    fps             | 487      |
|    iterations      | 209      |
|    time_elapsed    | 7025     |
|    total_timesteps | 3424256  |
---------------------------------
Eval num_timesteps=3424512, episode_reward=-149.21 +/- 54.67
Episode length: 73.70 +/- 14.28
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 73.7      |
|    mean_reward          | -149      |
| time/                   |           |
|    total_timesteps      | 3424512   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 1.03      |
|    n_updates            | 836       |
|    policy_gradient_loss | -2.36e-10 |
|    value_loss           | 1.23      |
---------------------------------------
Eval num_timesteps=3434496, episode_reward=-144.99 +/- 82.52
Episode length: 72.80 +/- 14.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.8     |
|    mean_reward     | -145     |
| time/              |          |
|    total_timesteps | 3434496  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68       |
|    ep_rew_mean     | -121     |
| time/              |          |
|    fps             | 487      |
|    iterations      | 210      |
|    time_elapsed    | 7057     |
|    total_timesteps | 3440640  |
---------------------------------
Eval num_timesteps=3444480, episode_reward=-145.71 +/- 23.27
Episode length: 75.10 +/- 11.84
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 75.1     |
|    mean_reward          | -146     |
| time/                   |          |
|    total_timesteps      | 3444480  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.602    |
|    n_updates            | 840      |
|    policy_gradient_loss | 4.78e-10 |
|    value_loss           | 1.32     |
--------------------------------------
Eval num_timesteps=3454464, episode_reward=-140.64 +/- 30.95
Episode length: 71.80 +/- 9.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.8     |
|    mean_reward     | -141     |
| time/              |          |
|    total_timesteps | 3454464  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.2     |
|    ep_rew_mean     | -126     |
| time/              |          |
|    fps             | 487      |
|    iterations      | 211      |
|    time_elapsed    | 7089     |
|    total_timesteps | 3457024  |
---------------------------------
Eval num_timesteps=3464448, episode_reward=-154.16 +/- 58.87
Episode length: 68.00 +/- 10.55
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 68        |
|    mean_reward          | -154      |
| time/                   |           |
|    total_timesteps      | 3464448   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 1.38      |
|    n_updates            | 844       |
|    policy_gradient_loss | -8.42e-10 |
|    value_loss           | 1.18      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.5     |
|    ep_rew_mean     | -129     |
| time/              |          |
|    fps             | 487      |
|    iterations      | 212      |
|    time_elapsed    | 7119     |
|    total_timesteps | 3473408  |
---------------------------------
Eval num_timesteps=3474432, episode_reward=-121.51 +/- 26.84
Episode length: 73.50 +/- 12.27
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 73.5      |
|    mean_reward          | -122      |
| time/                   |           |
|    total_timesteps      | 3474432   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 2.14      |
|    n_updates            | 848       |
|    policy_gradient_loss | -2.82e-10 |
|    value_loss           | 1.25      |
---------------------------------------
Eval num_timesteps=3484416, episode_reward=-140.26 +/- 43.10
Episode length: 75.10 +/- 12.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.1     |
|    mean_reward     | -140     |
| time/              |          |
|    total_timesteps | 3484416  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.7     |
|    ep_rew_mean     | -132     |
| time/              |          |
|    fps             | 488      |
|    iterations      | 213      |
|    time_elapsed    | 7150     |
|    total_timesteps | 3489792  |
---------------------------------
Eval num_timesteps=3494400, episode_reward=-134.62 +/- 43.94
Episode length: 74.30 +/- 12.06
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 74.3      |
|    mean_reward          | -135      |
| time/                   |           |
|    total_timesteps      | 3494400   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.819     |
|    loss                 | 1.55      |
|    n_updates            | 852       |
|    policy_gradient_loss | 6.55e-11  |
|    value_loss           | 1.22      |
---------------------------------------
Eval num_timesteps=3504384, episode_reward=-117.09 +/- 18.29
Episode length: 62.40 +/- 10.38
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.4     |
|    mean_reward     | -117     |
| time/              |          |
|    total_timesteps | 3504384  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.6     |
|    ep_rew_mean     | -128     |
| time/              |          |
|    fps             | 488      |
|    iterations      | 214      |
|    time_elapsed    | 7181     |
|    total_timesteps | 3506176  |
---------------------------------
Eval num_timesteps=3514368, episode_reward=-124.12 +/- 54.53
Episode length: 72.80 +/- 11.39
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 72.8     |
|    mean_reward          | -124     |
| time/                   |          |
|    total_timesteps      | 3514368  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.994    |
|    n_updates            | 856      |
|    policy_gradient_loss | 6.84e-10 |
|    value_loss           | 1.22     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72.2     |
|    ep_rew_mean     | -140     |
| time/              |          |
|    fps             | 488      |
|    iterations      | 215      |
|    time_elapsed    | 7214     |
|    total_timesteps | 3522560  |
---------------------------------
Eval num_timesteps=3524352, episode_reward=-111.08 +/- 55.42
Episode length: 71.80 +/- 17.06
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 71.8     |
|    mean_reward          | -111     |
| time/                   |          |
|    total_timesteps      | 3524352  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 1.19e-07 |
|    learning_rate        | 0.819    |
|    loss                 | 0.881    |
|    n_updates            | 860      |
|    policy_gradient_loss | 6.2e-10  |
|    value_loss           | 1.22     |
--------------------------------------
Eval num_timesteps=3534336, episode_reward=-119.63 +/- 45.19
Episode length: 67.30 +/- 14.23
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67.3     |
|    mean_reward     | -120     |
| time/              |          |
|    total_timesteps | 3534336  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.1     |
|    ep_rew_mean     | -131     |
| time/              |          |
|    fps             | 488      |
|    iterations      | 216      |
|    time_elapsed    | 7246     |
|    total_timesteps | 3538944  |
---------------------------------
Eval num_timesteps=3544320, episode_reward=-112.99 +/- 53.18
Episode length: 69.80 +/- 12.22
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 69.8      |
|    mean_reward          | -113      |
| time/                   |           |
|    total_timesteps      | 3544320   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.819     |
|    loss                 | 0.903     |
|    n_updates            | 864       |
|    policy_gradient_loss | 1.07e-10  |
|    value_loss           | 1.24      |
---------------------------------------
Eval num_timesteps=3554304, episode_reward=-124.26 +/- 45.33
Episode length: 71.10 +/- 11.64
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.1     |
|    mean_reward     | -124     |
| time/              |          |
|    total_timesteps | 3554304  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.4     |
|    ep_rew_mean     | -132     |
| time/              |          |
|    fps             | 488      |
|    iterations      | 217      |
|    time_elapsed    | 7278     |
|    total_timesteps | 3555328  |
---------------------------------
Eval num_timesteps=3564288, episode_reward=-125.75 +/- 62.15
Episode length: 69.50 +/- 10.27
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 69.5      |
|    mean_reward          | -126      |
| time/                   |           |
|    total_timesteps      | 3564288   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.995     |
|    n_updates            | 868       |
|    policy_gradient_loss | -4.76e-10 |
|    value_loss           | 1.25      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.2     |
|    ep_rew_mean     | -127     |
| time/              |          |
|    fps             | 488      |
|    iterations      | 218      |
|    time_elapsed    | 7309     |
|    total_timesteps | 3571712  |
---------------------------------
Eval num_timesteps=3574272, episode_reward=-111.75 +/- 53.66
Episode length: 72.50 +/- 18.15
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 72.5     |
|    mean_reward          | -112     |
| time/                   |          |
|    total_timesteps      | 3574272  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.878    |
|    n_updates            | 872      |
|    policy_gradient_loss | 3.07e-10 |
|    value_loss           | 1.2      |
--------------------------------------
Eval num_timesteps=3584256, episode_reward=-124.54 +/- 50.09
Episode length: 69.40 +/- 18.56
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.4     |
|    mean_reward     | -125     |
| time/              |          |
|    total_timesteps | 3584256  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.3     |
|    ep_rew_mean     | -135     |
| time/              |          |
|    fps             | 488      |
|    iterations      | 219      |
|    time_elapsed    | 7341     |
|    total_timesteps | 3588096  |
---------------------------------
Eval num_timesteps=3594240, episode_reward=-114.48 +/- 50.67
Episode length: 74.30 +/- 14.03
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 74.3      |
|    mean_reward          | -114      |
| time/                   |           |
|    total_timesteps      | 3594240   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.982     |
|    n_updates            | 876       |
|    policy_gradient_loss | -9.55e-11 |
|    value_loss           | 1.26      |
---------------------------------------
Eval num_timesteps=3604224, episode_reward=-103.56 +/- 67.10
Episode length: 69.20 +/- 9.34
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.2     |
|    mean_reward     | -104     |
| time/              |          |
|    total_timesteps | 3604224  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.1     |
|    ep_rew_mean     | -137     |
| time/              |          |
|    fps             | 488      |
|    iterations      | 220      |
|    time_elapsed    | 7373     |
|    total_timesteps | 3604480  |
---------------------------------
Eval num_timesteps=3614208, episode_reward=-141.07 +/- 25.29
Episode length: 68.30 +/- 8.80
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 68.3      |
|    mean_reward          | -141      |
| time/                   |           |
|    total_timesteps      | 3614208   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | -2.38e-07 |
|    learning_rate        | 0.819     |
|    loss                 | 0.895     |
|    n_updates            | 880       |
|    policy_gradient_loss | 4.26e-10  |
|    value_loss           | 1.26      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.9     |
|    ep_rew_mean     | -126     |
| time/              |          |
|    fps             | 489      |
|    iterations      | 221      |
|    time_elapsed    | 7404     |
|    total_timesteps | 3620864  |
---------------------------------
Eval num_timesteps=3624192, episode_reward=-116.54 +/- 46.07
Episode length: 72.00 +/- 11.41
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 72       |
|    mean_reward          | -117     |
| time/                   |          |
|    total_timesteps      | 3624192  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.605    |
|    n_updates            | 884      |
|    policy_gradient_loss | -3.8e-10 |
|    value_loss           | 1.29     |
--------------------------------------
Eval num_timesteps=3634176, episode_reward=-119.20 +/- 21.17
Episode length: 66.70 +/- 10.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 66.7     |
|    mean_reward     | -119     |
| time/              |          |
|    total_timesteps | 3634176  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.8     |
|    ep_rew_mean     | -130     |
| time/              |          |
|    fps             | 489      |
|    iterations      | 222      |
|    time_elapsed    | 7435     |
|    total_timesteps | 3637248  |
---------------------------------
Eval num_timesteps=3644160, episode_reward=-128.72 +/- 21.34
Episode length: 70.30 +/- 11.10
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 70.3      |
|    mean_reward          | -129      |
| time/                   |           |
|    total_timesteps      | 3644160   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.607     |
|    n_updates            | 888       |
|    policy_gradient_loss | -1.33e-10 |
|    value_loss           | 1.3       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.7     |
|    ep_rew_mean     | -135     |
| time/              |          |
|    fps             | 489      |
|    iterations      | 223      |
|    time_elapsed    | 7467     |
|    total_timesteps | 3653632  |
---------------------------------
Eval num_timesteps=3654144, episode_reward=-117.20 +/- 49.67
Episode length: 67.50 +/- 9.40
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 67.5      |
|    mean_reward          | -117      |
| time/                   |           |
|    total_timesteps      | 3654144   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.819     |
|    loss                 | 0.934     |
|    n_updates            | 892       |
|    policy_gradient_loss | 6.11e-10  |
|    value_loss           | 1.26      |
---------------------------------------
Eval num_timesteps=3664128, episode_reward=-155.16 +/- 31.96
Episode length: 69.60 +/- 13.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.6     |
|    mean_reward     | -155     |
| time/              |          |
|    total_timesteps | 3664128  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.7     |
|    ep_rew_mean     | -138     |
| time/              |          |
|    fps             | 489      |
|    iterations      | 224      |
|    time_elapsed    | 7502     |
|    total_timesteps | 3670016  |
---------------------------------
Eval num_timesteps=3674112, episode_reward=-130.35 +/- 52.39
Episode length: 77.10 +/- 9.58
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 77.1     |
|    mean_reward          | -130     |
| time/                   |          |
|    total_timesteps      | 3674112  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 1.03     |
|    n_updates            | 896      |
|    policy_gradient_loss | 2.54e-10 |
|    value_loss           | 1.32     |
--------------------------------------
Eval num_timesteps=3684096, episode_reward=-136.85 +/- 16.86
Episode length: 65.60 +/- 10.63
---------------------------------
| eval/              |          |
|    mean_ep_length  | 65.6     |
|    mean_reward     | -137     |
| time/              |          |
|    total_timesteps | 3684096  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.7     |
|    ep_rew_mean     | -132     |
| time/              |          |
|    fps             | 489      |
|    iterations      | 225      |
|    time_elapsed    | 7537     |
|    total_timesteps | 3686400  |
---------------------------------
Eval num_timesteps=3694080, episode_reward=-124.85 +/- 34.56
Episode length: 70.90 +/- 12.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 70.9      |
|    mean_reward          | -125      |
| time/                   |           |
|    total_timesteps      | 3694080   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.987     |
|    n_updates            | 900       |
|    policy_gradient_loss | -1.26e-09 |
|    value_loss           | 1.31      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.9     |
|    ep_rew_mean     | -132     |
| time/              |          |
|    fps             | 489      |
|    iterations      | 226      |
|    time_elapsed    | 7570     |
|    total_timesteps | 3702784  |
---------------------------------
Eval num_timesteps=3704064, episode_reward=-130.92 +/- 27.79
Episode length: 68.30 +/- 9.69
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 68.3     |
|    mean_reward          | -131     |
| time/                   |          |
|    total_timesteps      | 3704064  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.968    |
|    n_updates            | 904      |
|    policy_gradient_loss | -2.6e-10 |
|    value_loss           | 1.28     |
--------------------------------------
Eval num_timesteps=3714048, episode_reward=-128.64 +/- 53.66
Episode length: 71.50 +/- 9.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.5     |
|    mean_reward     | -129     |
| time/              |          |
|    total_timesteps | 3714048  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.7     |
|    ep_rew_mean     | -141     |
| time/              |          |
|    fps             | 489      |
|    iterations      | 227      |
|    time_elapsed    | 7603     |
|    total_timesteps | 3719168  |
---------------------------------
Eval num_timesteps=3724032, episode_reward=-119.26 +/- 21.65
Episode length: 65.40 +/- 12.05
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 65.4      |
|    mean_reward          | -119      |
| time/                   |           |
|    total_timesteps      | 3724032   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 1.54      |
|    n_updates            | 908       |
|    policy_gradient_loss | -1.85e-10 |
|    value_loss           | 1.2       |
---------------------------------------
Eval num_timesteps=3734016, episode_reward=-134.73 +/- 21.40
Episode length: 66.90 +/- 7.88
---------------------------------
| eval/              |          |
|    mean_ep_length  | 66.9     |
|    mean_reward     | -135     |
| time/              |          |
|    total_timesteps | 3734016  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.4     |
|    ep_rew_mean     | -138     |
| time/              |          |
|    fps             | 489      |
|    iterations      | 228      |
|    time_elapsed    | 7637     |
|    total_timesteps | 3735552  |
---------------------------------
Eval num_timesteps=3744000, episode_reward=-116.96 +/- 51.84
Episode length: 71.30 +/- 14.04
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 71.3      |
|    mean_reward          | -117      |
| time/                   |           |
|    total_timesteps      | 3744000   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 1.03      |
|    n_updates            | 912       |
|    policy_gradient_loss | -2.88e-10 |
|    value_loss           | 1.25      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.7     |
|    ep_rew_mean     | -132     |
| time/              |          |
|    fps             | 489      |
|    iterations      | 229      |
|    time_elapsed    | 7669     |
|    total_timesteps | 3751936  |
---------------------------------
Eval num_timesteps=3753984, episode_reward=-104.53 +/- 58.86
Episode length: 66.40 +/- 12.00
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 66.4     |
|    mean_reward          | -105     |
| time/                   |          |
|    total_timesteps      | 3753984  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | -0.0603  |
|    learning_rate        | 0.819    |
|    loss                 | 1.78     |
|    n_updates            | 916      |
|    policy_gradient_loss | 2.1e-10  |
|    value_loss           | 1.36     |
--------------------------------------
Eval num_timesteps=3763968, episode_reward=-142.70 +/- 15.32
Episode length: 66.10 +/- 10.14
---------------------------------
| eval/              |          |
|    mean_ep_length  | 66.1     |
|    mean_reward     | -143     |
| time/              |          |
|    total_timesteps | 3763968  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.5     |
|    ep_rew_mean     | -130     |
| time/              |          |
|    fps             | 489      |
|    iterations      | 230      |
|    time_elapsed    | 7703     |
|    total_timesteps | 3768320  |
---------------------------------
Eval num_timesteps=3773952, episode_reward=-138.81 +/- 35.61
Episode length: 67.00 +/- 12.64
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 67        |
|    mean_reward          | -139      |
| time/                   |           |
|    total_timesteps      | 3773952   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.819     |
|    loss                 | 1.03      |
|    n_updates            | 920       |
|    policy_gradient_loss | -5.13e-10 |
|    value_loss           | 1.21      |
---------------------------------------
Eval num_timesteps=3783936, episode_reward=-134.20 +/- 19.03
Episode length: 70.80 +/- 8.59
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.8     |
|    mean_reward     | -134     |
| time/              |          |
|    total_timesteps | 3783936  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 66.8     |
|    ep_rew_mean     | -130     |
| time/              |          |
|    fps             | 489      |
|    iterations      | 231      |
|    time_elapsed    | 7738     |
|    total_timesteps | 3784704  |
---------------------------------
Eval num_timesteps=3793920, episode_reward=-122.93 +/- 45.25
Episode length: 73.90 +/- 12.57
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 73.9     |
|    mean_reward          | -123     |
| time/                   |          |
|    total_timesteps      | 3793920  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 1.19e-07 |
|    learning_rate        | 0.819    |
|    loss                 | 0.947    |
|    n_updates            | 924      |
|    policy_gradient_loss | 6.22e-10 |
|    value_loss           | 1.32     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.2     |
|    ep_rew_mean     | -138     |
| time/              |          |
|    fps             | 488      |
|    iterations      | 232      |
|    time_elapsed    | 7775     |
|    total_timesteps | 3801088  |
---------------------------------
Eval num_timesteps=3803904, episode_reward=-133.32 +/- 38.07
Episode length: 68.00 +/- 11.20
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 68       |
|    mean_reward          | -133     |
| time/                   |          |
|    total_timesteps      | 3803904  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.715    |
|    n_updates            | 928      |
|    policy_gradient_loss | 6.64e-11 |
|    value_loss           | 1.27     |
--------------------------------------
Eval num_timesteps=3813888, episode_reward=-118.90 +/- 50.75
Episode length: 66.10 +/- 9.62
---------------------------------
| eval/              |          |
|    mean_ep_length  | 66.1     |
|    mean_reward     | -119     |
| time/              |          |
|    total_timesteps | 3813888  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.5     |
|    ep_rew_mean     | -120     |
| time/              |          |
|    fps             | 488      |
|    iterations      | 233      |
|    time_elapsed    | 7814     |
|    total_timesteps | 3817472  |
---------------------------------
Eval num_timesteps=3823872, episode_reward=-128.79 +/- 23.09
Episode length: 65.40 +/- 11.98
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 65.4      |
|    mean_reward          | -129      |
| time/                   |           |
|    total_timesteps      | 3823872   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.819     |
|    loss                 | 1.91      |
|    n_updates            | 932       |
|    policy_gradient_loss | -9.68e-10 |
|    value_loss           | 1.31      |
---------------------------------------
Eval num_timesteps=3833856, episode_reward=-135.49 +/- 24.90
Episode length: 75.30 +/- 12.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.3     |
|    mean_reward     | -135     |
| time/              |          |
|    total_timesteps | 3833856  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.8     |
|    ep_rew_mean     | -138     |
| time/              |          |
|    fps             | 488      |
|    iterations      | 234      |
|    time_elapsed    | 7849     |
|    total_timesteps | 3833856  |
---------------------------------
Eval num_timesteps=3843840, episode_reward=-164.52 +/- 77.40
Episode length: 74.30 +/- 14.04
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 74.3      |
|    mean_reward          | -165      |
| time/                   |           |
|    total_timesteps      | 3843840   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 1.05      |
|    n_updates            | 936       |
|    policy_gradient_loss | -3.73e-11 |
|    value_loss           | 1.28      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 73.2     |
|    ep_rew_mean     | -132     |
| time/              |          |
|    fps             | 488      |
|    iterations      | 235      |
|    time_elapsed    | 7882     |
|    total_timesteps | 3850240  |
---------------------------------
Eval num_timesteps=3853824, episode_reward=-136.84 +/- 25.27
Episode length: 77.70 +/- 10.34
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 77.7     |
|    mean_reward          | -137     |
| time/                   |          |
|    total_timesteps      | 3853824  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 2.6      |
|    n_updates            | 940      |
|    policy_gradient_loss | 3.46e-11 |
|    value_loss           | 1.28     |
--------------------------------------
Eval num_timesteps=3863808, episode_reward=-141.66 +/- 19.03
Episode length: 72.30 +/- 12.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.3     |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 3863808  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.1     |
|    ep_rew_mean     | -137     |
| time/              |          |
|    fps             | 488      |
|    iterations      | 236      |
|    time_elapsed    | 7916     |
|    total_timesteps | 3866624  |
---------------------------------
Eval num_timesteps=3873792, episode_reward=-132.41 +/- 35.58
Episode length: 67.20 +/- 10.58
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 67.2     |
|    mean_reward          | -132     |
| time/                   |          |
|    total_timesteps      | 3873792  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.99     |
|    n_updates            | 944      |
|    policy_gradient_loss | 2.38e-10 |
|    value_loss           | 1.2      |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.3     |
|    ep_rew_mean     | -138     |
| time/              |          |
|    fps             | 488      |
|    iterations      | 237      |
|    time_elapsed    | 7950     |
|    total_timesteps | 3883008  |
---------------------------------
Eval num_timesteps=3883776, episode_reward=-156.78 +/- 59.60
Episode length: 75.50 +/- 14.14
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 75.5      |
|    mean_reward          | -157      |
| time/                   |           |
|    total_timesteps      | 3883776   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 1.04      |
|    n_updates            | 948       |
|    policy_gradient_loss | -5.81e-10 |
|    value_loss           | 1.35      |
---------------------------------------
Eval num_timesteps=3893760, episode_reward=-133.45 +/- 31.50
Episode length: 71.80 +/- 17.79
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.8     |
|    mean_reward     | -133     |
| time/              |          |
|    total_timesteps | 3893760  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.8     |
|    ep_rew_mean     | -131     |
| time/              |          |
|    fps             | 488      |
|    iterations      | 238      |
|    time_elapsed    | 7982     |
|    total_timesteps | 3899392  |
---------------------------------
Eval num_timesteps=3903744, episode_reward=-116.38 +/- 51.01
Episode length: 75.10 +/- 14.03
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 75.1     |
|    mean_reward          | -116     |
| time/                   |          |
|    total_timesteps      | 3903744  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.979    |
|    n_updates            | 952      |
|    policy_gradient_loss | 5.6e-10  |
|    value_loss           | 1.22     |
--------------------------------------
Eval num_timesteps=3913728, episode_reward=-94.18 +/- 68.43
Episode length: 77.20 +/- 19.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 77.2     |
|    mean_reward     | -94.2    |
| time/              |          |
|    total_timesteps | 3913728  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.1     |
|    ep_rew_mean     | -133     |
| time/              |          |
|    fps             | 488      |
|    iterations      | 239      |
|    time_elapsed    | 8015     |
|    total_timesteps | 3915776  |
---------------------------------
Eval num_timesteps=3923712, episode_reward=-135.67 +/- 26.85
Episode length: 69.50 +/- 12.31
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 69.5      |
|    mean_reward          | -136      |
| time/                   |           |
|    total_timesteps      | 3923712   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.819     |
|    loss                 | 0.986     |
|    n_updates            | 956       |
|    policy_gradient_loss | -1.34e-11 |
|    value_loss           | 1.29      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.7     |
|    ep_rew_mean     | -144     |
| time/              |          |
|    fps             | 488      |
|    iterations      | 240      |
|    time_elapsed    | 8046     |
|    total_timesteps | 3932160  |
---------------------------------
Eval num_timesteps=3933696, episode_reward=-137.63 +/- 40.62
Episode length: 67.50 +/- 8.70
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 67.5      |
|    mean_reward          | -138      |
| time/                   |           |
|    total_timesteps      | 3933696   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.819     |
|    loss                 | 1.03      |
|    n_updates            | 960       |
|    policy_gradient_loss | -4.34e-10 |
|    value_loss           | 1.3       |
---------------------------------------
Eval num_timesteps=3943680, episode_reward=-149.40 +/- 19.58
Episode length: 67.80 +/- 10.45
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67.8     |
|    mean_reward     | -149     |
| time/              |          |
|    total_timesteps | 3943680  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72.4     |
|    ep_rew_mean     | -136     |
| time/              |          |
|    fps             | 488      |
|    iterations      | 241      |
|    time_elapsed    | 8077     |
|    total_timesteps | 3948544  |
---------------------------------
Eval num_timesteps=3953664, episode_reward=-119.51 +/- 42.69
Episode length: 71.90 +/- 12.51
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 71.9      |
|    mean_reward          | -120      |
| time/                   |           |
|    total_timesteps      | 3953664   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 1.01      |
|    n_updates            | 964       |
|    policy_gradient_loss | -1.08e-10 |
|    value_loss           | 1.29      |
---------------------------------------
Eval num_timesteps=3963648, episode_reward=-147.22 +/- 28.07
Episode length: 72.30 +/- 12.87
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.3     |
|    mean_reward     | -147     |
| time/              |          |
|    total_timesteps | 3963648  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.3     |
|    ep_rew_mean     | -128     |
| time/              |          |
|    fps             | 488      |
|    iterations      | 242      |
|    time_elapsed    | 8111     |
|    total_timesteps | 3964928  |
---------------------------------
Eval num_timesteps=3973632, episode_reward=-132.14 +/- 19.27
Episode length: 72.50 +/- 11.09
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 72.5      |
|    mean_reward          | -132      |
| time/                   |           |
|    total_timesteps      | 3973632   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.955     |
|    n_updates            | 968       |
|    policy_gradient_loss | -3.91e-10 |
|    value_loss           | 1.25      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.2     |
|    ep_rew_mean     | -140     |
| time/              |          |
|    fps             | 488      |
|    iterations      | 243      |
|    time_elapsed    | 8142     |
|    total_timesteps | 3981312  |
---------------------------------
Eval num_timesteps=3983616, episode_reward=-120.83 +/- 17.52
Episode length: 66.50 +/- 10.20
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 66.5     |
|    mean_reward          | -121     |
| time/                   |          |
|    total_timesteps      | 3983616  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.991    |
|    n_updates            | 972      |
|    policy_gradient_loss | 5.42e-10 |
|    value_loss           | 1.19     |
--------------------------------------
Eval num_timesteps=3993600, episode_reward=-133.86 +/- 31.96
Episode length: 71.70 +/- 14.53
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.7     |
|    mean_reward     | -134     |
| time/              |          |
|    total_timesteps | 3993600  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72.3     |
|    ep_rew_mean     | -124     |
| time/              |          |
|    fps             | 489      |
|    iterations      | 244      |
|    time_elapsed    | 8174     |
|    total_timesteps | 3997696  |
---------------------------------
Eval num_timesteps=4003584, episode_reward=-145.95 +/- 17.24
Episode length: 69.00 +/- 12.25
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 69        |
|    mean_reward          | -146      |
| time/                   |           |
|    total_timesteps      | 4003584   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.94      |
|    n_updates            | 976       |
|    policy_gradient_loss | -4.99e-10 |
|    value_loss           | 1.27      |
---------------------------------------
Eval num_timesteps=4013568, episode_reward=-129.66 +/- 21.93
Episode length: 65.00 +/- 8.35
---------------------------------
| eval/              |          |
|    mean_ep_length  | 65       |
|    mean_reward     | -130     |
| time/              |          |
|    total_timesteps | 4013568  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.3     |
|    ep_rew_mean     | -132     |
| time/              |          |
|    fps             | 489      |
|    iterations      | 245      |
|    time_elapsed    | 8206     |
|    total_timesteps | 4014080  |
---------------------------------
Eval num_timesteps=4023552, episode_reward=-124.49 +/- 25.86
Episode length: 69.20 +/- 12.47
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 69.2      |
|    mean_reward          | -124      |
| time/                   |           |
|    total_timesteps      | 4023552   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.942     |
|    n_updates            | 980       |
|    policy_gradient_loss | -7.57e-11 |
|    value_loss           | 1.3       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | -144     |
| time/              |          |
|    fps             | 489      |
|    iterations      | 246      |
|    time_elapsed    | 8238     |
|    total_timesteps | 4030464  |
---------------------------------
Eval num_timesteps=4033536, episode_reward=-118.09 +/- 54.15
Episode length: 65.60 +/- 12.28
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 65.6      |
|    mean_reward          | -118      |
| time/                   |           |
|    total_timesteps      | 4033536   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.568     |
|    n_updates            | 984       |
|    policy_gradient_loss | -9.84e-10 |
|    value_loss           | 1.21      |
---------------------------------------
Eval num_timesteps=4043520, episode_reward=-130.21 +/- 20.07
Episode length: 64.90 +/- 12.10
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64.9     |
|    mean_reward     | -130     |
| time/              |          |
|    total_timesteps | 4043520  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.3     |
|    ep_rew_mean     | -133     |
| time/              |          |
|    fps             | 489      |
|    iterations      | 247      |
|    time_elapsed    | 8271     |
|    total_timesteps | 4046848  |
---------------------------------
Eval num_timesteps=4053504, episode_reward=-121.88 +/- 41.27
Episode length: 66.70 +/- 13.44
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 66.7      |
|    mean_reward          | -122      |
| time/                   |           |
|    total_timesteps      | 4053504   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 1.28      |
|    n_updates            | 988       |
|    policy_gradient_loss | -4.57e-10 |
|    value_loss           | 1.24      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.2     |
|    ep_rew_mean     | -132     |
| time/              |          |
|    fps             | 489      |
|    iterations      | 248      |
|    time_elapsed    | 8303     |
|    total_timesteps | 4063232  |
---------------------------------
Eval num_timesteps=4063488, episode_reward=-125.91 +/- 40.04
Episode length: 74.00 +/- 9.28
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 74        |
|    mean_reward          | -126      |
| time/                   |           |
|    total_timesteps      | 4063488   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.819     |
|    loss                 | 0.953     |
|    n_updates            | 992       |
|    policy_gradient_loss | 8.36e-10  |
|    value_loss           | 1.28      |
---------------------------------------
Eval num_timesteps=4073472, episode_reward=-140.07 +/- 49.70
Episode length: 68.60 +/- 12.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.6     |
|    mean_reward     | -140     |
| time/              |          |
|    total_timesteps | 4073472  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70       |
|    ep_rew_mean     | -125     |
| time/              |          |
|    fps             | 489      |
|    iterations      | 249      |
|    time_elapsed    | 8335     |
|    total_timesteps | 4079616  |
---------------------------------
Eval num_timesteps=4083456, episode_reward=-146.78 +/- 25.48
Episode length: 74.30 +/- 12.00
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 74.3     |
|    mean_reward          | -147     |
| time/                   |          |
|    total_timesteps      | 4083456  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 1.02     |
|    n_updates            | 996      |
|    policy_gradient_loss | 9.88e-10 |
|    value_loss           | 1.24     |
--------------------------------------
Eval num_timesteps=4093440, episode_reward=-140.47 +/- 20.82
Episode length: 65.30 +/- 11.65
---------------------------------
| eval/              |          |
|    mean_ep_length  | 65.3     |
|    mean_reward     | -140     |
| time/              |          |
|    total_timesteps | 4093440  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68       |
|    ep_rew_mean     | -132     |
| time/              |          |
|    fps             | 489      |
|    iterations      | 250      |
|    time_elapsed    | 8367     |
|    total_timesteps | 4096000  |
---------------------------------
Eval num_timesteps=4103424, episode_reward=-127.78 +/- 39.56
Episode length: 65.10 +/- 8.53
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 65.1      |
|    mean_reward          | -128      |
| time/                   |           |
|    total_timesteps      | 4103424   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 1.79e-07  |
|    learning_rate        | 0.819     |
|    loss                 | 0.98      |
|    n_updates            | 1000      |
|    policy_gradient_loss | -9.02e-10 |
|    value_loss           | 1.26      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.2     |
|    ep_rew_mean     | -132     |
| time/              |          |
|    fps             | 489      |
|    iterations      | 251      |
|    time_elapsed    | 8398     |
|    total_timesteps | 4112384  |
---------------------------------
Eval num_timesteps=4113408, episode_reward=-121.79 +/- 24.88
Episode length: 66.10 +/- 13.21
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 66.1     |
|    mean_reward          | -122     |
| time/                   |          |
|    total_timesteps      | 4113408  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.949    |
|    n_updates            | 1004     |
|    policy_gradient_loss | -1e-11   |
|    value_loss           | 1.17     |
--------------------------------------
Eval num_timesteps=4123392, episode_reward=-116.59 +/- 31.65
Episode length: 70.40 +/- 13.31
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.4     |
|    mean_reward     | -117     |
| time/              |          |
|    total_timesteps | 4123392  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.2     |
|    ep_rew_mean     | -126     |
| time/              |          |
|    fps             | 489      |
|    iterations      | 252      |
|    time_elapsed    | 8432     |
|    total_timesteps | 4128768  |
---------------------------------
Eval num_timesteps=4133376, episode_reward=-147.94 +/- 50.21
Episode length: 67.70 +/- 13.97
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 67.7     |
|    mean_reward          | -148     |
| time/                   |          |
|    total_timesteps      | 4133376  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.987    |
|    n_updates            | 1008     |
|    policy_gradient_loss | 2.5e-10  |
|    value_loss           | 1.25     |
--------------------------------------
Eval num_timesteps=4143360, episode_reward=-147.33 +/- 18.49
Episode length: 69.60 +/- 10.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.6     |
|    mean_reward     | -147     |
| time/              |          |
|    total_timesteps | 4143360  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70       |
|    ep_rew_mean     | -127     |
| time/              |          |
|    fps             | 489      |
|    iterations      | 253      |
|    time_elapsed    | 8466     |
|    total_timesteps | 4145152  |
---------------------------------
Eval num_timesteps=4153344, episode_reward=-119.88 +/- 36.76
Episode length: 60.00 +/- 7.44
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 60       |
|    mean_reward          | -120     |
| time/                   |          |
|    total_timesteps      | 4153344  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 1.41     |
|    n_updates            | 1012     |
|    policy_gradient_loss | 1.11e-10 |
|    value_loss           | 1.25     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.2     |
|    ep_rew_mean     | -135     |
| time/              |          |
|    fps             | 489      |
|    iterations      | 254      |
|    time_elapsed    | 8497     |
|    total_timesteps | 4161536  |
---------------------------------
Eval num_timesteps=4163328, episode_reward=-140.35 +/- 18.20
Episode length: 73.80 +/- 11.38
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 73.8     |
|    mean_reward          | -140     |
| time/                   |          |
|    total_timesteps      | 4163328  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 1.14     |
|    n_updates            | 1016     |
|    policy_gradient_loss | 1.18e-09 |
|    value_loss           | 1.27     |
--------------------------------------
Eval num_timesteps=4173312, episode_reward=-127.82 +/- 46.41
Episode length: 68.50 +/- 9.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.5     |
|    mean_reward     | -128     |
| time/              |          |
|    total_timesteps | 4173312  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.1     |
|    ep_rew_mean     | -128     |
| time/              |          |
|    fps             | 489      |
|    iterations      | 255      |
|    time_elapsed    | 8530     |
|    total_timesteps | 4177920  |
---------------------------------
Eval num_timesteps=4183296, episode_reward=-128.64 +/- 52.80
Episode length: 76.10 +/- 10.13
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 76.1      |
|    mean_reward          | -129      |
| time/                   |           |
|    total_timesteps      | 4183296   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 1.13      |
|    n_updates            | 1020      |
|    policy_gradient_loss | -8.39e-10 |
|    value_loss           | 1.27      |
---------------------------------------
Eval num_timesteps=4193280, episode_reward=-144.71 +/- 62.68
Episode length: 69.30 +/- 16.05
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.3     |
|    mean_reward     | -145     |
| time/              |          |
|    total_timesteps | 4193280  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.1     |
|    ep_rew_mean     | -134     |
| time/              |          |
|    fps             | 489      |
|    iterations      | 256      |
|    time_elapsed    | 8563     |
|    total_timesteps | 4194304  |
---------------------------------
Eval num_timesteps=4203264, episode_reward=-108.14 +/- 34.74
Episode length: 68.10 +/- 14.51
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 68.1      |
|    mean_reward          | -108      |
| time/                   |           |
|    total_timesteps      | 4203264   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.819     |
|    loss                 | 0.976     |
|    n_updates            | 1024      |
|    policy_gradient_loss | -5.48e-10 |
|    value_loss           | 1.28      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.1     |
|    ep_rew_mean     | -124     |
| time/              |          |
|    fps             | 489      |
|    iterations      | 257      |
|    time_elapsed    | 8595     |
|    total_timesteps | 4210688  |
---------------------------------
Eval num_timesteps=4213248, episode_reward=-131.32 +/- 25.91
Episode length: 69.40 +/- 11.78
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 69.4     |
|    mean_reward          | -131     |
| time/                   |          |
|    total_timesteps      | 4213248  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.945    |
|    n_updates            | 1028     |
|    policy_gradient_loss | 1.65e-10 |
|    value_loss           | 1.32     |
--------------------------------------
Eval num_timesteps=4223232, episode_reward=-130.26 +/- 34.52
Episode length: 68.50 +/- 11.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.5     |
|    mean_reward     | -130     |
| time/              |          |
|    total_timesteps | 4223232  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.9     |
|    ep_rew_mean     | -135     |
| time/              |          |
|    fps             | 490      |
|    iterations      | 258      |
|    time_elapsed    | 8626     |
|    total_timesteps | 4227072  |
---------------------------------
Eval num_timesteps=4233216, episode_reward=-134.47 +/- 16.75
Episode length: 76.10 +/- 10.59
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 76.1      |
|    mean_reward          | -134      |
| time/                   |           |
|    total_timesteps      | 4233216   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.819     |
|    loss                 | 0.547     |
|    n_updates            | 1032      |
|    policy_gradient_loss | 7.95e-10  |
|    value_loss           | 1.35      |
---------------------------------------
Eval num_timesteps=4243200, episode_reward=-162.68 +/- 54.73
Episode length: 76.00 +/- 13.42
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76       |
|    mean_reward     | -163     |
| time/              |          |
|    total_timesteps | 4243200  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.8     |
|    ep_rew_mean     | -132     |
| time/              |          |
|    fps             | 490      |
|    iterations      | 259      |
|    time_elapsed    | 8659     |
|    total_timesteps | 4243456  |
---------------------------------
Eval num_timesteps=4253184, episode_reward=-124.49 +/- 31.36
Episode length: 66.00 +/- 9.82
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 66       |
|    mean_reward          | -124     |
| time/                   |          |
|    total_timesteps      | 4253184  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.953    |
|    n_updates            | 1036     |
|    policy_gradient_loss | 6.66e-10 |
|    value_loss           | 1.28     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70       |
|    ep_rew_mean     | -132     |
| time/              |          |
|    fps             | 490      |
|    iterations      | 260      |
|    time_elapsed    | 8692     |
|    total_timesteps | 4259840  |
---------------------------------
Eval num_timesteps=4263168, episode_reward=-138.68 +/- 23.56
Episode length: 70.80 +/- 12.35
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 70.8     |
|    mean_reward          | -139     |
| time/                   |          |
|    total_timesteps      | 4263168  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 1.15     |
|    n_updates            | 1040     |
|    policy_gradient_loss | 7.46e-10 |
|    value_loss           | 1.26     |
--------------------------------------
Eval num_timesteps=4273152, episode_reward=-133.26 +/- 54.79
Episode length: 71.70 +/- 5.69
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.7     |
|    mean_reward     | -133     |
| time/              |          |
|    total_timesteps | 4273152  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72.2     |
|    ep_rew_mean     | -132     |
| time/              |          |
|    fps             | 490      |
|    iterations      | 261      |
|    time_elapsed    | 8726     |
|    total_timesteps | 4276224  |
---------------------------------
Eval num_timesteps=4283136, episode_reward=-157.29 +/- 74.52
Episode length: 70.00 +/- 17.80
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 70       |
|    mean_reward          | -157     |
| time/                   |          |
|    total_timesteps      | 4283136  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 1.19e-07 |
|    learning_rate        | 0.819    |
|    loss                 | 1.41     |
|    n_updates            | 1044     |
|    policy_gradient_loss | 3.05e-11 |
|    value_loss           | 1.27     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.5     |
|    ep_rew_mean     | -138     |
| time/              |          |
|    fps             | 490      |
|    iterations      | 262      |
|    time_elapsed    | 8758     |
|    total_timesteps | 4292608  |
---------------------------------
Eval num_timesteps=4293120, episode_reward=-144.08 +/- 47.37
Episode length: 70.80 +/- 11.19
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 70.8     |
|    mean_reward          | -144     |
| time/                   |          |
|    total_timesteps      | 4293120  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.515    |
|    n_updates            | 1048     |
|    policy_gradient_loss | 8.24e-10 |
|    value_loss           | 1.18     |
--------------------------------------
Eval num_timesteps=4303104, episode_reward=-109.50 +/- 49.99
Episode length: 72.30 +/- 8.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.3     |
|    mean_reward     | -110     |
| time/              |          |
|    total_timesteps | 4303104  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.4     |
|    ep_rew_mean     | -136     |
| time/              |          |
|    fps             | 490      |
|    iterations      | 263      |
|    time_elapsed    | 8790     |
|    total_timesteps | 4308992  |
---------------------------------
Eval num_timesteps=4313088, episode_reward=-127.48 +/- 48.34
Episode length: 73.60 +/- 12.28
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 73.6      |
|    mean_reward          | -127      |
| time/                   |           |
|    total_timesteps      | 4313088   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 1.01      |
|    n_updates            | 1052      |
|    policy_gradient_loss | -2.55e-10 |
|    value_loss           | 1.34      |
---------------------------------------
Eval num_timesteps=4323072, episode_reward=-137.87 +/- 26.96
Episode length: 67.30 +/- 9.26
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67.3     |
|    mean_reward     | -138     |
| time/              |          |
|    total_timesteps | 4323072  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.7     |
|    ep_rew_mean     | -132     |
| time/              |          |
|    fps             | 490      |
|    iterations      | 264      |
|    time_elapsed    | 8821     |
|    total_timesteps | 4325376  |
---------------------------------
Eval num_timesteps=4333056, episode_reward=-143.30 +/- 31.91
Episode length: 74.70 +/- 10.78
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 74.7     |
|    mean_reward          | -143     |
| time/                   |          |
|    total_timesteps      | 4333056  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.518    |
|    n_updates            | 1056     |
|    policy_gradient_loss | 5.07e-10 |
|    value_loss           | 1.28     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.7     |
|    ep_rew_mean     | -131     |
| time/              |          |
|    fps             | 490      |
|    iterations      | 265      |
|    time_elapsed    | 8852     |
|    total_timesteps | 4341760  |
---------------------------------
Eval num_timesteps=4343040, episode_reward=-167.94 +/- 39.68
Episode length: 74.70 +/- 12.63
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 74.7     |
|    mean_reward          | -168     |
| time/                   |          |
|    total_timesteps      | 4343040  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 1.19e-07 |
|    learning_rate        | 0.819    |
|    loss                 | 1.03     |
|    n_updates            | 1060     |
|    policy_gradient_loss | 1e-09    |
|    value_loss           | 1.32     |
--------------------------------------
Eval num_timesteps=4353024, episode_reward=-126.70 +/- 21.92
Episode length: 58.40 +/- 7.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 58.4     |
|    mean_reward     | -127     |
| time/              |          |
|    total_timesteps | 4353024  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72.4     |
|    ep_rew_mean     | -135     |
| time/              |          |
|    fps             | 490      |
|    iterations      | 266      |
|    time_elapsed    | 8890     |
|    total_timesteps | 4358144  |
---------------------------------
Eval num_timesteps=4363008, episode_reward=-123.07 +/- 79.82
Episode length: 77.00 +/- 11.23
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 77        |
|    mean_reward          | -123      |
| time/                   |           |
|    total_timesteps      | 4363008   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 1.79e-07  |
|    learning_rate        | 0.819     |
|    loss                 | 0.991     |
|    n_updates            | 1064      |
|    policy_gradient_loss | -2.53e-10 |
|    value_loss           | 1.18      |
---------------------------------------
Eval num_timesteps=4372992, episode_reward=-138.05 +/- 18.04
Episode length: 64.00 +/- 8.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64       |
|    mean_reward     | -138     |
| time/              |          |
|    total_timesteps | 4372992  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.3     |
|    ep_rew_mean     | -133     |
| time/              |          |
|    fps             | 490      |
|    iterations      | 267      |
|    time_elapsed    | 8926     |
|    total_timesteps | 4374528  |
---------------------------------
Eval num_timesteps=4382976, episode_reward=-145.80 +/- 27.21
Episode length: 69.90 +/- 8.22
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 69.9     |
|    mean_reward          | -146     |
| time/                   |          |
|    total_timesteps      | 4382976  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 1.79e-07 |
|    learning_rate        | 0.819    |
|    loss                 | 2.17     |
|    n_updates            | 1068     |
|    policy_gradient_loss | -1.3e-10 |
|    value_loss           | 1.33     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.1     |
|    ep_rew_mean     | -131     |
| time/              |          |
|    fps             | 490      |
|    iterations      | 268      |
|    time_elapsed    | 8958     |
|    total_timesteps | 4390912  |
---------------------------------
Eval num_timesteps=4392960, episode_reward=-134.12 +/- 32.86
Episode length: 75.20 +/- 13.26
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 75.2     |
|    mean_reward          | -134     |
| time/                   |          |
|    total_timesteps      | 4392960  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.448    |
|    n_updates            | 1072     |
|    policy_gradient_loss | -6.9e-10 |
|    value_loss           | 1.22     |
--------------------------------------
Eval num_timesteps=4402944, episode_reward=-147.64 +/- 51.25
Episode length: 66.60 +/- 9.19
---------------------------------
| eval/              |          |
|    mean_ep_length  | 66.6     |
|    mean_reward     | -148     |
| time/              |          |
|    total_timesteps | 4402944  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.3     |
|    ep_rew_mean     | -140     |
| time/              |          |
|    fps             | 490      |
|    iterations      | 269      |
|    time_elapsed    | 8989     |
|    total_timesteps | 4407296  |
---------------------------------
Eval num_timesteps=4412928, episode_reward=-105.57 +/- 52.46
Episode length: 68.60 +/- 13.69
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 68.6      |
|    mean_reward          | -106      |
| time/                   |           |
|    total_timesteps      | 4412928   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.958     |
|    n_updates            | 1076      |
|    policy_gradient_loss | -2.43e-10 |
|    value_loss           | 1.2       |
---------------------------------------
Eval num_timesteps=4422912, episode_reward=-140.70 +/- 29.62
Episode length: 69.20 +/- 11.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.2     |
|    mean_reward     | -141     |
| time/              |          |
|    total_timesteps | 4422912  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.8     |
|    ep_rew_mean     | -132     |
| time/              |          |
|    fps             | 490      |
|    iterations      | 270      |
|    time_elapsed    | 9020     |
|    total_timesteps | 4423680  |
---------------------------------
Eval num_timesteps=4432896, episode_reward=-124.29 +/- 34.94
Episode length: 66.30 +/- 6.23
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 66.3     |
|    mean_reward          | -124     |
| time/                   |          |
|    total_timesteps      | 4432896  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 1.04     |
|    n_updates            | 1080     |
|    policy_gradient_loss | 7.79e-10 |
|    value_loss           | 1.31     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.8     |
|    ep_rew_mean     | -126     |
| time/              |          |
|    fps             | 490      |
|    iterations      | 271      |
|    time_elapsed    | 9051     |
|    total_timesteps | 4440064  |
---------------------------------
Eval num_timesteps=4442880, episode_reward=-125.54 +/- 17.80
Episode length: 67.50 +/- 10.57
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 67.5     |
|    mean_reward          | -126     |
| time/                   |          |
|    total_timesteps      | 4442880  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.991    |
|    n_updates            | 1084     |
|    policy_gradient_loss | 3.79e-10 |
|    value_loss           | 1.17     |
--------------------------------------
Eval num_timesteps=4452864, episode_reward=-138.61 +/- 40.43
Episode length: 62.60 +/- 10.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.6     |
|    mean_reward     | -139     |
| time/              |          |
|    total_timesteps | 4452864  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.4     |
|    ep_rew_mean     | -135     |
| time/              |          |
|    fps             | 490      |
|    iterations      | 272      |
|    time_elapsed    | 9084     |
|    total_timesteps | 4456448  |
---------------------------------
Eval num_timesteps=4462848, episode_reward=-121.12 +/- 50.47
Episode length: 64.30 +/- 9.90
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 64.3     |
|    mean_reward          | -121     |
| time/                   |          |
|    total_timesteps      | 4462848  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.489    |
|    n_updates            | 1088     |
|    policy_gradient_loss | 6.37e-10 |
|    value_loss           | 1.16     |
--------------------------------------
Eval num_timesteps=4472832, episode_reward=-116.76 +/- 37.56
Episode length: 70.50 +/- 10.74
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.5     |
|    mean_reward     | -117     |
| time/              |          |
|    total_timesteps | 4472832  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 67.6     |
|    ep_rew_mean     | -131     |
| time/              |          |
|    fps             | 490      |
|    iterations      | 273      |
|    time_elapsed    | 9116     |
|    total_timesteps | 4472832  |
---------------------------------
Eval num_timesteps=4482816, episode_reward=-132.24 +/- 12.20
Episode length: 67.20 +/- 11.43
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 67.2      |
|    mean_reward          | -132      |
| time/                   |           |
|    total_timesteps      | 4482816   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.939     |
|    n_updates            | 1092      |
|    policy_gradient_loss | -1.87e-10 |
|    value_loss           | 1.25      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.8     |
|    ep_rew_mean     | -132     |
| time/              |          |
|    fps             | 490      |
|    iterations      | 274      |
|    time_elapsed    | 9148     |
|    total_timesteps | 4489216  |
---------------------------------
Eval num_timesteps=4492800, episode_reward=-139.81 +/- 23.64
Episode length: 69.70 +/- 11.43
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 69.7      |
|    mean_reward          | -140      |
| time/                   |           |
|    total_timesteps      | 4492800   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.819     |
|    loss                 | 1.43      |
|    n_updates            | 1096      |
|    policy_gradient_loss | -6.18e-11 |
|    value_loss           | 1.22      |
---------------------------------------
Eval num_timesteps=4502784, episode_reward=-124.68 +/- 41.18
Episode length: 72.20 +/- 15.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.2     |
|    mean_reward     | -125     |
| time/              |          |
|    total_timesteps | 4502784  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69       |
|    ep_rew_mean     | -131     |
| time/              |          |
|    fps             | 490      |
|    iterations      | 275      |
|    time_elapsed    | 9180     |
|    total_timesteps | 4505600  |
---------------------------------
Eval num_timesteps=4512768, episode_reward=-105.52 +/- 50.07
Episode length: 69.00 +/- 11.81
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 69        |
|    mean_reward          | -106      |
| time/                   |           |
|    total_timesteps      | 4512768   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | -2.38e-07 |
|    learning_rate        | 0.819     |
|    loss                 | 1.02      |
|    n_updates            | 1100      |
|    policy_gradient_loss | -6.51e-10 |
|    value_loss           | 1.28      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.4     |
|    ep_rew_mean     | -133     |
| time/              |          |
|    fps             | 490      |
|    iterations      | 276      |
|    time_elapsed    | 9213     |
|    total_timesteps | 4521984  |
---------------------------------
Eval num_timesteps=4522752, episode_reward=-141.18 +/- 24.36
Episode length: 65.30 +/- 12.79
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 65.3      |
|    mean_reward          | -141      |
| time/                   |           |
|    total_timesteps      | 4522752   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 2.22      |
|    n_updates            | 1104      |
|    policy_gradient_loss | -4.73e-11 |
|    value_loss           | 1.33      |
---------------------------------------
Eval num_timesteps=4532736, episode_reward=-137.79 +/- 24.79
Episode length: 64.90 +/- 8.04
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64.9     |
|    mean_reward     | -138     |
| time/              |          |
|    total_timesteps | 4532736  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.7     |
|    ep_rew_mean     | -131     |
| time/              |          |
|    fps             | 490      |
|    iterations      | 277      |
|    time_elapsed    | 9251     |
|    total_timesteps | 4538368  |
---------------------------------
Eval num_timesteps=4542720, episode_reward=-123.12 +/- 26.69
Episode length: 61.60 +/- 7.47
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 61.6      |
|    mean_reward          | -123      |
| time/                   |           |
|    total_timesteps      | 4542720   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.819     |
|    loss                 | 0.967     |
|    n_updates            | 1108      |
|    policy_gradient_loss | -1e-09    |
|    value_loss           | 1.22      |
---------------------------------------
Eval num_timesteps=4552704, episode_reward=-135.65 +/- 50.15
Episode length: 69.80 +/- 15.70
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.8     |
|    mean_reward     | -136     |
| time/              |          |
|    total_timesteps | 4552704  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.6     |
|    ep_rew_mean     | -132     |
| time/              |          |
|    fps             | 490      |
|    iterations      | 278      |
|    time_elapsed    | 9281     |
|    total_timesteps | 4554752  |
---------------------------------
Eval num_timesteps=4562688, episode_reward=-118.72 +/- 62.64
Episode length: 72.50 +/- 13.07
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 72.5      |
|    mean_reward          | -119      |
| time/                   |           |
|    total_timesteps      | 4562688   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.964     |
|    n_updates            | 1112      |
|    policy_gradient_loss | -1.59e-10 |
|    value_loss           | 1.3       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.7     |
|    ep_rew_mean     | -127     |
| time/              |          |
|    fps             | 490      |
|    iterations      | 279      |
|    time_elapsed    | 9312     |
|    total_timesteps | 4571136  |
---------------------------------
Eval num_timesteps=4572672, episode_reward=-132.96 +/- 29.43
Episode length: 69.70 +/- 9.69
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 69.7     |
|    mean_reward          | -133     |
| time/                   |          |
|    total_timesteps      | 4572672  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 1.19e-07 |
|    learning_rate        | 0.819    |
|    loss                 | 1.02     |
|    n_updates            | 1116     |
|    policy_gradient_loss | 1.25e-10 |
|    value_loss           | 1.28     |
--------------------------------------
Eval num_timesteps=4582656, episode_reward=-130.83 +/- 44.08
Episode length: 70.30 +/- 7.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.3     |
|    mean_reward     | -131     |
| time/              |          |
|    total_timesteps | 4582656  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.3     |
|    ep_rew_mean     | -126     |
| time/              |          |
|    fps             | 490      |
|    iterations      | 280      |
|    time_elapsed    | 9345     |
|    total_timesteps | 4587520  |
---------------------------------
Eval num_timesteps=4592640, episode_reward=-130.79 +/- 60.18
Episode length: 69.00 +/- 10.00
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 69       |
|    mean_reward          | -131     |
| time/                   |          |
|    total_timesteps      | 4592640  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 1.99     |
|    n_updates            | 1120     |
|    policy_gradient_loss | 3.52e-10 |
|    value_loss           | 1.3      |
--------------------------------------
Eval num_timesteps=4602624, episode_reward=-136.83 +/- 30.10
Episode length: 64.30 +/- 11.57
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64.3     |
|    mean_reward     | -137     |
| time/              |          |
|    total_timesteps | 4602624  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.5     |
|    ep_rew_mean     | -127     |
| time/              |          |
|    fps             | 490      |
|    iterations      | 281      |
|    time_elapsed    | 9379     |
|    total_timesteps | 4603904  |
---------------------------------
Eval num_timesteps=4612608, episode_reward=-113.21 +/- 28.30
Episode length: 70.20 +/- 12.76
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 70.2      |
|    mean_reward          | -113      |
| time/                   |           |
|    total_timesteps      | 4612608   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 1.19e-07  |
|    learning_rate        | 0.819     |
|    loss                 | 0.941     |
|    n_updates            | 1124      |
|    policy_gradient_loss | -2.55e-11 |
|    value_loss           | 1.21      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.4     |
|    ep_rew_mean     | -131     |
| time/              |          |
|    fps             | 491      |
|    iterations      | 282      |
|    time_elapsed    | 9409     |
|    total_timesteps | 4620288  |
---------------------------------
Eval num_timesteps=4622592, episode_reward=-139.21 +/- 16.22
Episode length: 70.70 +/- 10.80
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 70.7     |
|    mean_reward          | -139     |
| time/                   |          |
|    total_timesteps      | 4622592  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.884    |
|    n_updates            | 1128     |
|    policy_gradient_loss | 3.62e-10 |
|    value_loss           | 1.22     |
--------------------------------------
Eval num_timesteps=4632576, episode_reward=-134.66 +/- 17.50
Episode length: 66.80 +/- 9.92
---------------------------------
| eval/              |          |
|    mean_ep_length  | 66.8     |
|    mean_reward     | -135     |
| time/              |          |
|    total_timesteps | 4632576  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.2     |
|    ep_rew_mean     | -135     |
| time/              |          |
|    fps             | 490      |
|    iterations      | 283      |
|    time_elapsed    | 9444     |
|    total_timesteps | 4636672  |
---------------------------------
Eval num_timesteps=4642560, episode_reward=-140.46 +/- 20.43
Episode length: 66.70 +/- 10.65
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 66.7      |
|    mean_reward          | -140      |
| time/                   |           |
|    total_timesteps      | 4642560   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 1.05      |
|    n_updates            | 1132      |
|    policy_gradient_loss | -7.06e-10 |
|    value_loss           | 1.21      |
---------------------------------------
Eval num_timesteps=4652544, episode_reward=-144.08 +/- 27.65
Episode length: 67.80 +/- 12.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67.8     |
|    mean_reward     | -144     |
| time/              |          |
|    total_timesteps | 4652544  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 67.4     |
|    ep_rew_mean     | -138     |
| time/              |          |
|    fps             | 490      |
|    iterations      | 284      |
|    time_elapsed    | 9484     |
|    total_timesteps | 4653056  |
---------------------------------
Eval num_timesteps=4662528, episode_reward=-157.06 +/- 32.71
Episode length: 76.60 +/- 17.00
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 76.6      |
|    mean_reward          | -157      |
| time/                   |           |
|    total_timesteps      | 4662528   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.901     |
|    n_updates            | 1136      |
|    policy_gradient_loss | -4.78e-10 |
|    value_loss           | 1.2       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.8     |
|    ep_rew_mean     | -130     |
| time/              |          |
|    fps             | 490      |
|    iterations      | 285      |
|    time_elapsed    | 9515     |
|    total_timesteps | 4669440  |
---------------------------------
Eval num_timesteps=4672512, episode_reward=-122.90 +/- 31.22
Episode length: 73.40 +/- 12.94
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 73.4     |
|    mean_reward          | -123     |
| time/                   |          |
|    total_timesteps      | 4672512  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.921    |
|    n_updates            | 1140     |
|    policy_gradient_loss | 7.93e-10 |
|    value_loss           | 1.3      |
--------------------------------------
Eval num_timesteps=4682496, episode_reward=-110.19 +/- 37.76
Episode length: 76.30 +/- 23.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 76.3     |
|    mean_reward     | -110     |
| time/              |          |
|    total_timesteps | 4682496  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.6     |
|    ep_rew_mean     | -134     |
| time/              |          |
|    fps             | 490      |
|    iterations      | 286      |
|    time_elapsed    | 9546     |
|    total_timesteps | 4685824  |
---------------------------------
Eval num_timesteps=4692480, episode_reward=-125.13 +/- 46.56
Episode length: 79.20 +/- 11.97
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 79.2      |
|    mean_reward          | -125      |
| time/                   |           |
|    total_timesteps      | 4692480   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.539     |
|    n_updates            | 1144      |
|    policy_gradient_loss | -4.72e-10 |
|    value_loss           | 1.17      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.1     |
|    ep_rew_mean     | -122     |
| time/              |          |
|    fps             | 490      |
|    iterations      | 287      |
|    time_elapsed    | 9577     |
|    total_timesteps | 4702208  |
---------------------------------
Eval num_timesteps=4702464, episode_reward=-141.98 +/- 26.45
Episode length: 73.50 +/- 10.77
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 73.5      |
|    mean_reward          | -142      |
| time/                   |           |
|    total_timesteps      | 4702464   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.867     |
|    n_updates            | 1148      |
|    policy_gradient_loss | -1.14e-09 |
|    value_loss           | 1.26      |
---------------------------------------
Eval num_timesteps=4712448, episode_reward=-99.04 +/- 57.74
Episode length: 75.60 +/- 14.27
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.6     |
|    mean_reward     | -99      |
| time/              |          |
|    total_timesteps | 4712448  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.9     |
|    ep_rew_mean     | -131     |
| time/              |          |
|    fps             | 491      |
|    iterations      | 288      |
|    time_elapsed    | 9609     |
|    total_timesteps | 4718592  |
---------------------------------
Eval num_timesteps=4722432, episode_reward=-132.04 +/- 56.83
Episode length: 75.20 +/- 13.78
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 75.2     |
|    mean_reward          | -132     |
| time/                   |          |
|    total_timesteps      | 4722432  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 1.03     |
|    n_updates            | 1152     |
|    policy_gradient_loss | 9.37e-10 |
|    value_loss           | 1.32     |
--------------------------------------
Eval num_timesteps=4732416, episode_reward=-136.56 +/- 37.81
Episode length: 70.20 +/- 11.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.2     |
|    mean_reward     | -137     |
| time/              |          |
|    total_timesteps | 4732416  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.7     |
|    ep_rew_mean     | -127     |
| time/              |          |
|    fps             | 491      |
|    iterations      | 289      |
|    time_elapsed    | 9642     |
|    total_timesteps | 4734976  |
---------------------------------
Eval num_timesteps=4742400, episode_reward=-126.94 +/- 24.74
Episode length: 64.30 +/- 8.88
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 64.3     |
|    mean_reward          | -127     |
| time/                   |          |
|    total_timesteps      | 4742400  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.886    |
|    n_updates            | 1156     |
|    policy_gradient_loss | 1.38e-09 |
|    value_loss           | 1.23     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.4     |
|    ep_rew_mean     | -134     |
| time/              |          |
|    fps             | 491      |
|    iterations      | 290      |
|    time_elapsed    | 9675     |
|    total_timesteps | 4751360  |
---------------------------------
Eval num_timesteps=4752384, episode_reward=-129.86 +/- 49.13
Episode length: 66.60 +/- 9.83
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 66.6     |
|    mean_reward          | -130     |
| time/                   |          |
|    total_timesteps      | 4752384  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.916    |
|    n_updates            | 1160     |
|    policy_gradient_loss | 1.5e-09  |
|    value_loss           | 1.32     |
--------------------------------------
Eval num_timesteps=4762368, episode_reward=-115.72 +/- 28.74
Episode length: 65.10 +/- 9.95
---------------------------------
| eval/              |          |
|    mean_ep_length  | 65.1     |
|    mean_reward     | -116     |
| time/              |          |
|    total_timesteps | 4762368  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.4     |
|    ep_rew_mean     | -128     |
| time/              |          |
|    fps             | 491      |
|    iterations      | 291      |
|    time_elapsed    | 9708     |
|    total_timesteps | 4767744  |
---------------------------------
Eval num_timesteps=4772352, episode_reward=-148.51 +/- 29.99
Episode length: 68.30 +/- 9.81
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 68.3     |
|    mean_reward          | -149     |
| time/                   |          |
|    total_timesteps      | 4772352  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 2.53     |
|    n_updates            | 1164     |
|    policy_gradient_loss | 5.13e-10 |
|    value_loss           | 1.22     |
--------------------------------------
Eval num_timesteps=4782336, episode_reward=-147.06 +/- 35.40
Episode length: 70.80 +/- 12.84
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.8     |
|    mean_reward     | -147     |
| time/              |          |
|    total_timesteps | 4782336  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70.3     |
|    ep_rew_mean     | -125     |
| time/              |          |
|    fps             | 490      |
|    iterations      | 292      |
|    time_elapsed    | 9744     |
|    total_timesteps | 4784128  |
---------------------------------
Eval num_timesteps=4792320, episode_reward=-141.59 +/- 32.19
Episode length: 70.80 +/- 11.59
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 70.8      |
|    mean_reward          | -142      |
| time/                   |           |
|    total_timesteps      | 4792320   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.505     |
|    n_updates            | 1168      |
|    policy_gradient_loss | -2.39e-10 |
|    value_loss           | 1.2       |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.6     |
|    ep_rew_mean     | -135     |
| time/              |          |
|    fps             | 491      |
|    iterations      | 293      |
|    time_elapsed    | 9776     |
|    total_timesteps | 4800512  |
---------------------------------
Eval num_timesteps=4802304, episode_reward=-125.62 +/- 29.66
Episode length: 67.90 +/- 11.89
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 67.9      |
|    mean_reward          | -126      |
| time/                   |           |
|    total_timesteps      | 4802304   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.557     |
|    n_updates            | 1172      |
|    policy_gradient_loss | -5.17e-10 |
|    value_loss           | 1.25      |
---------------------------------------
Eval num_timesteps=4812288, episode_reward=-151.47 +/- 59.26
Episode length: 68.20 +/- 15.20
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.2     |
|    mean_reward     | -151     |
| time/              |          |
|    total_timesteps | 4812288  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.9     |
|    ep_rew_mean     | -127     |
| time/              |          |
|    fps             | 491      |
|    iterations      | 294      |
|    time_elapsed    | 9809     |
|    total_timesteps | 4816896  |
---------------------------------
Eval num_timesteps=4822272, episode_reward=-128.06 +/- 49.08
Episode length: 69.30 +/- 10.94
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 69.3      |
|    mean_reward          | -128      |
| time/                   |           |
|    total_timesteps      | 4822272   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 2.01      |
|    n_updates            | 1176      |
|    policy_gradient_loss | -1.17e-09 |
|    value_loss           | 1.22      |
---------------------------------------
Eval num_timesteps=4832256, episode_reward=-116.94 +/- 38.67
Episode length: 66.40 +/- 9.80
---------------------------------
| eval/              |          |
|    mean_ep_length  | 66.4     |
|    mean_reward     | -117     |
| time/              |          |
|    total_timesteps | 4832256  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 67.9     |
|    ep_rew_mean     | -129     |
| time/              |          |
|    fps             | 491      |
|    iterations      | 295      |
|    time_elapsed    | 9841     |
|    total_timesteps | 4833280  |
---------------------------------
Eval num_timesteps=4842240, episode_reward=-128.35 +/- 38.16
Episode length: 65.20 +/- 11.69
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 65.2     |
|    mean_reward          | -128     |
| time/                   |          |
|    total_timesteps      | 4842240  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.985    |
|    n_updates            | 1180     |
|    policy_gradient_loss | 1.29e-09 |
|    value_loss           | 1.26     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.2     |
|    ep_rew_mean     | -140     |
| time/              |          |
|    fps             | 491      |
|    iterations      | 296      |
|    time_elapsed    | 9876     |
|    total_timesteps | 4849664  |
---------------------------------
Eval num_timesteps=4852224, episode_reward=-128.37 +/- 12.03
Episode length: 71.20 +/- 11.02
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 71.2     |
|    mean_reward          | -128     |
| time/                   |          |
|    total_timesteps      | 4852224  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 1.19e-07 |
|    learning_rate        | 0.819    |
|    loss                 | 0.967    |
|    n_updates            | 1184     |
|    policy_gradient_loss | 1.08e-09 |
|    value_loss           | 1.27     |
--------------------------------------
Eval num_timesteps=4862208, episode_reward=-139.38 +/- 23.12
Episode length: 71.80 +/- 10.58
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.8     |
|    mean_reward     | -139     |
| time/              |          |
|    total_timesteps | 4862208  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.8     |
|    ep_rew_mean     | -133     |
| time/              |          |
|    fps             | 490      |
|    iterations      | 297      |
|    time_elapsed    | 9910     |
|    total_timesteps | 4866048  |
---------------------------------
Eval num_timesteps=4872192, episode_reward=-137.67 +/- 24.47
Episode length: 72.20 +/- 12.71
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 72.2     |
|    mean_reward          | -138     |
| time/                   |          |
|    total_timesteps      | 4872192  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 1.18     |
|    n_updates            | 1188     |
|    policy_gradient_loss | 6.98e-10 |
|    value_loss           | 1.32     |
--------------------------------------
Eval num_timesteps=4882176, episode_reward=-141.76 +/- 30.50
Episode length: 67.00 +/- 12.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67       |
|    mean_reward     | -142     |
| time/              |          |
|    total_timesteps | 4882176  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.2     |
|    ep_rew_mean     | -121     |
| time/              |          |
|    fps             | 491      |
|    iterations      | 298      |
|    time_elapsed    | 9942     |
|    total_timesteps | 4882432  |
---------------------------------
Eval num_timesteps=4892160, episode_reward=-125.89 +/- 40.79
Episode length: 63.80 +/- 9.72
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 63.8     |
|    mean_reward          | -126     |
| time/                   |          |
|    total_timesteps      | 4892160  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 1.58     |
|    n_updates            | 1192     |
|    policy_gradient_loss | 3.04e-10 |
|    value_loss           | 1.27     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.6     |
|    ep_rew_mean     | -123     |
| time/              |          |
|    fps             | 491      |
|    iterations      | 299      |
|    time_elapsed    | 9975     |
|    total_timesteps | 4898816  |
---------------------------------
Eval num_timesteps=4902144, episode_reward=-145.38 +/- 76.99
Episode length: 75.30 +/- 16.65
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 75.3     |
|    mean_reward          | -145     |
| time/                   |          |
|    total_timesteps      | 4902144  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.936    |
|    n_updates            | 1196     |
|    policy_gradient_loss | 2.5e-10  |
|    value_loss           | 1.28     |
--------------------------------------
Eval num_timesteps=4912128, episode_reward=-122.79 +/- 26.84
Episode length: 67.80 +/- 11.29
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67.8     |
|    mean_reward     | -123     |
| time/              |          |
|    total_timesteps | 4912128  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.9     |
|    ep_rew_mean     | -131     |
| time/              |          |
|    fps             | 491      |
|    iterations      | 300      |
|    time_elapsed    | 10007    |
|    total_timesteps | 4915200  |
---------------------------------
Eval num_timesteps=4922112, episode_reward=-131.37 +/- 30.41
Episode length: 72.40 +/- 13.58
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 72.4     |
|    mean_reward          | -131     |
| time/                   |          |
|    total_timesteps      | 4922112  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.955    |
|    n_updates            | 1200     |
|    policy_gradient_loss | 4.2e-10  |
|    value_loss           | 1.26     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.5     |
|    ep_rew_mean     | -131     |
| time/              |          |
|    fps             | 491      |
|    iterations      | 301      |
|    time_elapsed    | 10038    |
|    total_timesteps | 4931584  |
---------------------------------
Eval num_timesteps=4932096, episode_reward=-119.03 +/- 44.58
Episode length: 66.20 +/- 13.28
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 66.2      |
|    mean_reward          | -119      |
| time/                   |           |
|    total_timesteps      | 4932096   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.991     |
|    n_updates            | 1204      |
|    policy_gradient_loss | -7.64e-11 |
|    value_loss           | 1.17      |
---------------------------------------
Eval num_timesteps=4942080, episode_reward=-138.04 +/- 31.21
Episode length: 67.70 +/- 12.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 67.7     |
|    mean_reward     | -138     |
| time/              |          |
|    total_timesteps | 4942080  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 70       |
|    ep_rew_mean     | -134     |
| time/              |          |
|    fps             | 491      |
|    iterations      | 302      |
|    time_elapsed    | 10071    |
|    total_timesteps | 4947968  |
---------------------------------
Eval num_timesteps=4952064, episode_reward=-148.69 +/- 23.94
Episode length: 75.00 +/- 10.06
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 75       |
|    mean_reward          | -149     |
| time/                   |          |
|    total_timesteps      | 4952064  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 1.19e-07 |
|    learning_rate        | 0.819    |
|    loss                 | 2.04     |
|    n_updates            | 1208     |
|    policy_gradient_loss | 3.4e-10  |
|    value_loss           | 1.25     |
--------------------------------------
Eval num_timesteps=4962048, episode_reward=-143.86 +/- 30.86
Episode length: 70.10 +/- 11.07
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.1     |
|    mean_reward     | -144     |
| time/              |          |
|    total_timesteps | 4962048  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.3     |
|    ep_rew_mean     | -129     |
| time/              |          |
|    fps             | 491      |
|    iterations      | 303      |
|    time_elapsed    | 10104    |
|    total_timesteps | 4964352  |
---------------------------------
Eval num_timesteps=4972032, episode_reward=-124.88 +/- 50.90
Episode length: 68.50 +/- 11.39
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 68.5     |
|    mean_reward          | -125     |
| time/                   |          |
|    total_timesteps      | 4972032  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.964    |
|    n_updates            | 1212     |
|    policy_gradient_loss | 6.52e-10 |
|    value_loss           | 1.29     |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 71.2     |
|    ep_rew_mean     | -124     |
| time/              |          |
|    fps             | 491      |
|    iterations      | 304      |
|    time_elapsed    | 10137    |
|    total_timesteps | 4980736  |
---------------------------------
Eval num_timesteps=4982016, episode_reward=-135.83 +/- 18.02
Episode length: 68.50 +/- 8.51
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 68.5     |
|    mean_reward          | -136     |
| time/                   |          |
|    total_timesteps      | 4982016  |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 1.05     |
|    n_updates            | 1216     |
|    policy_gradient_loss | 6.27e-10 |
|    value_loss           | 1.24     |
--------------------------------------
Eval num_timesteps=4992000, episode_reward=-118.82 +/- 64.64
Episode length: 73.40 +/- 11.16
---------------------------------
| eval/              |          |
|    mean_ep_length  | 73.4     |
|    mean_reward     | -119     |
| time/              |          |
|    total_timesteps | 4992000  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.4     |
|    ep_rew_mean     | -129     |
| time/              |          |
|    fps             | 491      |
|    iterations      | 305      |
|    time_elapsed    | 10169    |
|    total_timesteps | 4997120  |
---------------------------------
Eval num_timesteps=5001984, episode_reward=-116.74 +/- 49.20
Episode length: 71.70 +/- 11.90
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 71.7      |
|    mean_reward          | -117      |
| time/                   |           |
|    total_timesteps      | 5001984   |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | -2.38e-07 |
|    learning_rate        | 0.819     |
|    loss                 | 0.992     |
|    n_updates            | 1220      |
|    policy_gradient_loss | 5.18e-10  |
|    value_loss           | 1.26      |
---------------------------------------
Eval num_timesteps=5011968, episode_reward=-118.90 +/- 50.20
Episode length: 72.40 +/- 10.97
---------------------------------
| eval/              |          |
|    mean_ep_length  | 72.4     |
|    mean_reward     | -119     |
| time/              |          |
|    total_timesteps | 5011968  |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 72       |
|    ep_rew_mean     | -124     |
| time/              |          |
|    fps             | 491      |
|    iterations      | 306      |
|    time_elapsed    | 10200    |
|    total_timesteps | 5013504  |
---------------------------------
Saving to logs/ppo_lstm/LunarLander-v2_4
wandb: Waiting for W&B process to finish... (success).
wandb: 
wandb: Run history:
wandb:        eval/mean_ep_length ‚ñÜ‚ñá‚ñÑ‚ñÖ‚ñÑ‚ñà‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñÉ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñÑ‚ñÖ‚ñÜ‚ñÉ‚ñÖ‚ñá‚ñÜ‚ñá‚ñÜ‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÅ‚ñÜ‚ñÖ‚ñà‚ñÑ‚ñá
wandb:           eval/mean_reward ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÇ‚ñÜ‚ñÑ‚ñá‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÜ‚ñÑ‚ñá‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÅ‚ñÜ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÇ‚ñÉ‚ñÜ‚ñÖ‚ñÑ‚ñÜ‚ñÉ‚ñÉ‚ñÖ‚ñÜ‚ñÑ‚ñà‚ñÜ‚ñÇ
wandb:                global_step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà
wandb:        rollout/ep_len_mean ‚ñÑ‚ñÖ‚ñÉ‚ñÑ‚ñà‚ñá‚ñÖ‚ñÖ‚ñÑ‚ñá‚ñÖ‚ñÖ‚ñÜ‚ñà‚ñà‚ñÑ‚ñÑ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñá‚ñÑ‚ñÑ‚ñÖ‚ñÅ‚ñÖ‚ñÑ‚ñÖ‚ñÜ‚ñÖ‚ñà‚ñÑ‚ñá‚ñÖ‚ñÖ‚ñÑ‚ñà‚ñà
wandb:        rollout/ep_rew_mean ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñà‚ñÖ‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÖ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÅ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñá‚ñÖ‚ñÇ‚ñà
wandb:                   time/fps ‚ñà‚ñÖ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ
wandb:            train/approx_kl ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:        train/clip_fraction ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:           train/clip_range ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:         train/entropy_loss ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   train/explained_variance ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
wandb:        train/learning_rate ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:                 train/loss ‚ñÖ‚ñÉ‚ñÉ‚ñÅ‚ñÖ‚ñÉ‚ñà‚ñá‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÉ‚ñÅ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÉ‚ñÅ‚ñÖ‚ñÉ‚ñÅ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÅ‚ñÅ‚ñá‚ñÉ‚ñÉ‚ñÉ
wandb: train/policy_gradient_loss ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñà‚ñÑ‚ñÅ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñá‚ñÉ‚ñÜ‚ñÜ‚ñÖ‚ñá‚ñá‚ñÜ
wandb:           train/value_loss ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: 
wandb: Run summary:
wandb:        eval/mean_ep_length 72.4
wandb:           eval/mean_reward -118.8992
wandb:                global_step 5013504
wandb:        rollout/ep_len_mean 71.99
wandb:        rollout/ep_rew_mean -124.43533
wandb:                   time/fps 491.0
wandb:            train/approx_kl 0.0
wandb:        train/clip_fraction 0.0
wandb:           train/clip_range 0.938
wandb:         train/entropy_loss 0.0
wandb:   train/explained_variance -0.0
wandb:        train/learning_rate 0.819
wandb:                 train/loss 0.99246
wandb: train/policy_gradient_loss 0.0
wandb:           train/value_loss 1.26372
wandb: 
wandb: üöÄ View run LunarLander-v2__ppo_lstm__268318194__1690461402 at: https://wandb.ai/awesomepossum/thesis/runs/xu42jib0
wandb: Synced 7 W&B file(s), 0 media file(s), 3 artifact file(s) and 2 other file(s)
wandb: Find logs at: ./wandb/run-20230727_073643-xu42jib0/logs
Traceback (most recent call last):
  File "/home/ftn0813/projects/SparseArch/rl-baselines3-zoo/train.py", line 1, in <module>
    from rl_zoo3.train import train
  File "/home/ftn0813/projects/SparseArch/rl-baselines3-zoo/rl_zoo3/__init__.py", line 6, in <module>
    import rl_zoo3.gym_patches  # noqa: F401
  File "/home/ftn0813/projects/SparseArch/rl-baselines3-zoo/rl_zoo3/gym_patches.py", line 12, in <module>
    import gym  # noqa: E402
ModuleNotFoundError: No module named 'gym'
2023-07-31 20:01:51.540396: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
wandb: Currently logged in as: nhartzler (awesomepossum). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.15.7 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.14.1
wandb: Run data is saved locally in /home/ftn0813/projects/SparseArch/wandb/run-20230731_200153-zl0wjl5x
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run LunarLander-v2__ppo_lstm__1250940563__1690851710
wandb: ‚≠êÔ∏è View project at https://wandb.ai/awesomepossum/thesis
wandb: üöÄ View run at https://wandb.ai/awesomepossum/thesis/runs/zl0wjl5x
========== LunarLander-v2 ==========
Seed: 1250940563
Loading hyperparameters from: ga_tuned_configs/ga_baseline_tuned.yml
Default hyperparameters for environment (ones being tuned will be overridden):
OrderedDict([('batch_size', 128),
             ('clip_range', 0.938),
             ('ent_coef', 0.953),
             ('gae_lambda', 0.833),
             ('gamma', 0.708),
             ('learning_rate', 0.819),
             ('n_envs', 32),
             ('n_epochs', 4),
             ('n_steps', 512),
             ('n_timesteps', 5000000.0),
             ('normalize', True),
             ('policy', 'MlpLstmPolicy'),
             ('policy_kwargs',
              'dict( ortho_init=False, activation_fn=nn.ReLU, '
              'lstm_hidden_size=64, enable_critic_lstm=True, '
              'net_arch=dict(pi=[64], vf=[64]) )'),
             ('vf_coef', 0.827)])
Using 32 environments
Creating test environment
Normalization activated: {'gamma': 0.708, 'norm_reward': False, 'training': False}
Normalization activated: {'gamma': 0.708}
Using cuda device
Log path: logs/ppo_lstm/LunarLander-v2_5
Logging to runs/LunarLander-v2__ppo_lstm__1250940563__1690851710/LunarLander-v2/RecurrentPPO_1
2023-07-31 20:02:02.208428: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Eval num_timesteps=9984, episode_reward=-119.97 +/- 50.14
Episode length: 71.50 +/- 14.51
---------------------------------
| eval/              |          |
|    mean_ep_length  | 71.5     |
|    mean_reward     | -120     |
| time/              |          |
|    total_timesteps | 9984     |
---------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 93       |
|    ep_rew_mean     | -216     |
| time/              |          |
|    fps             | 8837     |
|    iterations      | 1        |
|    time_elapsed    | 1        |
|    total_timesteps | 16384    |
---------------------------------
Eval num_timesteps=19968, episode_reward=-638.46 +/- 139.02
Episode length: 72.30 +/- 9.87
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 72.3      |
|    mean_reward          | -638      |
| time/                   |           |
|    total_timesteps      | 19968     |
| train/                  |           |
|    approx_kl            | 3782.0837 |
|    clip_fraction        | 0.998     |
|    clip_range           | 0.938     |
|    entropy_loss         | -0.00271  |
|    explained_variance   | 0.00416   |
|    learning_rate        | 0.819     |
|    loss                 | 1.3       |
|    n_updates            | 4         |
|    policy_gradient_loss | 0.145     |
|    value_loss           | 4.27e+03  |
---------------------------------------
Eval num_timesteps=29952, episode_reward=-571.08 +/- 166.91
Episode length: 68.20 +/- 9.44
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.2     |
|    mean_reward     | -571     |
| time/              |          |
|    total_timesteps | 29952    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 67.7     |
|    ep_rew_mean     | -582     |
| time/              |          |
|    fps             | 676      |
|    iterations      | 2        |
|    time_elapsed    | 48       |
|    total_timesteps | 32768    |
---------------------------------
Eval num_timesteps=39936, episode_reward=-527.47 +/- 128.13
Episode length: 61.20 +/- 8.28
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 61.2      |
|    mean_reward          | -527      |
| time/                   |           |
|    total_timesteps      | 39936     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.576     |
|    n_updates            | 8         |
|    policy_gradient_loss | -9.01e-10 |
|    value_loss           | 1.05      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 65.1     |
|    ep_rew_mean     | -563     |
| time/              |          |
|    fps             | 556      |
|    iterations      | 3        |
|    time_elapsed    | 88       |
|    total_timesteps | 49152    |
---------------------------------
Eval num_timesteps=49920, episode_reward=-616.71 +/- 199.49
Episode length: 69.30 +/- 12.96
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 69.3      |
|    mean_reward          | -617      |
| time/                   |           |
|    total_timesteps      | 49920     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.516     |
|    n_updates            | 12        |
|    policy_gradient_loss | -4.42e-10 |
|    value_loss           | 0.694     |
---------------------------------------
Eval num_timesteps=59904, episode_reward=-715.77 +/- 169.61
Episode length: 75.20 +/- 10.93
---------------------------------
| eval/              |          |
|    mean_ep_length  | 75.2     |
|    mean_reward     | -716     |
| time/              |          |
|    total_timesteps | 59904    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.7     |
|    ep_rew_mean     | -584     |
| time/              |          |
|    fps             | 492      |
|    iterations      | 4        |
|    time_elapsed    | 132      |
|    total_timesteps | 65536    |
---------------------------------
Eval num_timesteps=69888, episode_reward=-564.29 +/- 92.22
Episode length: 66.00 +/- 7.91
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 66       |
|    mean_reward          | -564     |
| time/                   |          |
|    total_timesteps      | 69888    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.447    |
|    n_updates            | 16       |
|    policy_gradient_loss | 8.31e-10 |
|    value_loss           | 0.682    |
--------------------------------------
Eval num_timesteps=79872, episode_reward=-592.11 +/- 140.02
Episode length: 66.30 +/- 8.91
---------------------------------
| eval/              |          |
|    mean_ep_length  | 66.3     |
|    mean_reward     | -592     |
| time/              |          |
|    total_timesteps | 79872    |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 66.9     |
|    ep_rew_mean     | -585     |
| time/              |          |
|    fps             | 463      |
|    iterations      | 5        |
|    time_elapsed    | 176      |
|    total_timesteps | 81920    |
---------------------------------
Eval num_timesteps=89856, episode_reward=-608.01 +/- 153.78
Episode length: 69.80 +/- 13.68
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 69.8     |
|    mean_reward          | -608     |
| time/                   |          |
|    total_timesteps      | 89856    |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 1.19e-07 |
|    learning_rate        | 0.819    |
|    loss                 | 0.628    |
|    n_updates            | 20       |
|    policy_gradient_loss | 1.53e-10 |
|    value_loss           | 0.681    |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 66.7     |
|    ep_rew_mean     | -573     |
| time/              |          |
|    fps             | 446      |
|    iterations      | 6        |
|    time_elapsed    | 220      |
|    total_timesteps | 98304    |
---------------------------------
Eval num_timesteps=99840, episode_reward=-561.51 +/- 106.10
Episode length: 64.60 +/- 9.43
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 64.6      |
|    mean_reward          | -562      |
| time/                   |           |
|    total_timesteps      | 99840     |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.355     |
|    n_updates            | 24        |
|    policy_gradient_loss | -3.21e-10 |
|    value_loss           | 0.606     |
---------------------------------------
Eval num_timesteps=109824, episode_reward=-566.89 +/- 92.34
Episode length: 66.50 +/- 8.09
---------------------------------
| eval/              |          |
|    mean_ep_length  | 66.5     |
|    mean_reward     | -567     |
| time/              |          |
|    total_timesteps | 109824   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 66.9     |
|    ep_rew_mean     | -577     |
| time/              |          |
|    fps             | 431      |
|    iterations      | 7        |
|    time_elapsed    | 266      |
|    total_timesteps | 114688   |
---------------------------------
Eval num_timesteps=119808, episode_reward=-566.99 +/- 143.98
Episode length: 68.00 +/- 10.70
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 68       |
|    mean_reward          | -567     |
| time/                   |          |
|    total_timesteps      | 119808   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.361    |
|    n_updates            | 28       |
|    policy_gradient_loss | 1.86e-10 |
|    value_loss           | 0.664    |
--------------------------------------
Eval num_timesteps=129792, episode_reward=-513.82 +/- 121.72
Episode length: 62.00 +/- 8.33
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62       |
|    mean_reward     | -514     |
| time/              |          |
|    total_timesteps | 129792   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 66.6     |
|    ep_rew_mean     | -548     |
| time/              |          |
|    fps             | 423      |
|    iterations      | 8        |
|    time_elapsed    | 309      |
|    total_timesteps | 131072   |
---------------------------------
Eval num_timesteps=139776, episode_reward=-605.99 +/- 123.16
Episode length: 72.80 +/- 15.37
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 72.8     |
|    mean_reward          | -606     |
| time/                   |          |
|    total_timesteps      | 139776   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.727    |
|    n_updates            | 32       |
|    policy_gradient_loss | 1.69e-10 |
|    value_loss           | 0.704    |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 67.2     |
|    ep_rew_mean     | -575     |
| time/              |          |
|    fps             | 417      |
|    iterations      | 9        |
|    time_elapsed    | 353      |
|    total_timesteps | 147456   |
---------------------------------
Eval num_timesteps=149760, episode_reward=-578.49 +/- 144.48
Episode length: 65.40 +/- 8.86
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 65.4      |
|    mean_reward          | -578      |
| time/                   |           |
|    total_timesteps      | 149760    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.452     |
|    n_updates            | 36        |
|    policy_gradient_loss | -3.78e-10 |
|    value_loss           | 0.697     |
---------------------------------------
Eval num_timesteps=159744, episode_reward=-515.26 +/- 171.41
Episode length: 61.90 +/- 9.22
---------------------------------
| eval/              |          |
|    mean_ep_length  | 61.9     |
|    mean_reward     | -515     |
| time/              |          |
|    total_timesteps | 159744   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 67       |
|    ep_rew_mean     | -583     |
| time/              |          |
|    fps             | 410      |
|    iterations      | 10       |
|    time_elapsed    | 399      |
|    total_timesteps | 163840   |
---------------------------------
Eval num_timesteps=169728, episode_reward=-494.07 +/- 132.28
Episode length: 64.40 +/- 9.83
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 64.4      |
|    mean_reward          | -494      |
| time/                   |           |
|    total_timesteps      | 169728    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.448     |
|    n_updates            | 40        |
|    policy_gradient_loss | -3.17e-10 |
|    value_loss           | 0.674     |
---------------------------------------
Eval num_timesteps=179712, episode_reward=-548.83 +/- 158.12
Episode length: 63.50 +/- 10.83
---------------------------------
| eval/              |          |
|    mean_ep_length  | 63.5     |
|    mean_reward     | -549     |
| time/              |          |
|    total_timesteps | 179712   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.3     |
|    ep_rew_mean     | -584     |
| time/              |          |
|    fps             | 406      |
|    iterations      | 11       |
|    time_elapsed    | 443      |
|    total_timesteps | 180224   |
---------------------------------
Eval num_timesteps=189696, episode_reward=-555.80 +/- 162.26
Episode length: 72.80 +/- 22.55
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 72.8     |
|    mean_reward          | -556     |
| time/                   |          |
|    total_timesteps      | 189696   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 1.79e-07 |
|    learning_rate        | 0.819    |
|    loss                 | 1.35     |
|    n_updates            | 44       |
|    policy_gradient_loss | 5.77e-10 |
|    value_loss           | 0.663    |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 65.6     |
|    ep_rew_mean     | -569     |
| time/              |          |
|    fps             | 402      |
|    iterations      | 12       |
|    time_elapsed    | 488      |
|    total_timesteps | 196608   |
---------------------------------
Eval num_timesteps=199680, episode_reward=-572.98 +/- 132.45
Episode length: 64.80 +/- 9.59
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 64.8     |
|    mean_reward          | -573     |
| time/                   |          |
|    total_timesteps      | 199680   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.727    |
|    n_updates            | 48       |
|    policy_gradient_loss | 7.2e-10  |
|    value_loss           | 0.639    |
--------------------------------------
Eval num_timesteps=209664, episode_reward=-631.16 +/- 170.50
Episode length: 69.20 +/- 10.02
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.2     |
|    mean_reward     | -631     |
| time/              |          |
|    total_timesteps | 209664   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.7     |
|    ep_rew_mean     | -617     |
| time/              |          |
|    fps             | 399      |
|    iterations      | 13       |
|    time_elapsed    | 533      |
|    total_timesteps | 212992   |
---------------------------------
Eval num_timesteps=219648, episode_reward=-595.10 +/- 175.27
Episode length: 66.90 +/- 12.95
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 66.9     |
|    mean_reward          | -595     |
| time/                   |          |
|    total_timesteps      | 219648   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.644    |
|    n_updates            | 52       |
|    policy_gradient_loss | 5.78e-10 |
|    value_loss           | 0.691    |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 69.5     |
|    ep_rew_mean     | -616     |
| time/              |          |
|    fps             | 397      |
|    iterations      | 14       |
|    time_elapsed    | 577      |
|    total_timesteps | 229376   |
---------------------------------
Eval num_timesteps=229632, episode_reward=-562.42 +/- 127.42
Episode length: 64.20 +/- 9.40
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 64.2     |
|    mean_reward          | -562     |
| time/                   |          |
|    total_timesteps      | 229632   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 1.19e-07 |
|    learning_rate        | 0.819    |
|    loss                 | 0.911    |
|    n_updates            | 56       |
|    policy_gradient_loss | 1.13e-09 |
|    value_loss           | 0.695    |
--------------------------------------
Eval num_timesteps=239616, episode_reward=-542.54 +/- 169.02
Episode length: 64.40 +/- 14.13
---------------------------------
| eval/              |          |
|    mean_ep_length  | 64.4     |
|    mean_reward     | -543     |
| time/              |          |
|    total_timesteps | 239616   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 66.7     |
|    ep_rew_mean     | -581     |
| time/              |          |
|    fps             | 397      |
|    iterations      | 15       |
|    time_elapsed    | 618      |
|    total_timesteps | 245760   |
---------------------------------
Eval num_timesteps=249600, episode_reward=-669.22 +/- 200.71
Episode length: 72.40 +/- 10.73
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 72.4      |
|    mean_reward          | -669      |
| time/                   |           |
|    total_timesteps      | 249600    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.455     |
|    n_updates            | 60        |
|    policy_gradient_loss | -3.26e-10 |
|    value_loss           | 0.653     |
---------------------------------------
Eval num_timesteps=259584, episode_reward=-527.37 +/- 94.47
Episode length: 69.70 +/- 14.99
---------------------------------
| eval/              |          |
|    mean_ep_length  | 69.7     |
|    mean_reward     | -527     |
| time/              |          |
|    total_timesteps | 259584   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.4     |
|    ep_rew_mean     | -599     |
| time/              |          |
|    fps             | 397      |
|    iterations      | 16       |
|    time_elapsed    | 658      |
|    total_timesteps | 262144   |
---------------------------------
Eval num_timesteps=269568, episode_reward=-520.84 +/- 146.20
Episode length: 62.50 +/- 9.68
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 62.5      |
|    mean_reward          | -521      |
| time/                   |           |
|    total_timesteps      | 269568    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.543     |
|    n_updates            | 64        |
|    policy_gradient_loss | -2.68e-10 |
|    value_loss           | 0.629     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 67.6     |
|    ep_rew_mean     | -589     |
| time/              |          |
|    fps             | 400      |
|    iterations      | 17       |
|    time_elapsed    | 695      |
|    total_timesteps | 278528   |
---------------------------------
Eval num_timesteps=279552, episode_reward=-509.54 +/- 129.15
Episode length: 66.00 +/- 8.87
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 66       |
|    mean_reward          | -510     |
| time/                   |          |
|    total_timesteps      | 279552   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.401    |
|    n_updates            | 68       |
|    policy_gradient_loss | 3.37e-11 |
|    value_loss           | 0.646    |
--------------------------------------
Eval num_timesteps=289536, episode_reward=-618.57 +/- 183.22
Episode length: 68.60 +/- 10.81
---------------------------------
| eval/              |          |
|    mean_ep_length  | 68.6     |
|    mean_reward     | -619     |
| time/              |          |
|    total_timesteps | 289536   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 66.7     |
|    ep_rew_mean     | -581     |
| time/              |          |
|    fps             | 400      |
|    iterations      | 18       |
|    time_elapsed    | 735      |
|    total_timesteps | 294912   |
---------------------------------
Eval num_timesteps=299520, episode_reward=-540.82 +/- 168.96
Episode length: 69.50 +/- 12.81
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 69.5     |
|    mean_reward          | -541     |
| time/                   |          |
|    total_timesteps      | 299520   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 1.19e-07 |
|    learning_rate        | 0.819    |
|    loss                 | 0.489    |
|    n_updates            | 72       |
|    policy_gradient_loss | -2.5e-10 |
|    value_loss           | 0.692    |
--------------------------------------
Eval num_timesteps=309504, episode_reward=-467.95 +/- 110.70
Episode length: 63.70 +/- 16.55
---------------------------------
| eval/              |          |
|    mean_ep_length  | 63.7     |
|    mean_reward     | -468     |
| time/              |          |
|    total_timesteps | 309504   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 64.6     |
|    ep_rew_mean     | -564     |
| time/              |          |
|    fps             | 400      |
|    iterations      | 19       |
|    time_elapsed    | 776      |
|    total_timesteps | 311296   |
---------------------------------
Eval num_timesteps=319488, episode_reward=-628.44 +/- 193.38
Episode length: 69.10 +/- 11.93
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 69.1      |
|    mean_reward          | -628      |
| time/                   |           |
|    total_timesteps      | 319488    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.384     |
|    n_updates            | 76        |
|    policy_gradient_loss | -1.96e-09 |
|    value_loss           | 0.66      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 66.5     |
|    ep_rew_mean     | -568     |
| time/              |          |
|    fps             | 400      |
|    iterations      | 20       |
|    time_elapsed    | 817      |
|    total_timesteps | 327680   |
---------------------------------
Eval num_timesteps=329472, episode_reward=-588.08 +/- 140.27
Episode length: 65.50 +/- 10.41
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 65.5      |
|    mean_reward          | -588      |
| time/                   |           |
|    total_timesteps      | 329472    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.819     |
|    loss                 | 0.494     |
|    n_updates            | 80        |
|    policy_gradient_loss | -9.09e-12 |
|    value_loss           | 0.651     |
---------------------------------------
Eval num_timesteps=339456, episode_reward=-544.83 +/- 183.29
Episode length: 66.40 +/- 15.15
---------------------------------
| eval/              |          |
|    mean_ep_length  | 66.4     |
|    mean_reward     | -545     |
| time/              |          |
|    total_timesteps | 339456   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 67.2     |
|    ep_rew_mean     | -572     |
| time/              |          |
|    fps             | 401      |
|    iterations      | 21       |
|    time_elapsed    | 857      |
|    total_timesteps | 344064   |
---------------------------------
Eval num_timesteps=349440, episode_reward=-588.41 +/- 156.99
Episode length: 65.80 +/- 12.16
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 65.8      |
|    mean_reward          | -588      |
| time/                   |           |
|    total_timesteps      | 349440    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.819     |
|    loss                 | 0.524     |
|    n_updates            | 84        |
|    policy_gradient_loss | -1.1e-09  |
|    value_loss           | 0.684     |
---------------------------------------
Eval num_timesteps=359424, episode_reward=-557.38 +/- 127.10
Episode length: 66.70 +/- 10.00
---------------------------------
| eval/              |          |
|    mean_ep_length  | 66.7     |
|    mean_reward     | -557     |
| time/              |          |
|    total_timesteps | 359424   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68       |
|    ep_rew_mean     | -574     |
| time/              |          |
|    fps             | 401      |
|    iterations      | 22       |
|    time_elapsed    | 897      |
|    total_timesteps | 360448   |
---------------------------------
Eval num_timesteps=369408, episode_reward=-564.07 +/- 204.96
Episode length: 66.20 +/- 11.36
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 66.2     |
|    mean_reward          | -564     |
| time/                   |          |
|    total_timesteps      | 369408   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.415    |
|    n_updates            | 88       |
|    policy_gradient_loss | 8.69e-10 |
|    value_loss           | 0.716    |
--------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 66.2     |
|    ep_rew_mean     | -566     |
| time/              |          |
|    fps             | 401      |
|    iterations      | 23       |
|    time_elapsed    | 938      |
|    total_timesteps | 376832   |
---------------------------------
Eval num_timesteps=379392, episode_reward=-575.55 +/- 119.62
Episode length: 68.40 +/- 10.17
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 68.4      |
|    mean_reward          | -576      |
| time/                   |           |
|    total_timesteps      | 379392    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.511     |
|    n_updates            | 92        |
|    policy_gradient_loss | -6.94e-10 |
|    value_loss           | 0.715     |
---------------------------------------
Eval num_timesteps=389376, episode_reward=-546.36 +/- 107.52
Episode length: 66.50 +/- 13.72
---------------------------------
| eval/              |          |
|    mean_ep_length  | 66.5     |
|    mean_reward     | -546     |
| time/              |          |
|    total_timesteps | 389376   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 65.7     |
|    ep_rew_mean     | -560     |
| time/              |          |
|    fps             | 400      |
|    iterations      | 24       |
|    time_elapsed    | 982      |
|    total_timesteps | 393216   |
---------------------------------
Eval num_timesteps=399360, episode_reward=-614.11 +/- 141.42
Episode length: 73.60 +/- 14.79
--------------------------------------
| eval/                   |          |
|    mean_ep_length       | 73.6     |
|    mean_reward          | -614     |
| time/                   |          |
|    total_timesteps      | 399360   |
| train/                  |          |
|    approx_kl            | 0.0      |
|    clip_fraction        | 0        |
|    clip_range           | 0.938    |
|    entropy_loss         | 0        |
|    explained_variance   | 0        |
|    learning_rate        | 0.819    |
|    loss                 | 0.382    |
|    n_updates            | 96       |
|    policy_gradient_loss | 9.28e-11 |
|    value_loss           | 0.625    |
--------------------------------------
Eval num_timesteps=409344, episode_reward=-503.74 +/- 132.89
Episode length: 60.90 +/- 9.61
---------------------------------
| eval/              |          |
|    mean_ep_length  | 60.9     |
|    mean_reward     | -504     |
| time/              |          |
|    total_timesteps | 409344   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 66.3     |
|    ep_rew_mean     | -568     |
| time/              |          |
|    fps             | 399      |
|    iterations      | 25       |
|    time_elapsed    | 1026     |
|    total_timesteps | 409600   |
---------------------------------
Eval num_timesteps=419328, episode_reward=-588.48 +/- 144.92
Episode length: 68.20 +/- 10.82
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 68.2      |
|    mean_reward          | -588      |
| time/                   |           |
|    total_timesteps      | 419328    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.635     |
|    n_updates            | 100       |
|    policy_gradient_loss | -9.57e-10 |
|    value_loss           | 0.69      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 67.3     |
|    ep_rew_mean     | -574     |
| time/              |          |
|    fps             | 397      |
|    iterations      | 26       |
|    time_elapsed    | 1070     |
|    total_timesteps | 425984   |
---------------------------------
Eval num_timesteps=429312, episode_reward=-551.44 +/- 167.87
Episode length: 63.30 +/- 10.57
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 63.3      |
|    mean_reward          | -551      |
| time/                   |           |
|    total_timesteps      | 429312    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.354     |
|    n_updates            | 104       |
|    policy_gradient_loss | -5.28e-10 |
|    value_loss           | 0.666     |
---------------------------------------
Eval num_timesteps=439296, episode_reward=-549.76 +/- 139.45
Episode length: 62.60 +/- 10.37
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.6     |
|    mean_reward     | -550     |
| time/              |          |
|    total_timesteps | 439296   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 67.6     |
|    ep_rew_mean     | -605     |
| time/              |          |
|    fps             | 396      |
|    iterations      | 27       |
|    time_elapsed    | 1116     |
|    total_timesteps | 442368   |
---------------------------------
Eval num_timesteps=449280, episode_reward=-551.97 +/- 157.07
Episode length: 61.70 +/- 10.87
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 61.7      |
|    mean_reward          | -552      |
| time/                   |           |
|    total_timesteps      | 449280    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | -1.19e-07 |
|    learning_rate        | 0.819     |
|    loss                 | 0.434     |
|    n_updates            | 108       |
|    policy_gradient_loss | 1.35e-09  |
|    value_loss           | 0.65      |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 67.5     |
|    ep_rew_mean     | -602     |
| time/              |          |
|    fps             | 395      |
|    iterations      | 28       |
|    time_elapsed    | 1160     |
|    total_timesteps | 458752   |
---------------------------------
Eval num_timesteps=459264, episode_reward=-552.51 +/- 84.15
Episode length: 64.30 +/- 7.55
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 64.3      |
|    mean_reward          | -553      |
| time/                   |           |
|    total_timesteps      | 459264    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 1.06      |
|    n_updates            | 112       |
|    policy_gradient_loss | -2.56e-10 |
|    value_loss           | 0.658     |
---------------------------------------
Eval num_timesteps=469248, episode_reward=-519.23 +/- 152.04
Episode length: 62.30 +/- 8.46
---------------------------------
| eval/              |          |
|    mean_ep_length  | 62.3     |
|    mean_reward     | -519     |
| time/              |          |
|    total_timesteps | 469248   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 67.9     |
|    ep_rew_mean     | -591     |
| time/              |          |
|    fps             | 394      |
|    iterations      | 29       |
|    time_elapsed    | 1203     |
|    total_timesteps | 475136   |
---------------------------------
Eval num_timesteps=479232, episode_reward=-443.51 +/- 99.46
Episode length: 67.40 +/- 13.52
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 67.4      |
|    mean_reward          | -444      |
| time/                   |           |
|    total_timesteps      | 479232    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.722     |
|    n_updates            | 116       |
|    policy_gradient_loss | -1.47e-09 |
|    value_loss           | 0.653     |
---------------------------------------
Eval num_timesteps=489216, episode_reward=-640.62 +/- 207.59
Episode length: 70.30 +/- 14.01
---------------------------------
| eval/              |          |
|    mean_ep_length  | 70.3     |
|    mean_reward     | -641     |
| time/              |          |
|    total_timesteps | 489216   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 68.3     |
|    ep_rew_mean     | -585     |
| time/              |          |
|    fps             | 394      |
|    iterations      | 30       |
|    time_elapsed    | 1245     |
|    total_timesteps | 491520   |
---------------------------------
Eval num_timesteps=499200, episode_reward=-645.91 +/- 174.91
Episode length: 73.70 +/- 10.73
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 73.7      |
|    mean_reward          | -646      |
| time/                   |           |
|    total_timesteps      | 499200    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.56      |
|    n_updates            | 120       |
|    policy_gradient_loss | -3.37e-10 |
|    value_loss           | 0.675     |
---------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 66.4     |
|    ep_rew_mean     | -568     |
| time/              |          |
|    fps             | 393      |
|    iterations      | 31       |
|    time_elapsed    | 1291     |
|    total_timesteps | 507904   |
---------------------------------
Eval num_timesteps=509184, episode_reward=-535.42 +/- 132.38
Episode length: 70.00 +/- 11.61
---------------------------------------
| eval/                   |           |
|    mean_ep_length       | 70        |
|    mean_reward          | -535      |
| time/                   |           |
|    total_timesteps      | 509184    |
| train/                  |           |
|    approx_kl            | 0.0       |
|    clip_fraction        | 0         |
|    clip_range           | 0.938     |
|    entropy_loss         | 0         |
|    explained_variance   | 0         |
|    learning_rate        | 0.819     |
|    loss                 | 0.474     |
|    n_updates            | 124       |
|    policy_gradient_loss | -5.29e-10 |
|    value_loss           | 0.676     |
---------------------------------------
Eval num_timesteps=519168, episode_reward=-692.59 +/- 180.60
Episode length: 74.90 +/- 15.03
---------------------------------
| eval/              |          |
|    mean_ep_length  | 74.9     |
|    mean_reward     | -693     |
| time/              |          |
|    total_timesteps | 519168   |
---------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 66       |
|    ep_rew_mean     | -554     |
| time/              |          |
|    fps             | 392      |
|    iterations      | 32       |
|    time_elapsed    | 1335     |
|    total_timesteps | 524288   |
---------------------------------
